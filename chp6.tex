% !TeX root = main.tex
\chapter{Paper-based semantic speech editing}\label{chp:paper}

% Need for paper
One of the findings from Chapter~\ref{chp:screen} was that some producers find their work environment noisy and
distracting, and do not like working with screens for extended periods. This leads many to print out transcripts of
speech recordings so that they can review and edit the recordings away from the screen and/or office.

% Advantage of paper
Working on paper offers a number of advantages over working on screens. Most of these are self-evident, but still
worth reviewing.  Paper is lightweight, portable and does not require any power, which allows users to work almost
anywhere.  It is not back-lit, so is easier on the eyes.  It can be navigated quickly, annotated freely whilst reading,
and individual pages can be laid out and easily compared.  Its physical low-tech nature also means that it is
intuitive, robust, durable and does not crash or lose data.  Reading from paper rather than a screen allows readers to
gain a deeper understanding, easily cross-reference other documents, and interleave reading and writing
\citep{OHara1997, Mangen2013, Singer2017}.

%Transcripts are often printed out because:
%- it is easier to read than on screen
%- allows portability,
%- allows annotation,
%- flexible spatial layout
%- quick navigation
%- cross-referencing,
%- reduces battery anxiety,
%- more trusted by non-tech-savvy users

%A similar process happens in the production of media content. The production workflow typically involves recording
%material, selecting which parts of that material to use, then editing the desired material down to the final output
%\citep{Baume2015}.  Many producers will `log' the material after it is recorded by writing transcripts of what was
%said.  This is either done themselves or using a third-party service. These transcripts help producers to recall what
%was said and when, identify themes, and make links between different parts of their content.

% Disadvantage of paper
Compared to digital media, working with a physical medium like paper introduces restrictions that make it more
difficult to copy, share, store and archive information. Printing a document breaks the link to its digital source, so
is normally a one-way process, where any information that is changed/added cannot is not fed back. Additionally,
freehand annotations are unstructured, and cannot easily be digitised and re-used. This forces users to manually
re-enter the information into a computer. In the case of radio production, this involves using a DAW to edit the audio
based on paper notes, which can be slow and tedious.

% Digital paper
There are a number of technological solutions that can be used to create a `digital bridge' between paper and
its digital source. These offer the possibility to combine the advantages of paper and digital workflows. Using this
approach, it is possible to link the words on a printed transcript of speech to the time they were spoken in the
original audio recording. Furthermore, by linking the paper annotations to audio edit commands, the paper could be used
to directly edit the audio content.

% Intro
In this chapter, we investigate paper-based workflows in the context of professional radio production. We describe how
we captured the requirements for a paper-based speech editor, and how we designed and built a working prototype. We
then describe a contextual study of radio production in which we directly compared screen-based and paper-based
workflows. 

\section{Background}\label{sec:paper-background}
Radio production using a paper interface requires a system that automatically translates annotations on a printed
transcript to audio edit commands. Such a system requires a method of digitally capturing the annotations and linking
those annotations to audio timestamps.
These features have been explored using a variety of different approaches,
including using digital pens, barcodes, digital ink and synchronised note-taking. In this section, we will outline
these methods and their benefits and challenges.

%\subsection{Screen-based}
%Transcript-based interfaces have already successfully been applied to both audio and video editing. SCANMail
%\citep{Whittaker2002} demonstrated the advantages of navigating voicemail recordings using a transcript, but did not
%include editing capabilities.  The LIDS Editor \citep{Apperley2002}, and later TRAED \citep{Masoodian2006}, used
%automatically-generated transcripts to allow users to navigate and edit lecture recordings by removing and rearranging
%sentences and words. Even though automatically-generated transcripts are imperfect, Whittaker and Amento found they are
%sufficiently accurate to allow navigation and editing \citep{Whittaker2004}.  More recently, Rubin \citep{Rubin2013}
%created a system for using editable crowd-sourced transcripts to create audio stories.  Similar techniques have been
%applied to video editing. SILVER \citep{Casares2002} was a video editor that had an editable transcript window,
%generated from subtitles, and Berthouzoz et. al.  \citep{Berthouzoz2012} developed a system that used crowd-sourced
%transcripts and image processing to allow text-based editing of multi-camera video interviews.

\subsection{Digital pen systems}
A digital pen is a pen that includes an on-board infrared camera that tracks the position of the pen while it writes on
paper. Digital pens must be used in combination with paper that has a unique non-repeating dot pattern printed onto it.
By reading this pattern, the pen can calculate exactly where it is when touching the page. This information is captured
up to 100 times a second and recorded digitally onto the pen, which can later be downloaded onto a computer. This
technology has been patented by Anoto Group \citep{Fahraeus2003}, who exclusively manufacture and licence digital pen
products. As such, this technology is often referred to as the `Anoto dot pattern'.

\citet{Guimbretiere2003} introduced a concept called `PADD', which was a system of editing documents that uses the
Anoto pattern to allow users to move from digital documents to paper and back again. \citet{Conroy2004} created
`ProofRite', which was the first full implementation of a PADD system. It captured annotations made to a printed text
document, and overlaid the annotations onto the text in a word processor. The annotations were anchored to the text,
such that they `reflow' when the text is moved. Through informal feedback, users suggested that their annotations
should translate into actions such as delete.

\citet{Weibel2008} created `PaperProof', which interpreted the edit annotations and automatically applied them to the
document. Gestures for delete, insert, replace, move and annotate were translated into modifications in a word
processor, and intelligent character recognition was used to digitise any hand-written text. Processing the annotations
allows for a two-way interaction between the digital and paper representations. There were no user studies of the
PaperProof system.

%TODO Make a better summary
Digital pens are an attractive proposition because they are familiar and allow for two-way interaction. However, the
technology is expensive and closed, which makes development difficult.

\subsection{Barcodes}
Paper transcripts have been explored as a method of navigating video recordings by using a device to detect the
position in the text and play the video from that position. \citet{Hull2003} describes a system called `Video Paper',
which embedded video keyframes with barcodes down the side of the page. It used a PDA to scan the barcodes that linked
to a position in a video, which was downloaded and played on the device. \citet{Klemmer2003} applied Video Paper to
oral history in a project called `Books with Voices'. An evaluation of 13 users found that it had substantial benefits
with minimal overhead.  \citet{Erol2007} went a step further by embedding the video in the barcode data, removing the
need to download the video from a separate source. \citet{Erol2008} removed the need for barcodes by creating a system
called `HotPaper', which used a camera to measure the whitespace between words and matched that to unique patterns in
the text.

Barcode-based systems provide a link between text and media, however they do not provide a convenient method of
capturing annotations. It would be possible to use a PDA-style device to capture annotations and link them to a
particular barcode. However, this requires that the annotation are entered into a handheld device, rather that just
written on the paper. Additionally, the size of the barcodes means that the precision of the timestamps would be at a
sentance-level rather than word-level.

\subsection{Digital ink}
`Digital ink' refers to technology that digitally captures and responds to the moments of a pen, such as a stylus on a
tablet PC.  Several system have experimented with using pens with interactive sliders to provide advanced control for
navigating video content, such as `LEAN' \citep{Ramos2003}, `Zlider' \citep{Ramos2005} and MobileZoomSlider/ScrollWheel
\citep{Huerst2008}. However, these systems are limited to the navigation of content, without changing or labelling it.
Our interest is primarily in the annotation and editing of media, which the following systems have explored using
digital ink interfaces.

\citet{Diakopoulos2006} created a digital ink interface for creating and annotating segments of a pre-recorded video,
called `VideoTater'. Segments could be created by drawing a vertical line on a video timeline, and merged by drawing a
horizontal line between them. Each segment could be tagged by selecting it and hand-writing text. The back of
the pen could be used to erase tags. Pen pressure was used to distinguish between selection and tagging, with low
pressure for selecting and high for tagging. Informal feedback from three users found that the gestures were
successful, and that the pressure mapping worked well.

\citet{Cattelan2008} added functionality for marking edit commands using digital ink in their system `WaCTool'. The
system included a variety of features for annotation, editing and real-time collaboration. Users could use a pen to
write annotations by tapping the video to freeze it, then drawing on the video frame.  Users could apply a `skip'
command to a segment of the video by using the pen to tap the bottom left of the video at the start of an unwanted
segment, and tapping the bottom right at the end. The `skip' command is analogous to editing out part of the video.
Similar commands for looping and slow motion were also available by tapping different regions.

%\citet{Yoon2014} created a collaborative tablet-based document annotation system called `RichReview', which offered
%users three modalities in which to annotate documents - freeform inking, voice recording and deictic gestures. The
%voice recordings were displayed using a waveform, overlaid with an automatically generated transcript of the speech.
%Users could trim or tidy the voice recordings by drawing a line through words or pauses to remove them.  The system was
%evaluated using a qualitative study of 12 students which found that the editing features were considered easy to use
%and efficient for removing `umm's and long pauses.  However many participants reported that the transcripts were not
%accurate enough to use without having to listen to the audio.

`Video as Ink' \citep{Cabral2016} took an alternative approach by allowing users to `paint' video frames and segments
onto a 2D canvas using a pen and tablet interface. The canvas works as a timeline, but extends vertically in both
directions so that the video can be painted onto multiple rows. The system includes gestures for adding, moving,
erasing and selecting content. An evaluation of 12 participants found that the canvas allowed users to creatively
explore different possibilities. However, the interface relies on the visual organisation of images, which does not
necessarily translate well to audio and text.

\subsection{Synchronised note-taking}
A number of previous systems have explored how media can be annotated as it is recorded or replayed. These have often
been developed for note-taking during meetings or lectures, but could also be applied to radio production. Although we
did not find that it was commmon to write notes during interviews, we found that producers often listen back to their
recordings, so these systems could be using during that replay process.

`Marquee' \citep{Weher1994} was a digital ink system for supporting the task of logging during a live video recording.
Users could make synchronised handwritten notes by drawing a horizontal line to mark a timestamp, then writing their
notes below.  Additionally, they could create a list of keywords, and add them to their notes by pressing them at the
right moment.  An evaluation of the system with three participants found that users did not partake in discussions
while logging, which may prevent such a system being used during an interview.  The freehand nature of the notes meant
that it could easily handle the different styles of each user.  Users reported that they didn't feel they had to make
as many notes as they normally would, because they could later refer back to the video recording. The authors also
noted that videos can be further annotated during replay, allowing for an iterative logging process.

`Dynomite' \citep{Wilcox1997} was a virtual notebook that recorded audio synchronously with digital ink handwritten
notes. The user's notes could be assigned to different `properties', either before or after they were written, to
indicate an action point, for example, or a user-specified keyword.  Users could also highlight a portion of the audio
by pressing a `mark' button or making a specific gesture.  This would highlight the audio for a specified time period,
unless the `extend' or `end mark' buttons were pressed.  Any notes made during this period were displayed in bold and
the highlighted segments were displayed using colours on a horizontal timeline. An evaluation of nine users found that
users took fewer notes when using the audio highlighting, and that they wanted to go back and use the audio to improve
the notes afterwards. Both of these findings mirror those from \citet{Weher1994}.

`The Audio Notebook' \citep{Stifelman2001} was a system that used a physical pen and paper interface in combination
with a device that recorded audio synchronously with the page and vertical location of the written notes.  The device
could also replay the audio from a page and display which notes relate to the current playback position using an LED
scrollbar display at the side of the page. Users could also use the scrollbar to control the position of the audio
playback.  A longitudinal study of six participants over five months again found that users needed to take fewer notes,
and made more notes during replay. Two of the participants were reporters who used the system while recording
interviews. One reporter made minimal notes during the interview, but replayed the interview and made additional notes,
including star symbols to indicate important moments. They also extracted quotes of interest by typing them into a
computer. The other reporter was skeptical of the system, so made detailed notes so not to rely on the audio. However,
they were later able to use the system to recall bits of the interview, which was many times faster than their existing
technique of fully transcribing the audio recording.

ChronoVis \citep{Fouse2011} used the Anoto dot pattern to record paper notes during playback of a video. The on-screen
playback interface allowed users to click on the digital display of the handwritten notes to navigate to that position
in the video, or browse a list of timestamped notes. Alternatively, they could reprint their notes and use a
wirelessly-connected digital pen to tap on the notes, which controlled the playback position.

\subsection{Correction}
In chapter~\ref{chp:screen}, we identified that some producers were interested in correcting the transcript for sharing
with others, or for later publication.

There are clearly defined symbols for use in proof correction (e.g. \citet{ISO5776}).

Interfaces to correct errors in transcripts have also been considered, such as that from \citet{Suhm2001}.

\subsection{Summary}

In short, our review of the literature indicates that while many of the issues with which we are concerned – 
annotation, navigation and editing of media – have been
explored individually by various researchers, they have yet to be explored in combination.

Our system primarily merges the first two approaches -- transcript-based editing and
annotation using digital pens. In this paper, we will consider this technique in the context of professional media
production.

\section{System requirements}\label{sec:paper-requirements}

%TODO What were the problems?

%TODO What other solutions were considered?

%TODO How was the design of the prototype informed?

We informally tested a paper prototype of our system concept on five radio producers from the BBC. The purpose of the
test was to validate our assumptions about annotation of transcripts, gather feedback on which features were valued and
to select an approach of translating annotations to edit commands.  Two of the participants worked in current affairs,
two in science and one in documentaries. The participants had between 7 and 13 years experience working as a radio
producer.

\subsection{Paper prototype}
Each participant was asked to provide an interview they had recently recorded, which was automatically transcribed.  We
used a speech-to-text engine based on Kaldi\footnote{\url{http://kaldi-asr.org}}, trained on subtitled television
broadcasts, to extract a transcript from each recording. In addition to providing precise timings for the start and
duration of each word, the speech-to-text system provides a confidence rating to signify the probability of that word
being correct. A speaker diarization \citep{AngueraMiro2012} algorithm was also run on the content to estimate the
times and identities of when different people are speaking.

\begin{figure}[h]
  \centering
  \includegraphics[width=\columnwidth]{figs/mockup}
  \caption{Paper prototype with natural annotations, including
    underlining, line down the side with notes, word corrections, and a
    vertical line to indicate the end of an edit.}
  \label{fig:natural}
\end{figure}

The transcript was printed double-spaced, with a grey box below each word to indicate the underlining area.  Words with
a low confidence rating were `low-lighted' by shading them grey -- a technique known as confidence
shading \citep{Vemuri2004}.  Each line of the transcript had a timestamp on the left and a square box on the right, which
could be used to select a whole line at a time.

\subsection{Method}
We tested two inactive paper prototypes of the paper interface.  One used the speaker diarization information to
segment the transcript into paragraphs, with the start of each paragraph labeled with the speaker identifier in square
brackets (e.g.  \texttt{{[}S1{]}}). The other version did not use any of this information. The users were presented
with the normal transcript first, followed by the one with speaker information.

For each version, we asked the participants to use a normal pen to annotate the first page of the transcript as they
would normally, to only use strikethrough on the second page and only use underline on the third page. At the end of
the test, each participant was interviewed about their experience overall and with each method.

\subsection{Results}
The reaction to the system was overwhelmingly positive. All of the participants could immediately see the value of such
a system and remarked that it would save them significant amounts of time and \textit{``revolutionise''} their
production workflow.

\subsubsection{Natural behaviour}
The participants started the exercise by annotating the transcript any way they wanted.  Three of the five participants
used underlining to select desired words, with one using strikethrough to remove words and the other using both
methods.  Three drew a line down the side of the page to select multiple lines at a time.  Two used short vertical
lines between words to signify in- and out-points for edits.  Two corrected mistakes in the transcript by writing over
or above the incorrect word. Finally, one participant selected words using a `lasso' technique.

Each participant used a different mixture of annotation techniques. Some of these techniques, such as underlining,
strikethrough and lines down the side, are easy to detect.  Others, such as corrections, lasso and lines between words,
are less easy to detect. There was no room to write notes at the side of the page, but one participant did it anyway
and two others expressed an interest in doing so if there was a margin. One participant said that they would not want a
margin.

\subsubsection{Additional features}
Four of the five participants found the paragraphs and speaker information useful. Typically, interviews have a
presenter and contributor and the producers find it valuable to know when the presenter is asking a question.  Three of
the participants said that they were able to find the questions much more easily with this feature enabled. However,
another participant said they found the speaker diarization \textit{``distracting''}.

All participants found the timestamps and confidence shading features useful, with some commenting that one timestamp
per page would be sufficient.  All of the participants liked being able to select whole lines at a time, with one
asking whether a similar function could be available to delete content. It was also suggested that the side boxes could
be used to rate or star a selection.

Four of the participants found the transcript was accurate enough to use for editing their material.  The interview
used by the other participant has a strong Scottish accent which affected the transcript quality. Finally, several
participants noted the importance of listening to the recording and expressed a desire to hear the content whilst
reading the paper transcript.

\subsubsection{Select or delete}
The participants were asked whether they preferred selecting or deleting words. Three of them preferred select, with
one commenting that it \textit{``felt more natural''} and another saying deleting felt \textit{``counter-intuitive''}.
The other two participants preferred deleting, with one commenting that it was \textit{``the way my brain works''} and
the other saying they prefer to \textit{``get stuff out of the way''}. As there was no agreement, the system should
have options to cater for a variety of translation methods.

%- Useful to know where presenter is asking question

%Diarization
%- Neal found it easier
  %- Shows where the questions are
%- Phil found it distracting
%- Wes found it absolutely useful
%- Marnie 'made it much easier', liked having paragraphs (usually one point per
%paragraph)


%Timestamps
%needed every couple of minutes

%Use L/R channels for presenter/contributor
%- Phil and Wes
%- Sometimes multiple on R channel (about one in eight interviews)

%Prefer landscape to portrait (landscape is `irritating' and doesn't match
%what's on screen)

%Whole line
%- Would like to delete line at a time

%Note-taking
%- One doesn't take many notes

%Correction
%- One corrects then selects edits
%- Wes would like option

%Highlighting
%- Asterix/star to mark important bits (Phil, Neal)
%- Underline twice (Neal, Marnie liked idea)
%- Wes sometimes double-stars

%Delete vs select (2 vs 3)
%- Phil likes to `get stuff out of the way'
  %- Words that aren't marked should be kept by default
  %- Underline should undo delete
%- Neal prefers selecting over deleting
  %- Delete should undo select, if underlined too far
%- Wes prefers selecting - deleting is `counter-intuitive'
  %- Would like delete to override select, but would like options for opposite
%- Marnie found it trickier to delete than select, selection is more natural
  %- worried about deleting something good
%- Alex feels delete is more natural, 'way may brain works, 'challenge is to
%nibble away', thinks delete should be active, select should override delete

%Margin
%- Phil wouldn't need a margin
%- Neal would like one

%Export
%- gaps should be put between edits (real-time?)

%Transcript
%- Phil and Neal no problems
%- Wes had problems (strong Scottish accent) made it unusable

%Observation
%- Neal
  %- underlined as he read (unprompted), scribbled out when mistake made
  %- found it more difficult to only delete
%- Wes
  %- vertical lines for in/out points

%Other
  %- Neal would like to press and hear word (on laptop?)
    %- Wes: Don't know what sounds good
  %- Wes works in a team of 3/4. They use Box to collaborate

\subsection{Discussion}
%Our system performs the same function as these systems, but uses a paper
%interface rather than a screen-based one.
%Our system
%similarly interprets the paper annotations and applies them to the media
%content.
%Our system does not yet have the ability to replay content from the
%paper interface, but this is a feature that could later be included.
%Our system is based on printed transcripts rather than video frames on
%a tablet, but approaches the same problems from a different angle.
%These systems do not provide the user with any pre-written notes.
%Our system uses speech-to-text to generate a transcript, which the user can
%then annotate further.
The participants used a variety of techniques to annotate the transcripts, but underline, strikethrough and lines down
the side were common. There were strong but mixed opinions on whether selection of desired material, or removal of
unwanted material, was the best approach to use for editing, so we decided to make both options available.

%TODO What happens when underline and strike same word?

The transcripts were generated using speech-to-text but were considered good enough to edit with. We found that
participants enjoyed having paragraphs, speaker information, timestamps, confidence shading and being able to select
whole lines.

Some participants expressed a desire to listen to the media from the paper interface, and this functionality has been
shown to be useful in previous work. %TODO Reference
Users could start playback by pressing a word with the digital pen. This could be
achieved either by wirelessly linking the digital pen to a computer that plays the content, or by storing the audio on
the pen itself similarly to the LiveScribe Sound
Stickers\footnote{\url{http://store.livescribe.com/sound-stickers-1-1.html}}.

Previous work has also shown the benefits of being able to write freehand notes which link to specific points in
recordings. These could be attached to a screen-based interface similarly to ProofRite \citep{Conroy2004}, or
handwriting recognition could be used to store and organise these notes in a structured way. This approach could also
be used to fix incorrect words in the transcript.

Media content is often produced in teams rather than individually.  Although our system does not prevent multiple
people from working on a transcript, it does not exploit opportunities to smoothly handle multiple users.

\subsection{Summary}
The participants all reacted very positively to the system. They enjoyed the additional features we tested including
speaker information, paragraphs, timestamps, confidence shading and selection of whole lines. Based on their feedback,
we also added an optional margin to the interface.

A variety of annotation marks were used by the participants to indicate edit decisions, including underline,
strikethrough, lines down the side and lines between words.  Participants either preferred to select content they
wanted or to delete content they didn't want, and felt strongly either way.

There are a number of opportunities to build on the system, including being able to capture freehand notes, listen to
the content using the paper interface, fix incorrect words and work collaboratively.

\section{System design}\label{sec:paper-design}
Using the feedback gathered from our requirements gathering, we designed and created a working prototype of the system
(see Figure~\ref{fig:diagram}). To build our prototype, we collaborated with Anoto who are the manufacturers of the
digital pen. Anoto developed a system that generated the print-out and procesed the annotations. We then integrated
these system with our screen-based semantic speech editor, which includes speech-to-text and EDL generation.

\subsubsection{Paper interface design}

Anoto used an existing product called `Anoto Live Forms' to create the editing system. It works by dividing the paper
into rectangular zones. When a digital pen draws inside one of these zone, that data can then be extracted digitally.
Traditionally this is used to work out when someone has ticked a box on a form, or to extract an image of a signature
drawn inside of a box. However, by creating zones that aligned with the text of a transcript, we were able to use this
system to digital capture editing annotations.

Based on the feedback from users, the timestamps, paragraphs, speaker information and confidence shading have been
retained. However the timestamps only appear at the start of each paragraph. An optional margin has also been added to
allow users to make unstructured notes using their digital pen.  It is not currently possible to select a whole line at
a time, but this will shortly be added.

For each word in the transcript, two rectangular active areas are defined on the page -- one around the word itself and
another in the space directly below the word, which is lightly shaded. Any marks made on or below the word label that
word as `deleted' and `selected', respectively. This allows the user to use a digital pen to delete words using a
strikethrough, or to select words by underlining.  The annotations are automatically uploaded in batch to the system
when the pen is docked.

\begin{figure}[h]
  \centering
  \includegraphics[width=\columnwidth]{figs/paper-interface-diagram.pdf}
  \caption{Diagram of paper interface layout with numbers features. Dotted lines indicate hidden active zones. 1:
    Timestamp at beginning of each paragraph, 2: Speaker diarization, 3: Word selection, 4: Word removal, 5: Line
    selection, 6: Margin for freehand notes}
  \label{fig:paper-interface-diagram}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=\columnwidth]{figs/interface-darkened}
  \caption{Example of working prototype, showing the shaded box beneath each
    word, paragraph for each speaker with timestamp (06.45) and identity (S19),
    and optional margin for freehand annotations.}
  \label{fig:layout}
\end{figure}

\subsubsection{Paper interface integration}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.5\columnwidth]{figs/uist-sys-diagram}
  \caption{System diagram, flowing from top to bottom}
  \label{fig:diagram}
\end{figure}

The uploaded annotations are combined with the word timings to calculate the edit commands.  These can be used to
directly edit the media (as shown in Figure~\ref{fig:diagram}) which removes the unwanted parts of the content. This is
known as a `destructive' edit.  Alternatively, the edit commands can be used to create an edit decision list (`EDL').
An EDL file describes the in- and out-point for a series of edits, which can be loaded into audio or video editing
software.  As it is `non-destructive', this approach allows the user to make corrections to the edit before it is
rendered. 

%Several approaches can be taken in interpreting the delete and
%select annotations:
%\begin{itemize}
  %\itemsep0em
  %\item Remove words marked as deleted, except those marked as selected
  %\item Only keep words marked as selected, except those marked as deleted
  %\item Remove words marked as deleted, highlight words marked as selected
%\end{itemize}

\subsection{Screen interface design}

\begin{figure}[ht]
  \centering
  \includegraphics[width=\columnwidth]{figs/discourse-interface-labelled.pdf}
  \caption{Layout of the screen interface, with numbered features.
  1: Media storage,
  2: Media upload,
  3: Highlight current playback position,
  4: Print transcript,
  5: Save edits and corrections to transcript,
  6: Edit storage and export,
  7: Display timestamps of selection,
  8: Underline transcript,
  9: Confidence shading,
  10: Strikethrough transcript,
  11: Duration of edited audio,
  12: Name of current asset,
  13: Show/hide strikethrough,
  14: Underline/strie buttons,
  15: Playback buttons,
  16: Speaker diarization}
  \label{fig:dialogger-interface}
\end{figure}

\section{Method}\label{sec:method}

\subsection{Design and procedure}

% Explain why video recording wasn't used

\paragraph{Stage 1: Training and usability study}
Firstly, the participant will be briefed on the background, objectives and design of the study as detailed in the
protocol.  Should they wish to participate, they will be asked to read and agree to the consent form.

There are two objectives to the training stage – firstly to introduce the participant to the prototype system so that
they understand its capabilities, and secondly to test the usability of the interface.

The experimenter will set the participant up on the prototype system, including giving them access to the web-based
interface, giving them a digital pen and configuring the pen’s docking system. The participant will then receive
training on how to operate the web interface, print out transcripts correctly and use the pen.

The participant will then be asked to perform a series of typical tasks. The participant can ask questions and, if they
become stuck, the experimenter can prompt them. However, the experimenter will not give any other directions. The
experimenter will note whether the participant was able to successfully complete the task and document any problems or
confusion experienced during the execution of the tasks. This process will help to flag any obvious stumbling blocks.

\paragraph{Stage 2: Observation}

This stage will involve observing the participant editing two pieces of content - one using the screen-based interface
and the other using the paper-based interface. The order of the observation will be alternated between participants.
The two pieces of content should be similar in nature, but different so that the participant isn't already familiar
with the content.

The observation will be done passively to allow natural interaction with the system in the participant's normal work
environment. The experimenter will sit beside them making written notes. When the participant has some 'down-time', the
experimenter may choose to ask some questions in order to verify or clarify something they have observed.

The specific items of interest at this stage include:
\begin{itemize}
\item Editing workflow
\item Tools used
\item Data generated
\item Usability challenges and problems
\item Navigation and edit actions
\item Time taken to complete tasks
\item Unexpected reactions
\item Unanticipated usage
\end{itemize}

The prototype will be configured to electronically log the actions of the participant, which will help provide insights
into usage of the various features of the prototype.

Software Usability Scale \citep{Brooke1996}
Perceived Usefulness \citep{Davis1989}

With the participant's permission, photos may be taken of paper notes, annotations or the work environment (see 'data
protection' section below).
{\singlespacing \begin{itemize} \item Using this system in my job would enable me to accomplish tasks more quickly.
  \item Using this system would improve my job performance.
  \item Using this system in my job would increase my productivity.
  \item Using this system would enhance my effectiveness on the job.
  \item Using this system would make it easier to do my job.
\end{itemize}
}

{\singlespacing
\begin{itemize}
  \item I would find this system useful in my job.
  \item I think that I would like to use this system frequently.
  \item I found the system unnecessarily complex.
  \item I thought the system was easy to use.
  \item I think that I would need the support of a technical person to be able to use this system.
  \item I found the various functions in this system were well integrated.
  \item I thought there was too much inconsistency in this system.
  \item I would imagine that most people would learn to use this system very quickly.
  \item I found the system very cumbersome to use.
  \item I felt very confident using the system.
  \item I needed to learn a lot of things before I could get going with this system
\end{itemize}
}

\paragraph{Stage 3: Interview}
Following the observation stage, the participant will be left with access to the prototype system for at least a
further week. This will give them the opportunity to use the system as part of their day-to-day work and uncover issues
which may be appear in the observation.

After this period has passed, the participant will be interviewed about their experience. The primary objective is to
compare the two production methods and extract the advantages and disadvantages of both.

An audio recording will be made of the interview (see 'data protection' section below). This will allow the participant
to speak their mind and allow the experimenter to give the participant their full attention whilst capturing all of the
provided information.

The questions asked will include:
\begin{itemize}
\item Which aspects of the screen-based system did you / did you not find useful?
\item Which aspects of the paper-based system did you / did you not find useful?
\item Overall, which system did you prefer and why?
\end{itemize}


\subsection{Participants}
To make the most of the principal investigator's position in the BBC, participants will be recruited exclusively from
the BBC. They will be invited to volunteer by way of email passed around using existing contacts in various departments
that produce speech-based content. 

The nature of the study requires that participants conduct the tasks as part of their day-to-day job. Therefore,
participants will be required to gain permission from their line manager to take part.

Programmes and producers vary significantly in genre and production techniques.  To take this into account,
participants will be recruited so that there is a representative range of styles.

The participants had between 8 and 28 years experience of professional radio production, with a mean average of
16 years experience.

\subsubsection{Selection criteria}

\begin{itemize}
\item The programme being created should be speech-based.
\item The content of the programme must not contain any sensitive material, as the transcription will be sent to a
  third-party for transcription (see 'data protection' section below).
\item The turn-around time of the programme must be long enough to allow time to recover from any technical issues
  without affecting broadcast output.
\item The participant must have permission from their line manager to take part.
\end{itemize}

The study will involve between six and nine participants, depending on how many meet the criteria. This should uncover
over 90\% of usability problems \citep{Nielsen1993} whilst covering a range of genres and styles, and being a
manageable number given the duration of the experiment.

The study will take place at the participant's own desk at their normal place of work. As the prototype is web-based,
they can use their own computer. There is a standard configuration for BBC desktop computers, so the environment should
be similar for each participant. Use of display screen equipment is already covered under the existing BBC risk
assessment process.

\subsection{Analysis}


\section{Study results}\label{sec:paper-results}

\subsection{Qualitative analysis}

% MAIN TOPICS:
% Annotation
% Correction
% Editing
% Listening
% Normal workflow
% Paper admiration/restrictions/waste
% Screen-based working/reading paper
% Transcript accuracy
% Transcript itself


% OTHER TOPICS:
% Collaboration
% Cost
% Custom vocab
% Digital/analogue
% Combining recordings
% Non-linear workflow
% Pen hardwaare
% Working remotely
% Speaker diarization
% Speed
% Structuring thoughts
% Transcription speed
% Translation


\subsection{Metrics}

% Popularity
\subsubsection{Preference}

\begin{itemize}
  \item Normal paper: 2
  \item Screen: 2
  \item Pen: 4
\end{itemize}

% Usefulness and usability
\subsubsection{Usefulness and usability}

\begin{figure}[h]
  \centering
	\begin{tikzpicture}
	\begin{axis}[
			ybar,
			ymin=0,
			enlarge x limits=0.5,
			legend style={at={(0.5,-0.15)},anchor=north,legend columns=-1},
			ylabel={\% score},
			symbolic x coords={Usefulness, Usability},
			xtick=data,
			nodes near coords,
			nodes near coords align={vertical},
			]
	\addplot coordinates {(Usefulness,85.42) (Usability,81.67)};
	\addplot coordinates {(Usefulness,77.78) (Usability,75.21)};
	\addplot coordinates {(Usefulness,75.00) (Usability,73.33)};
	\legend{Normal paper, Screen, Pen}
	\end{axis}
	\end{tikzpicture}
  \label{fig:usefulusable}
  \caption{Mean average scores for usefulness and usability}
\end{figure}


% Speed
\subsubsection{Speed}

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
  \begin{axis}[
    legend pos=outer north east,
    legend cell align=left,
    xmin=0,
    ymin=0,
    xlabel={Audio length (mins)},
    ylabel={Edit time (mins)}]

  % NORMAL
  \pgfplotstableread{
    X Y
    30 30
    58 43
    32 23
    35 53
    28 35
    28 32
    47 62
    28 22
  }\normaltimes
  \addplot [only marks, mark = *, red] table {\normaltimes};
  \addlegendentry{Normal paper}
  \addplot [thick, red, forget plot] table[
      y={create col/linear regression={y=Y}}
  ]{\normaltimes};
  %\addlegendentry{Normal trend}

  % SCREEN
  \pgfplotstableread{
    X Y
    37 24
    61 61
    30 30
    25 49
    26 31
    41 28
    37 37
    33 45
  }\screentimes
  \addplot [only marks, mark = square*, blue] table {\screentimes};
  \addlegendentry{Screen}
  \addplot [thick, dotted, blue, forget plot] table[
      y={create col/linear regression={y=Y}}
  ]{\screentimes};
  %\addlegendentry{Screen trend}

  % PEN
  \pgfplotstableread{
    X Y
    12 9
    75 56
    31 17
    22 19
    27 27
    38 37
    33 33
    28 41
  }\pentimes
  \addplot [only marks, mark = diamond*, green] table {\pentimes};
  \addlegendentry{Pen}
  \addplot [thick, loosely dashed, green, forget plot] table[
      y={create col/linear regression={y=Y}}
  ]{\pentimes};
  %\addlegendentry{Pen trend}

  \end{axis}
  \end{tikzpicture}
  \caption{Edit time performance for each editing system, with linear regression plot}
  \label{fig:penedittime}
\end{figure}

Mean edit time, per minute of input audio:
\begin{itemize}
    \item Normal paper: 59.2 seconds (x0.9867 real-time)
    \item Screen: 59.34 seconds (x0.9890 real-time)
    \item Pen: 49.77 seconds (x0.8295 real-time)
\end{itemize}

% Usefulness


% Usability




\section{Discussion}\label{sec:paper-discussion}

\section{Conclusion}\label{sec:paper-conclusion}

\subsection{Future work}
