% Encoding: windows-1252

@MastersThesis{Abdulhamid2013,
  author =    {Abdulhamid, Fahmi},
  title =     {SpEx: A Tool for Visualising and Navigating Speech Audio},
  school =    {Victoria University of Wellington},
  year =      {2013},
  file =      {Abdulhamid2013.pdf:Abdulhamid2013.pdf:PDF},
  keywords =  {rank5},
  publisher = {Victoria University of Wellington}
}

@InProceedings{Abdulhamid2013a,
  author =    {Abdulhamid, Fahmi and Marshall, Stuart},
  title =     {Treemaps to Visualise and Navigate Speech Audio},
  booktitle = {Proceedings of the 25th Australian Computer-Human Interaction Conference: Augmentation, Application, Innovation, Collaboration},
  year =      {2013},
  series =    {OzCHI '13},
  pages =     {555--564},
  address =   {New York, NY, USA},
  publisher = {ACM},
  acmid =     {2541021},
  doi =       {10.1145/2541016.2541021},
  file =      {Abdulhamid2013a.pdf:Abdulhamid2013a.pdf:PDF},
  isbn =      {978-1-4503-2525-7},
  keywords =  {audio, browse, search, speech retrieval, speech visualisation, user interface, rank5},
  numpages =  {10}
}

@Misc{AdobeSystems2016,
  author =    {{Adobe Systems Inc.}},
  title =     {Let's Get Experimental: Behind the {Adobe MAX} Sneaks},
  month =     apr,
  year =      {2016},
  comment =   {Accessed 15/01/2018},
  owner =     {chrisbau},
  timestamp = {2018.01.15},
  url =       {https://theblog.adobe.com/lets-get-experimental-behind-the-adobe-max-sneaks/}
}

@Book{Aigner2011,
  title =     {Visualization of Time-Oriented Data},
  publisher = {Springer},
  year =      {2011},
  author =    {Wolfgang Aigner and Silvia Miksch and Heidrun Schumann and Christian Tominski},
  editor =    {John Karat and Jean Vanderdonckt},
  keywords =  {visualization, rank1},
  owner =     {Chris},
  status =    {read},
  timestamp = {2013.11.25}
}

@InProceedings{Akkermans2011,
  author =    {Akkermans, V. and Font, F. and Funollet, J. and de Jong, B. and Roma, G. and Togias, S. and Serra, X.},
  title =     {Freesound 2: An improved platform for sharing audio clips},
  booktitle = {Proceedings of the 12th International Society for Music Information Retrieval Conference (ISMIR)},
  year =      {2011},
  address =   {Miami, Florida, USA},
  month =     oct,
  file =      {Akkermans2011.pdf:Akkermans2011.pdf:PDF},
  owner =     {chrisbau},
  timestamp = {2017.12.04}
}

@Book{Albers1971,
  title =     {Interaction of Color},
  publisher = {Yale University Press},
  year =      {1971},
  author =    {Josef Albers},
  owner =     {Chris},
  timestamp = {2013.12.05}
}

@InProceedings{Amatriain2005,
  author =    {Amatriain, Xavier and Massaguer, Jordi and Garcia, David and Mosquera, Ismael},
  title =     {The CLAM Annotator: A Cross-Platform Audio Descriptors Editing Tool.},
  booktitle = {Proc. International Society for Music Information Retrieval},
  year =      {2005},
  pages =     {426--429},
  file =      {Amatriain2005.pdf:Amatriain2005.pdf:PDF},
  keywords =  {interface}
}

@Article{AngueraMiro2012,
  author =   {Anguera Miro, X. and Bozonnet, S. and Evans, N. and Fredouille, C. and Friedland, G. and Vinyals, O.},
  title =    {Speaker Diarization: A Review of Recent Research},
  journal =  {IEEE Transactions on Audio, Speech, and Language Processing},
  year =     {2012},
  volume =   {20},
  number =   {2},
  pages =    {356-370},
  abstract = {Speaker diarization is the task of determining "who spoke when?" in
	an audio or video recording that contains an unknown amount of speech
	and also an unknown number of speakers. Initially, it was proposed
	as a research topic related to automatic speech recognition, where
	speaker diarization serves as an upstream processing step. Over recent
	years, however, speaker diarization has become an important key technology
	for many tasks, such as navigation, retrieval, or higher level inference
	on audio data. Accordingly, many important improvements in accuracy
	and robustness have been reported in journals and conferences in
	the area. The application domains, from broadcast news, to lectures
	and meetings, vary greatly and pose different problems, such as having
	access to multiple microphones and multimodal information or overlapping
	speech. The most recent review of existing technology dates back
	to 2006 and focuses on the broadcast news domain. In this paper,
	we review the current state-of-the-art, focusing on research developed
	since 2006 that relates predominantly to speaker diarization for
	conference meetings. Finally, we present an analysis of speaker diarization
	performance as reported through the NIST Rich Transcription evaluations
	on meeting data and identify important areas for future research.},
  doi =      {10.1109/TASL.2011.2125954},
  file =     {AngueraMiro2012.pdf:AngueraMiro2012.pdf:PDF},
  issn =     {1558-7916},
  keywords = {audio recording, audio signal processing, information resources, speaker recognition, teleconferencing, television broadcasting, NIST Rich Transcription evaluations, audio data, audio recording, automatic speech recognition, broadcast news, conference meetings, multimodal information, speaker diarization, speech overlapping, upstream processing, video recording, Acoustics, Adaptation models, Data models, Microphones, NIST, Speech, Speech recognition, Meetings, rich transcription, speaker diarization, rank3},
  status =   {printed}
}

@InBook{Annett2000,
  chapter =   {Task and training requirements analysis methodology (TTRAM): An analytic methodology for identifying potential training uses of simulator networks in teamwork-intensive task environments},
  pages =     {150--169},
  title =     {Task Analysis},
  publisher = {CRC Press},
  year =      {2000},
  author =    {John Annett and Neville Anthony Stanton},
  file =      {Annett2000.pdf:Annett2000.pdf:PDF},
  owner =     {chrisbau},
  timestamp = {2017.11.16}
}

@InProceedings{Apperley2002,
  author =    {Apperley, Mark and Edwards, Orion and Jansen, Sam and Masoodian, Masood and McKoy, Sam and Rogers, Bill and Voyle, Tony and Ware, David},
  title =     {Application of Imperfect Speech Recognition to Navigation and Editing of Audio Documents},
  booktitle = {Proceedings of the SIGCHI-NZ Symposium on Computer-Human Interaction},
  year =      {2002},
  series =    {CHINZ '02},
  pages =     {97--102},
  address =   {New York, NY, USA},
  publisher = {ACM},
  acmid =     {2181233},
  doi =       {10.1145/2181216.2181233},
  file =      {Apperley2002.pdf:Apperley2002.pdf:PDF},
  isbn =      {0-473-08500-3},
  keywords =  {audio editing, audio navigation, large interactive display surface, speech recognition, rank5},
  numpages =  {6},
  review =    {LIDS editor}
}

@InProceedings{Arawjo2017,
  author =    {Arawjo, Ian and Yoon, Dongwook and Guimbreti\`{e}re, Fran\c{c}ois},
  title =     {TypeTalker: A Speech Synthesis-Based Multi-Modal Commenting System},
  booktitle = {Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
  year =      {2017},
  series =    {CSCW '17},
  pages =     {1970--1981},
  address =   {Portland, Oregon, USA},
  publisher = {ACM},
  acmid =     {2998260},
  doi =       {10.1145/2998181.2998260},
  file =      {Arawjo2017.pdf:Arawjo2017.pdf:PDF},
  isbn =      {978-1-4503-4335-0},
  keywords =  {automatic speech recognition, multi-modal comment, self-consciousness, speech comments, transcript-based speech editing, transcription error, rank5},
  numpages =  {12}
}

@InProceedings{Armitage2012,
  author =    {Jack Armitage and Kyle Molleson and Michael Battcock and Chris Earnshaw and David Moore and Kia Ng},
  title =     {Visualising Sound: Localisation, Feature Analysis and Visualisation},
  booktitle = {Proc. Electronic Visualisation and the Arts},
  year =      {2012},
  file =      {Armitage2012.pdf:Armitage2012.pdf:PDF},
  keywords =  {installation, visualization, rank2},
  owner =     {Chris},
  review =    {Describes an art installation which turns audio direction and pitch
	into a visualisation. Sections 1 and 2 contain lots of useful references
	and discussion.},
  status =    {printed, read},
  timestamp = {2013.11.21}
}

@Article{Arons1992,
  author =  {Barry Arons},
  title =   {A Review of The Cocktail Party Effect},
  journal = {Journal of the American Voice {I/O} Society},
  year =    {1992},
  volume =  {12},
  pages =   {35--50},
  file =    {Arons1992.pdf:Arons1992.pdf:PDF}
}

@Article{Arons1997,
  author =     {Arons, Barry},
  title =      {SpeechSkimmer: A System for Interactively Skimming Recorded Speech},
  journal =    {ACM Trans. Comput.-Hum. Interact.},
  year =       {1997},
  volume =     {4},
  number =     {1},
  pages =      {3--38},
  month =      mar,
  acmid =      {244758},
  address =    {New York, NY, USA},
  doi =        {10.1145/244754.244758},
  file =       {Arons1997.pdf:Arons1997.pdf:PDF},
  issn =       {1073-0516},
  issue_date = {March 1997},
  keywords =   {audio browsing, interactive listening, nonspeech audio, speech as data, speech skimming, speech user interfaces, time compression, interface, rank5},
  numpages =   {36},
  publisher =  {ACM},
  review =     {Seminal work. Presents a system for time compression and skimming
	(jumping to bits) of speech recordings with a physical user interface.
	Time compression techniques: - Pause removal - Pause shortening -
	Sampling method - 30-50ms segments are regularly dropped - Synchronized
	overlap add method (SOLA) - like sampling but segments are overlapped
	at varying amounts until overlap with highest cross-correlation is
	found - Dichotic sampling - speech is segmented, odd segments played
	in left ear, even in right Skimming techniques: - Isochronous skimming
	(equal time interval) - Segmentation based on: - Long pauses - Increased
	pitch (emphasis) - Energy - Word spotting - Speaker ID - User selected
	segments Order of speed increases: 1. Pause removal 2. Time compression
	3. Time compression and pause removal 4. Isochronous skimming 5.
	Segmentation},
  status =     {printed, read}
}

@InProceedings{Arora2014,
  author =    {Raman Arora and Karen Livescu},
  title =     {Multi-view Learning with Supervision for Transformed Bottleneck Features},
  booktitle = {Proc. IEEE International Conference on Acoustic, Speech and Signal Processing (ICASSP)},
  year =      {2014},
  abstract =  {Previous work has shown that acoustic features can be improved by
	unsupervised learning of transformations based on canonical correlation
	analysis (CCA) using articulatory measurements that are available
	at training time. In this paper, we investigate whether this second
	view (articulatory data) still helps even when labels are also available
	at training time. We begin with strong baseline bottleneck features,
	which can be learned when the training set is phonetically labeled.
	We then compare several options for learning transformations of the
	bottleneck features in the presence of both articulatory measurements
	and phonetic labels for the training data. The methods compared include
	combinations of LDA and CCA, as well as a three-view extension of
	CCA that simultaneously uses the labels and articulatory measurements
	as additional views. Phonetic recognition experiments on data from
	the University of Wisconsin X-ray microbeam database show that the
	learned features improve performance over using either just the labels
	or just the articulatory measurements for learning acoustic transformations.},
  file =      {Arora2014.pdf:Arora2014.pdf:PDF},
  keywords =  {rank1},
  owner =     {Chris},
  review =    {Using CCA to take a set of features and reduce them to a smaller set
	(bottlenecking).},
  status =    {printed},
  timestamp = {2014.05.28}
}

@Misc{AskAudio2015,
  author =    {{Ask Audio}},
  title =     {The Top 11 Most Popular DAWs (You Voted For)},
  month =     nov,
  year =      {2015},
  comment =   {Accessed 23/1/2017.},
  keywords =  {rank1},
  owner =     {chrisbau},
  review =    {"One question was about your primary preferred DAW. Almost 25,000
	musicians and producers gave their answer."},
  timestamp = {2017.02.23},
  url =       {https://ask.audio/articles/the-top-11-most-popular-daws-you-voted-for}
}

@InProceedings{Avdelidis2007,
  author =    {Avdelidis, Kostantinos A. and Dimoulas, Charalampos A. and Kalliris, George M. and Papanikolaou, George V. and Vegiris, Christos},
  title =     {Automated Audio Detection, Segmentation and Indexing, with Application to Post-Production Editing},
  booktitle = {Proc. 122nd Audio Engineering Society Convention},
  year =      {2007},
  month =     may,
  file =      {Avdelidis2007.pdf:Avdelidis2007.pdf:PDF},
  keywords =  {rank4},
  owner =     {chrisbau},
  timestamp = {2017.11.03},
  url =       {http://www.aes.org/e-lib/browse.cfm?elib=14123}
}

@Manual{Avid2011,
  title =     {User manual: ``Getting Started with {ScriptSync}''},
  author =    {{Avid Technology Inc.}},
  year =      {2011},
  note =      {Accessed 15.08.16},
  owner =     {chrisbau},
  timestamp = {2016.08.15},
  url =       {http://resources.avid.com/SupportFiles/attach/GettingStartedScriptSync_v6.pdf}
}

@Misc{Avid2017,
  author =    {{Avid Technology Inc.}},
  title =     {{Media Composer}: {ScriptSync} Option},
  year =      {2017},
  note =      {Accessed 29.11.17},
  owner =     {chrisbau},
  timestamp = {2017.11.29},
  url =       {http://www.avid.com/products/media-composer-scriptsync-option}
}

@Article{Bangor2008,
  author =    {Aaron Bangor and Philip T. Kortum and James T. Miller},
  title =     {An Empirical Evaluation of the System Usability Scale},
  journal =   {International Journal of Humanâ€“Computer Interaction},
  year =      {2008},
  volume =    {24},
  number =    {6},
  pages =     {574-594},
  abstract =  { This article presents nearly 10 year's worth of System Usability
	Scale (SUS) data collected on numerous products in all phases of
	the development lifecycle. The SUS, developed by Brooke (1996), reflected
	a strong need in the usability community for a tool that could quickly
	and easily collect a user's subjective rating of a product's usability.
	The data in this study indicate that the SUS fulfills that need.
	Results from the analysis of this large number of SUS scores show
	that the SUS is a highly robust and versatile tool for usability
	professionals. The article presents these results and discusses their
	implications, describes nontraditional uses of the SUS, explains
	a proposed modification to the SUS to provide an adjective rating
	that correlates with a given score, and provides details of what
	constitutes an acceptable SUS score. },
  doi =       {10.1080/10447310802205776},
  eprint =    { http://dx.doi.org/10.1080/10447310802205776 },
  file =      {Bangor2008.pdf:Bangor2008.pdf:PDF},
  keywords =  {rank4},
  publisher = {Taylor \& Francis}
}

@InProceedings{Barbour2004,
  author =    {Jim Barbour},
  title =     {Analytic Listening: {A} Case Study of Radio Production},
  booktitle = {Proc. International Conference on Auditory Display},
  year =      {2004},
  address =   {Sydney, Australia},
  file =      {Barbour2004.pdf:Barbour2004.pdf:PDF},
  keywords =  {rank5},
  timestamp = {Thu, 07 Oct 2004 14:51:52 +0200},
  url =       {http://www.icad.org/websiteV2.0/Conferences/ICAD2004/papers/barbour.pdf}
}

@Article{Barras2001,
  author =    {Barras, Claude and Geoffrois, Edouard and Wu, Zhibiao and Liberman, Mark},
  title =     {Transcriber: development and use of a tool for assisting speech corpora production},
  journal =   {Speech Communication},
  year =      {2001},
  volume =    {33},
  number =    {1},
  pages =     {5--22},
  file =      {Barras2001.pdf:Barras2001.pdf:PDF},
  keywords =  {rank2},
  publisher = {Elsevier}
}

@Article{Baume2016a,
  author =    {Chris Baume and Werner Bleisteiner},
  title =     {Requirements, designs and workflows of an object-based production environment},
  journal =   {{ORPHEUS} project deliverable},
  year =      {2016},
  doi =       {10.5281/zenodo.844007},
  file =      {Baume2016a.pdf:Baume2016a.pdf:PDF},
  owner =     {chrisbau},
  timestamp = {2017.12.15}
}

@InProceedings{Baume2015,
  author =    {Chris Baume and Mark D. Plumbley and Janko \'{C}ali\'{c}},
  title =     {Use of audio editors in radio production},
  booktitle = {Proc. 138th Audio Engineering Society Convention},
  year =      {2015},
  file =      {Baume2015.pdf:Baume2015.pdf:PDF},
  keywords =  {rank5},
  owner =     {chrisbau},
  timestamp = {2015.04.10},
  url =       {http://www.aes.org/e-lib/browse.cfm?elib=17661}
}

@Article{Baume2018a,
  author =    {Chris Baume and Mark D. Plumbley and Janko \'{C}ali\'{c} and David Frohlich},
  title =     {A Contextual Study of Semantic Speech Editing in Radio Production},
  journal =   {International Journal of Human-Computer Studies},
  year =      {2018},
  note =      {In press},
  keywords =  {rank5},
  owner =     {chrisbau},
  timestamp = {2018.01.16}
}

@Article{Baume2018,
  author =    {Chris Baume and Mark D. Plumbley and David Frohlich and Janko \'{C}ali\'{c}},
  title =     {PaperClip: A Digital Pen Interface for Semantic Speech Editing in Radio Production},
  journal =   {Journal of the Audio Engineering Society},
  year =      {2018},
  note =      {In press},
  doi =       {10.17743/jaes.2018.0006},
  keywords =  {rank5},
  owner =     {chrisbau},
  timestamp = {2018.01.02}
}

@Misc{BBC2015,
  author =    {BBC},
  title =     {History of the BBC factsheets},
  year =      {2015},
  note =      {Accessed 20/01/2018},
  owner =     {chrisbau},
  timestamp = {2018.01.20},
  url =       {http://www.bbc.co.uk/historyofthebbc/resources/fact-sheets}
}

@Misc{BBC2015a,
  author =    {BBC},
  title =     {History of the {BBC}: Broadcasting House},
  year =      {2015},
  note =      {Accessed 20/01/2018},
  owner =     {chrisbau},
  timestamp = {2018.01.20},
  url =       {http://www.bbc.co.uk/historyofthebbc/buildings/broadcasting-house}
}

@Misc{BBC2017,
  author =       {BBC},
  title =        {BBC's global audience rises to 372m},
  howpublished = {BBC Media Centre},
  month =        may,
  year =         {2017},
  owner =        {chrisbau},
  timestamp =    {2017.12.16},
  url =          {http://www.bbc.co.uk/mediacentre/latestnews/2017/global-audience-measure}
}

@Misc{BBC2017a,
  author =    {BBC},
  title =     {Annual Report and Accounts 2016/17},
  year =      {2017},
  file =      {BBC2017a.pdf:BBC2017a.pdf:PDF},
  owner =     {chrisbau},
  timestamp = {2018.01.16},
  url =       {http://downloads.bbc.co.uk/aboutthebbc/insidethebbc/reports/pdf/bbc-annualreport-201617.pdf}
}

@Misc{BBCCharter2016,
  author =    {{BBC Charter}},
  title =     {Royal charter for the continuance of the {British Broadcasting Corporation}},
  month =     dec,
  year =      {2016},
  file =      {BBCCharter2016.pdf:BBCCharter2016.pdf:PDF},
  isbn =      {9781474138918},
  owner =     {chrisbau},
  timestamp = {2017.12.16},
  url =       {http://downloads.bbc.co.uk/bbctrust/assets/files/pdf/about/how_we_govern/2016/charter.pdf}
}

@Misc{BBCNews2013,
  author =    {{BBC News}},
  title =     {Queen officially opens {BBC}'s new {B}roadcasting {H}ouse building},
  month =     jun,
  year =      {2013},
  owner =     {chrisbau},
  timestamp = {2018.01.20},
  url =       {http://www.bbc.com/news/uk-22804844}
}

@Misc{BBCWorldwide2017,
  author =    {{BBC Worldwide}},
  title =     {Annual Review 2016--2017},
  year =      {2017},
  owner =     {chrisbau},
  timestamp = {2018.01.20},
  url =       {https://www.bbcworldwide.com/media/2176/bbcw_annual_report_16-17.pdf}
}

@InProceedings{Bell2015,
  author =    {P. Bell and M. J. F. Gales and T. Hain and J. Kilgour and P. Lanchantin and X. Liu and A. McParland and S. Renals and O. Saz and M. Wester and P. C. Woodland},
  title =     {The MGB challenge: Evaluating multi-genre broadcast media recognition},
  booktitle = {Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)},
  year =      {2015},
  pages =     {687-693},
  month =     dec,
  abstract =  {This paper describes the Multi-Genre Broadcast (MGB) Challenge at
	ASRU 2015, an evaluation focused on speech recognition, speaker diarization,
	and "lightly supervised" alignment of BBC TV recordings. The challenge
	training data covered the whole range of seven weeks BBC TV output
	across four channels, resulting in about 1,600 hours of broadcast
	audio. In addition several hundred million words of BBC subtitle
	text was provided for language modelling. A novel aspect of the evaluation
	was the exploration of speech recognition and speaker diarization
	in a longitudinal setting - i.e. recognition of several episodes
	of the same show, and speaker diarization across these episodes,
	linking speakers. The longitudinal tasks also offered the opportunity
	for systems to make use of supplied metadata including show title,
	genre tag, and date/time of transmission. This paper describes the
	task data and evaluation process used in the MGB challenge, and summarises
	the results obtained.},
  doi =       {10.1109/ASRU.2015.7404863},
  file =      {Bell2015.pdf:Bell2015.pdf:PDF},
  keywords =  {audio signal processing, meta data, speech processing, speech recognition, television broadcasting, ASRU 2015, BBC TV recordings, BBC subtitle text, MGB challenge, broadcast audio, language modelling, lightly supervised recordings alignment, metadata, multigenre broadcast media recognition, speaker diarization, speech recognition, Metadata, Speech, Speech recognition, TV, Training, Training data, Speech recognition, alignment, broadcast speech, diarization, longitudinal, multi-genre, transcription, rank4},
  review =    {Speech-to-text accuracy}
}

@Article{Bendel2017,
  author =    {Bendel, Oliver},
  title =     {The synthetization of human voices},
  journal =   {{AI \& SOCIETY}},
  year =      {2017},
  month =     jul,
  abstract =  {The synthetization of voices, or speech synthesis, has been an object of interest for centuries. It is mostly realized with a text-to-speech system, an automaton that interprets and reads aloud. This system refers to text available for instance on a website or in a book, or entered via popup menu on the website. Today, just a few minutes of samples are enough to be able to imitate a speaker convincingly in all kinds of statements. This article abstracts from actual products and actual technological realization. Rather, after a short historical outline of the synthetization of voices, exemplary applications of this kind of technology are gathered for promoting the development, and potential applications are discussed critically to be able to limit them if necessary. The ethical and legal challenges should not be underestimated, in particular with regard to informational and personal autonomy and the trustworthiness of media.},
  day =       {26},
  doi =       {10.1007/s00146-017-0748-x},
  issn =      {1435-5655},
  owner =     {chrisbau},
  timestamp = {2018.01.15}
}

@Article{Berthouzoz2012,
  author =     {Berthouzoz, Floraine and Li, Wilmot and Agrawala, Maneesh},
  title =      {Tools for Placing Cuts and Transitions in Interview Video},
  journal =    {ACM Trans. Graph.},
  year =       {2012},
  volume =     {31},
  number =     {4},
  pages =      {67:1--67:8},
  month =      jul,
  abstract =   {We present a set of tools designed to help editors place cuts and
	create transitions in interview video. To help place cuts, our interface
	links a text transcript of the video to the corresponding locations
	in the raw footage. It also visualizes the suitability of cut locations
	by analyzing the audio/visual features of the raw footage to find
	frames where the speaker is relatively quiet and still. With these
	tools editors can directly highlight segments of text, check if the
	endpoints are suitable cut locations and if so, simply delete the
	text to make the edit. For each cut our system generates visible
	(e.g. jump-cut, fade, etc.) and seamless, hidden transitions. We
	present a hierarchical, graph-based algorithm for efficiently generating
	hidden transitions that considers visual features specific to interview
	footage. We also describe a new data-driven technique for setting
	the timing of the hidden transition. Finally, our tools offer a one
	click method for seamlessly removing 'ums' and repeated words as
	well as inserting natural-looking pauses to emphasize semantic content.
	We apply our tools to edit a variety of interviews and also show
	how they can be used to quickly compose multiple takes of an actor
	narrating a story.},
  acmid =      {2185563},
  address =    {New York, NY, USA},
  articleno =  {67},
  doi =        {10.1145/2185520.2185563},
  file =       {Berthouzoz2012.pdf:Berthouzoz2012.pdf:PDF},
  issn =       {0730-0301},
  issue_date = {July 2012},
  keywords =   {human-computer interfaces, interaction, rank4},
  numpages =   {8},
  publisher =  {ACM}
}

@Unpublished{Boas2011,
  author =    {Mark Boas},
  title =     {Blog post: ``The {Hyperaudio Pad - A} Software Product Proposal''},
  note =      {Accessed 15.08.16},
  month =     aug,
  year =      {2011},
  keywords =  {rank3},
  owner =     {chrisbau},
  timestamp = {2015.02.10},
  url =       {http://happyworm.com/blog/2011/08/08/the-hyperaudio-pad-a-software-product-proposal/}
}

@Article{Boczkowski2004,
  author =    {Boczkowski, Pablo J.},
  title =     {The Processes of Adopting Multimedia and Interactivity in Three Online Newsrooms},
  journal =   {Journal of Communication},
  year =      {2004},
  volume =    {54},
  number =    {2},
  pages =     {197--213},
  abstract =  {This article examines the material culture of newsroom practices by
	focusing on the dynamics of the processes through which news workers
	adopt new technologies. More specifically, it looks at some key factors
	that shape the adoption of multimedia and interactive technologies
	in online newspapers. Through ethnographic case studies of innovations
	in 3 online newsrooms, I show that variations in organizational structures,
	work practices, and representations of the users are related to different
	ways in which members of the newsroom appropriate these technologies.
	I draw from this analysis to reflect on issues related to the technological
	dimension of editorial work and the dynamics of media convergence.},
  doi =       {10.1111/j.1460-2466.2004.tb02624.x},
  file =      {Boczkowski2004.pdf:Boczkowski2004.pdf:PDF},
  issn =      {1460-2466},
  owner =     {chrisbau},
  publisher = {Blackwell Publishing Ltd},
  review =    {Ethnographic study of newsroom},
  timestamp = {2017.11.14}
}

@InCollection{Bohac2013,
  author =    {Boh\'{a}\v{c}, Marek and Blavka, Karel},
  title =     {Text-to-Speech Alignment for Imperfect Transcriptions},
  booktitle = {Text, Speech, and Dialogue},
  publisher = {Springer},
  year =      {2013},
  volume =    {8082},
  series =    {Lecture Notes in Computer Science},
  pages =     {536-543},
  doi =       {10.1007/978-3-642-40585-3_67},
  file =      {Bohac2013.pdf:Bohac2013.pdf:PDF},
  isbn =      {978-3-642-40584-6},
  keywords =  {unsupervised, text-to-speech alignment, inaccurate transcription, automatic speech recognition, rank3}
}

@InProceedings{Boogaart2006,
  author =    {Boogaart, C. G. v. d. and Lienhart, R.},
  title =     {Audio Brush: A Tool for Computer-assisted Smart Audio Editing},
  booktitle = {Proceedings of the 1st ACM Workshop on Audio and Music Computing Multimedia},
  year =      {2006},
  series =    {AMCMM '06},
  pages =     {115--124},
  address =   {Santa Barbara, CA, USA},
  publisher = {ACM},
  abstract =  {Starting with a novel audio analysis and editing paradigm, a set of
	new and adaptive audio analysis and editing algorithms in the spectrogram
	are developed and integrated into a smart visual audio editing tool
	in a "what you see is what you hear" style. At the core of our algorithms
	and methods is a very?exible audio spectrogram that goes beyond FFT
	and Wavelets and supports manipulating a signal at any chosen time-frequency
	resolution:the Gabor analysis and synthesis. It gives maximum accuracy
	of the representation, is fully invertible, and enables resolution
	zooming. Simple audio objects are localized in time and frequency.
	They can easily be identified visually and selected by simple geometric
	selection masks such as rectangles, combs and polygons. For many
	audio objects, however the structures in the spectrogram are rather
	complex. Therefore, we present several intelligent and adaptive mask
	selection approaches. They are based on audio fingerprinting and
	visual pattern matching algorithms. Spectrograms of individually
	recorded sounds under controlled conditions or interactively selected
	in the current spectrogram can be regarded as visual and sophisticated
	templates. We discuss how to generate templates, how to find the
	best match out of a database and how to adapt the match to the sound
	which we want to edit.},
  acmid =     {1178741},
  doi =       {10.1145/1178723.1178741},
  file =      {Boogaart2006.pdf:Boogaart2006.pdf:PDF},
  isbn =      {1-59593-501-0},
  keywords =  {audio fingerprinting, audio objects, gabor analysis, visual audio editing, interface, rank2},
  numpages =  {10},
  review =    {Editing spectrograms like photoshop}
}

@InProceedings{Borodin2010,
  author =    {Borodin, Yevgen and Bigham, Jeffrey P. and Dausch, Glenn and Ramakrishnan, I. V.},
  title =     {More Than Meets the Eye: A Survey of Screen-reader Browsing Strategies},
  booktitle = {Proceedings of the 2010 International Cross Disciplinary Conference on Web Accessibility (W4A)},
  year =      {2010},
  series =    {W4A '10},
  pages =     {13:1--13:10},
  address =   {Raleigh, North Carolina},
  publisher = {ACM},
  abstract =  {Browsing the Web with screen readers can be difficult and frustrating.
	Web pages often contain inaccessible content that is expressed only
	visually or that can be accessed only with the mouse. Screen-reader
	users must also contend with usability challenges encountered when
	the reading content is designed with built-in assumptions of how
	it will be accessed -- generally by a sighted person on a standard
	display. Far from passive consumers of content who simply accept
	web content as accessible or not, many screen-reader users are adept
	at developing, discovering, and employing browsing strategies that
	help them overcome the accessibility and usability problems they
	encounter. In this paper, we overview the browsing strategies that
	we have observed screen-reader users employ when faced with challenges,
	ranging from unfamiliar web sites and complex web pages to dynamic
	and automatically-refreshing content. A better understanding of existing
	browsing strategies can inform the design of accessible websites,
	development of new tools that make experienced users more effective,
	and help overcome the initial learning curve for users who have not
	yet acquired effective browsing strategies.},
  acmid =     {1806005},
  articleno = {13},
  doi =       {10.1145/1805986.1806005},
  file =      {Borodin2010.pdf:Borodin2010.pdf:PDF},
  isbn =      {978-1-4503-0045-2},
  keywords =  {eval, rank1},
  numpages =  {10},
  review =    {Expert screenreader users use a high speech rate and older formative
	speech synthesisers. Very experienced users can understand 500 words
	per minute.}
}

@Article{Bouamrane2007,
  author =    {Bouamrane, Matt-M. and Luz, Saturnino},
  title =     {Meeting browsing},
  journal =   {Multimedia Systems},
  year =      {2007},
  volume =    {12},
  number =    {4-5},
  pages =     {439-457},
  doi =       {10.1007/s00530-006-0066-5},
  file =      {Bouamrane2007.pdf:Bouamrane2007.pdf:PDF},
  issn =      {0942-4962},
  keywords =  {Multimedia segmentation, Indexing and retrieval, Multimodal meeting browsers, rank5},
  language =  {English},
  publisher = {Springer-Verlag}
}

@Article{Braun2006,
  author =  {Virginia Braun and Victoria Clarke},
  title =   {Using thematic analysis in psychology},
  journal = {Qualitative Research in Psychology},
  year =    {2006},
  volume =  {3},
  number =  {2},
  pages =   {77-101},
  doi =     {10.1191/1478088706qp063oa},
  eprint =  { http://www.tandfonline.com/doi/pdf/10.1191/1478088706qp063oa }
}

@InProceedings{Brixen2003,
  author =    {Brixen, Eddy B.},
  title =     {Audio Production in Large Office Environments},
  booktitle = {Proc. 115th Audio Engineering Society Convention},
  year =      {2003},
  month =     oct,
  file =      {Brixen2003.pdf:Brixen2003.pdf:PDF},
  keywords =  {rank5},
  owner =     {chrisbau},
  timestamp = {2017.11.03},
  url =       {http://www.aes.org/e-lib/browse.cfm?elib=12467}
}

@InBook{Brooke1996,
  chapter =   {SUS: A `quick and dirty' usability scale},
  pages =     {189--206},
  title =     {Usability Evaluation In Industry},
  publisher = {{Taylor \& Francis}},
  year =      {1996},
  author =    {Brooke, John},
  journal =   {Usability evaluation in industry}
}

@InProceedings{Burke2006,
  author =    {Burke, Moira and Amento, Brian and Isenhour, Philip},
  title =     {Error Correction of Voicemail Transcripts in SCANMail},
  booktitle = {Proc. SIGCHI Conference on Human Factors in Computing Systems},
  year =      {2006},
  series =    {CHI '06},
  pages =     {339--348},
  address =   {Montr\'{e}al, Qu\'{e}bec, Canada},
  publisher = {ACM},
  abstract =  {Despite its widespread use, voicemail presents numerous usability
	challenges: People must listen to messages in their entirety, they
	cannot search by keywords, and audio files do not naturally support
	visual skimming. SCANMail overcomes these flaws by automatically
	generating text transcripts of voicemail messages and presenting
	them in an email-like interface. Transcripts facilitate quick browsing
	and permanent archive. However, errors from the automatic speech
	recognition (ASR) hinder the usefulness of the transcripts. The work
	presented here specifically addresses these problems by evaluating
	user-initiated error correction of transcripts. User studies of two
	editor interfaces - a grammar-assisted menu and simple replacement
	by typing - reveal reduced audio playback times and an emphasis on
	editing important words with the menu, suggesting its value in mobile
	environments where limited input capabilities are the norm and user
	privacy is essential. The study also adds to the scarce body of work
	on ASR confidence shading, suggesting that shading may be more helpful
	than previously reported.},
  acmid =     {1124823},
  doi =       {10.1145/1124772.1124823},
  file =      {Burke2006.pdf:Burke2006.pdf:PDF},
  isbn =      {1-59593-372-7},
  keywords =  {confidence shading, editor interfaces, error correction, speech recognition, voicemail, rank5},
  numpages =  {10},
  review =    {ASR user error correction and confidence shading.
	
	Speed increase of confidence shading was not tested, but shading was
	considered important by users in a questionnaire: "Previous studies
	have suggested confidence shading does not improve time to locate
	errors [17, 6], but participants in the present study agreed with
	the statement "the gray- colored text was helpful for identifying
	mistakes in the transcripts" (4.06 with 5=completely agree, One sample
	t- test, p<.001)."}
}

@Article{Cabral2016,
  author =   {Cabral, Diogo and Correia, Nuno},
  title =    {Video editing with pen-based technology},
  journal =  {Multimedia Tools and Applications},
  year =     {2016},
  pages =    {1--26},
  abstract = {The manipulation of video content is still a difficult task due to
	its complexity and richness. This paper applies pen-based technology
	to video editing, with the goal to improve such interaction. In this
	research, digital ink is replaced by video content, aiming to provide
	a more familiar and creative interaction for video editing and to
	study how pen gestures can be used on this context as well as what
	kind of changes are needed in the interface. The concept was implemented
	in a Tablet PC prototype and evaluated by expert and non-expert users.
	The user feedback shows that this approach proved to be natural and
	at the same time to foster user creativity as measured by the Creative
	Support Index.},
  doi =      {10.1007/s11042-016-3329-y},
  file =     {Cabral2016.pdf:Cabral2016.pdf:PDF},
  issn =     {1573-7721},
  keywords = {rank4}
}

@InProceedings{Cannam2010,
  author =    {C. Cannam and C. Landone and M. Sandler},
  title =     {Sonic Visualiser: An Open Source Application for Viewing, Analysing, and Annotating Music Audio Files},
  booktitle = {Proceedings of the ACM Multimedia 2010 International Conference},
  year =      {2010},
  pages =     {1467--1468},
  address =   {Firenze, Italy},
  month =     oct,
  file =      {Cannam2010.pdf:Cannam2010.pdf:PDF},
  keywords =  {rank2}
}

@InProceedings{Carey1999,
  author =    {Carey, M.J. and Parris, E.S. and Lloyd-Thomas, H.},
  title =     {A comparison of features for speech, music discrimination},
  booktitle = {Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing},
  year =      {1999},
  volume =    {1},
  pages =     {149-152 vol.1},
  abstract =  {Several approaches have previously been taken to the problem of discriminating
	between speech and music signals. These have used different features
	as the input to the classifier and have tested and trained on different
	material. In this paper we examine the discrimination achieved by
	several different features using common training and test sets and
	the same classifier. The database assembled for these tests includes
	speech from thirteen languages and music from all over the world.
	In each case the distributions in the feature space were modelled
	by a Gaussian mixture model. Experiments were carried out on four
	types of feature, amplitude, cepstra, pitch and zero-crossings. In
	each case the derivative of the feature was also used and found to
	improve performance. The best performance resulted from using the
	cepstra and delta cepstra which gave an equal error rate (EER) of
	1.28. This was closely followed by normalised amplitude and delta
	amplitude. This however used a much less complex model. The pitch
	and delta pitch gave an EER of 4% which was better than the zero-crossing
	which produced an EER of 6%},
  doi =       {10.1109/ICASSP.1999.758084},
  file =      {Carey1999.pdf:Carey1999.pdf:PDF},
  issn =      {1520-6149},
  keywords =  {Gaussian processes, audio signal processing, cepstral analysis, error statistics, music, signal classification, speech recognition, EER, Gaussian mixture model, amplitude, cepstra, classifier, delta amplitude, delta cepstra, equal error rate, feature, music discrimination, normalised amplitude, pitch, speech discrimination, zero-crossings, Amplitude estimation, Assembly, Bandwidth, Cepstral analysis, Frequency estimation, Materials testing, Multiple signal classification, Natural languages, Spatial databases, Speech, SMD, rank3},
  review =    {Varience of amplitude and varience of delta amplitude perform well
	for SMD. Doesn't find ZCR to be a good discriminator. Delta cepstral
	features perform best but have highest performance requirements.},
  status =    {printed, read}
}

@InProceedings{Carter2013,
  author =    {Rebecca Carter and Thomas McKenzie and Lawrence Sarkar and Kia Ng},
  title =     {From Seeing to Feeling Sound: A Multimodal Interface for Trans-Domain Mapping of Sound},
  booktitle = {Proc. Electronic Visualisation and the Arts},
  year =      {2013},
  file =      {Carter2013.pdf:Carter2013.pdf:PDF},
  keywords =  {multimodal, installation, rank2},
  owner =     {Chris},
  review =    {Simple visual and haptic interface to indicate pitch similarity.},
  status =    {printed, read},
  timestamp = {2013.11.21}
}

@InProceedings{Casares2002,
  author =    {Casares, Juan and Long, A. Chris and Myers, Brad A. and Bhatnagar, Rishi and Stevens, Scott M. and Dabbish, Laura and Yocum, Dan and Corbett, Albert},
  title =     {Simplifying Video Editing Using Metadata},
  booktitle = {Proceedings of the 4th Conference on Designing Interactive Systems: Processes, Practices, Methods, and Techniques},
  year =      {2002},
  series =    {DIS '02},
  pages =     {157--166},
  address =   {London, England},
  publisher = {ACM},
  abstract =  {Digital video is becoming increasingly ubiquitous. However, editing
	video remains difficult for several reasons: it is a time-based medium,
	it has dual tracks of audio and video, and current tools force users
	to work at the smallest level of detail. Based on interviews with
	professional video editors, we developed a video editor, called Silver,
	that uses metadata to make digital video editing more accessible
	to novices. To help users visualize video, Silver provides multiple
	views with different semantic content and at different levels of
	abstraction, including storyboard, editable transcript, and timeline
	views. Silver offers smart editing operations that help users resolve
	the inconsistencies that arise because of the different boundaries
	in audio and video. We conducted a preliminary user study to investigate
	the effectiveness of the Silver smart editing. Participants successfully
	edited video after only a short tutorial, both with and without smart
	editing assistance. Our research suggests several ways in which video
	editing tools could use metadata to assist users in the reuse and
	composition of video.},
  acmid =     {778737},
  doi =       {10.1145/778712.778737},
  file =      {Casares2002.pdf:Casares2002.pdf:PDF},
  isbn =      {1-58113-515-7},
  keywords =  {Informedia., Silver, digital video editing, metadata, multimedia authoring, rank5},
  numpages =  {10}
}

@Article{Cattelan2008,
  author =     {Cattelan, Renan G. and Teixeira, Cesar and Goularte, Rudinei and Pimentel, Maria Da Gra\c{c}a C.},
  title =      {Watch-and-comment As a Paradigm Toward Ubiquitous Interactive Video Editing},
  journal =    {ACM Trans. Multimedia Comput. Commun. Appl.},
  year =       {2008},
  volume =     {4},
  number =     {4},
  pages =      {28:1--28:24},
  month =      nov,
  acmid =      {1412201},
  address =    {New York, NY, USA},
  articleno =  {28},
  doi =        {10.1145/1412196.1412201},
  file =       {Cattelan2008.pdf:Cattelan2008.pdf:PDF},
  issn =       {1551-6857},
  issue_date = {October 2008},
  keywords =   {Annotation, Ginga-NCL, P2P collaboration, interactive digital video, rank3},
  numpages =   {24},
  publisher =  {ACM},
  review =     {WaCTool captures ink and voice annotations using a tablet. Can draw
	freeform ink annotations on video frames. Can create a skip operation
	by tapping at in and out points.}
}

@Article{Chandrasegaran2017,
  author =   {Senthil Chandrasegaran and Sriram Karthik Badam and Lorraine Kisselburgh and Kylie Peppler and Niklas Elmqvist and Karthik Ramani},
  title =    {VizScribe: A visual analytics approach to understand designer behavior},
  journal =  {International Journal of Human-Computer Studies},
  year =     {2017},
  volume =   {100},
  pages =    {66 - 80},
  abstract = {Abstract Design protocol analysis is a technique to understand designers'
	cognitive processes by analyzing sequences of observations on their
	behavior. These observations typically use audio, video, and transcript
	data in order to gain insights into the designer's behavior and the
	design process. The recent availability of sophisticated sensing
	technology has made such data highly multimodal, requiring more flexible
	protocol analysis tools. To address this need, we present VizScribe,
	a visual analytics framework that employs multiple coordinated multiple
	views that enable the viewing of such data from different perspectives.
	VizScribe allows designers to create, customize, and extend interactive
	visualizations for design protocol data such as video, transcripts,
	sketches, sensor data, and user logs. User studies where design researchers
	used VizScribe for protocol analysis indicated that the linked views
	and interactive navigation offered by VizScribe afforded the researchers
	multiple, useful ways to approach and interpret such multimodal data.
	},
  doi =      {10.1016/j.ijhcs.2016.12.007},
  file =     {Chandrasegaran2017.pdf:Chandrasegaran2017.pdf:PDF},
  issn =     {1071-5819},
  keywords = {Protocol analysis, rank4},
  review =   {Web-based tool for annotating video with interactive transcript, sketch,
	sensor data and word cloud. Codes can be applied to selected parts
	of the transcript.}
}

@Article{Chang2013,
  author =   {Po-Chun Chang and Shuo-Yan Chou and Kong-King Shieh},
  title =    {Reading performance and visual fatigue when using electronic paper displays in long-duration reading tasks under various lighting conditions},
  journal =  {Displays},
  year =     {2013},
  volume =   {34},
  number =   {3},
  pages =    {208 - 214},
  abstract = {Abstract In this study, the effects of ambient illuminance and light
	source on participants reading performance and visual fatigue during
	a long reading task were investigated using three electronic paper
	displays. Reading on electronic paper displays was also compared
	with reading on paper. In Experiment 1, 100 participants performed
	a reading task where the display area for the text was equated for
	the displays. The results indicated that participants visual performance
	and visual fatigue did not differ significantly among different electronic
	paper displays, ambient illuminance conditions, or light sources.
	In Experiment 2, another 60 participants performed the same reading
	task where the full screen of each electronic paper display was used
	to present the text. The results showed that reading speed differed
	significantly across different electronic paper displays and ambient
	illuminance levels. The reading speed was slower for displays with
	smaller screens and increased as the ambient illuminance increased.
	Changes in the critical flicker fusion frequency significantly differed
	across ambient illuminance levels. Implications of the results for
	the use of electronic paper displays are discussed.},
  doi =      {10.1016/j.displa.2013.06.001},
  file =     {Chang2013.pdf:Chang2013.pdf:PDF},
  issn =     {0141-9382},
  keywords = {Electronic paper display}
}

@InProceedings{Chaudhuri2009,
  author =    {Kamalika Chaudhuri},
  title =     {Multi-View Clustering via Canonical Correlation Analysis},
  booktitle = {Proceedings of the 26th International Conference on Machine Learning},
  year =      {2009},
  file =      {Chaudhuri2009.pdf:Chaudhuri2009.pdf:PDF},
  owner =     {Chris},
  timestamp = {2014.07.15}
}

@InProceedings{Choi2000,
  author =    {Choi, Freddy Y. Y.},
  title =     {Advances in Domain Independent Linear Text Segmentation},
  booktitle = {Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference},
  year =      {2000},
  series =    {NAACL 2000},
  pages =     {26--33},
  address =   {Seattle, Washington, USA},
  publisher = {Association for Computational Linguistics},
  acmid =     {974309},
  file =      {Choi2000.pdf:Choi2000.pdf:PDF},
  keywords =  {rank3},
  numpages =  {8},
  url =       {http://dl.acm.org/citation.cfm?id=974305.974309}
}

@InProceedings{Christel2008,
  author =    {Christel, Michael G. and Lin, Wei-Hao and Maher, Bryan},
  title =     {Evaluating Audio Skimming and Frame Rate Acceleration for Summarizing BBC Rushes},
  booktitle = {Proceedings of the 2008 International Conference on Content-based Image and Video Retrieval},
  year =      {2008},
  series =    {CIVR '08},
  pages =     {407--416},
  address =   {Niagara Falls, Canada},
  publisher = {ACM},
  abstract =  {For the first time in 2007, TRECVID considered structured evaluation
	of automated video summarization, utilizing BBC rushes video. In
	2007, we conducted user evaluations with the published TRECVID summary
	assessment procedure to rate a cluster method for producing summaries,
	a 25x (sampling every 25th frame), and pz (emphasizing pans and zooms).
	Data from 4 human assessors shows significant differences between
	the cluster, pz, and 25x approaches. The best coverage (text inclusion
	performance) is obtained by 25x, but at the expense of 25x taking
	the most time to evaluate and judged as being the most redundant.
	Method pz was easier to use than cluster and rated best on redundancy.
	A question following the TRECVID workshop was whether simple speed-ups
	would still work at 50x or 100x, leading to a study with 15 human
	assessors looking at pzA (pz but with better audio), 25x, 50x, and
	100x summaries (these latter 3 with an unsynchronized more comprehensive
	audio track as well). 100x gives the fastest time on task but with
	poor usability and performance. PzA gives the best usability measures
	but poor time on task and performance. 25x does well on performance
	as before, with 50x doing just as well but with much less time on
	task and better ease of use and redundancy scores. Based on these
	results, 50x with its audio skimming is recommended as the best way
	to summarize video rushes materials.},
  acmid =     {1386405},
  doi =       {10.1145/1386352.1386405},
  file =      {Christel2008.pdf:Christel2008.pdf:PDF},
  isbn =      {978-1-60558-070-8},
  keywords =  {TRECVID, benchmarking, evaluation, user studies, video abstract, video skim, video summarization, video surrogate, rank2},
  numpages =  {10},
  review =    {Concentrates more on video skimming than audio.},
  status =    {printed, read}
}

@InProceedings{Coleman2007,
  author =    {Coleman, G},
  title =     {Mused: Navigating the Personal Sample Library},
  booktitle = {Proc. International Computer Music Conference},
  year =      {2007},
  address =   {Copenhagen, Denmark},
  month =     {August},
  abstract =  {This work outlines the concept of personal sample library, a large
	database of event-synchronous audio segments extracted from a user{\textquoteright}s
	digital music collection. It identifies tasks involved in making
	sample-based music and audio collage, and shows how interfaces to
	the personal sample library can aid these tasks and simplify the
	music development cycle. In this experiment, we implement an interactive
	scatter plot with dynamic queries to browse, discover, and select
	material from our database of samples.},
  file =      {Coleman2007.pdf:Coleman2007.pdf:PDF},
  url =       {system/files/publications/mused07_0.pdf}
}

@Article{Collins2012,
  author =   {Karen Collins and Peter J. Taillon},
  title =    {Visualized sound effect icons for improved multimedia accessibility: A pilot study },
  journal =  {Entertainment Computing },
  year =     {2012},
  volume =   {3},
  number =   {1},
  pages =    {11 - 17},
  abstract = {Sound effects are often used to communicate important information
	in multimedia such as video games. For instance, they may tell the
	player that a character has just snuck up on them, is firing at them,
	or is about to paddle over a waterfall. Nevertheless, there are times
	when playing sound may be inappropriate, may be inaudible, may become
	fatiguing and/or may be inaccessible for hard of hearing and deaf
	users. Therefore, an alternative to sound that can relay the same
	information would be beneficial to many users. The majority of studies
	into alternative presentations of sound for these purposes have focused
	on dialogue at the expense of music and sound effects. The paper
	introduces a pilot study of "SoundSign", a prototype symbolic representation
	of sound effects for multimedia, using an innovative icon and compass
	that indicates direction, sound cue and proximity. Users who have
	disabled the sound, are hearing-impaired or are otherwise unable
	to hear sound will still get the information needed. A description
	of SoundSign and the results of a usability test are presented.},
  doi =      {10.1016/j.entcom.2011.09.002},
  file =     {Collins2012.pdf:Collins2012.pdf:PDF},
  issn =     {1875-9521},
  keywords = {User interfaces}
}

@InProceedings{Conroy2004,
  author =    {Conroy, Kevin and Levin, Dave and Guimbreti{\`e}re, Fran{\c{c}}ois},
  title =     {{ProofRite}: A paper-augmented word processor},
  booktitle = {Proc. ACM Symposium on User Interface Software and Technology},
  year =      {2004},
  file =      {Conroy2004.pdf:Conroy2004.pdf:PDF},
  keywords =  {rank5}
}

@Article{Cooper2006,
  author =     {Cooper, Matthew and Foote, Jonathan and Pampalk, Elias and Tzanetakis, George},
  title =      {Visualization in Audio-Based Music Information Retrieval},
  journal =    {Comput. Music J.},
  year =       {2006},
  volume =     {30},
  number =     {2},
  pages =      {42--62},
  month =      jun,
  acmid =      {1176365},
  address =    {Cambridge, MA, USA},
  doi =        {10.1162/comj.2006.30.2.42},
  file =       {Cooper2006.pdf:Cooper2006.pdf:PDF},
  issn =       {0148-9267},
  issue_date = {June 2006},
  numpages =   {21},
  publisher =  {MIT Press}
}

@Article{Cridford2005,
  author =    {Alan Cridford},
  title =     {Inside Track: SpotOn},
  journal =   {Line Up},
  year =      {2005},
  volume =    {1},
  number =    {99},
  pages =     {34--35},
  month =     feb,
  booktitle = {Line Up},
  file =      {Cridford2005.pdf:Cridford2005.pdf:PDF},
  owner =     {chrisbau},
  timestamp = {2014.11.05},
  url =       {https://ips.org.uk/wp-content/uploads/2016/02/LU099-10-SpotOn.pdf}
}

@InProceedings{Dalhuijsen2010,
  author =    {Lisa Dalhuijsen and Lievan van Velthoven},
  title =     {MusicalNodes - the visual music library},
  booktitle = {Proc. Electronic Visualisation and the Arts},
  year =      {2010},
  file =      {Dalhuijsen2010.pdf:Dalhuijsen2010.pdf:PDF},
  keywords =  {visualization},
  owner =     {Chris},
  timestamp = {2013.11.21}
}

@InProceedings{Damm2008,
  author =    {Damm, David and Fremerey, Christian and Kurth, Frank and M\"{u}ller, Meinard and Clausen, Michael},
  title =     {Multimodal Presentation and Browsing of Music},
  booktitle = {Proceedings of the 10th International Conference on Multimodal Interfaces},
  year =      {2008},
  series =    {ICMI '08},
  pages =     {205--208},
  address =   {Chania, Crete, Greece},
  publisher = {ACM},
  abstract =  {Recent digitization efforts have led to large music collections, which
	contain music documents of various modes comprising textual, visual
	and acoustic data. In this paper, we present a multimodal music player
	for presenting and browsing digitized music collections consisting
	of heterogeneous document types. In particular, we concentrate on
	music documents of two widely used types for representing a musical
	work, namely visual music representation (scanned images of sheet
	music) and associated interpretations (audio recordings). We introduce
	novel user interfaces for multimodal (audio-visual) music presentation
	as well as intuitive navigation and browsing. Our system offers high
	quality audio playback with time-synchronous display of the digitized
	sheet music associated to a musical work. Furthermore, our system
	enables a user to seamlessly crossfade between various interpretations
	belonging to the currently selected musical work.},
  acmid =     {1452436},
  doi =       {10.1145/1452392.1452436},
  file =      {Damm2008.pdf:Damm2008.pdf:PDF},
  isbn =      {978-1-60558-198-9},
  keywords =  {music alignment, music browsing, music information retrieval, music navigation, music synchronization, interface, rank2},
  numpages =  {4},
  review =    {Synchronises music recordings using chromagrams. Syncs to sheet music
	by synthesising the sheet music and syncing to the chromagram. Interface
	which shows the sheet music following the actual music. Also allows
	users to swap between different performances of the same piece, whilst
	staying in time. System is described but not evaluated.},
  status =    {printed, read}
}

@Article{Daniel2013,
  author =    {David B. Daniel and William Douglas Woody},
  title =     {E-textbooks at what cost? Performance and use of electronic versus print texts},
  journal =   {Computers \& Education},
  year =      {2013},
  volume =    {62},
  pages =     {18--23},
  abstract =  {Abstract While e-book sales continue to increase, electronic textbooks
	are not very popular with college students. This may be due to the
	fact that e-textbooks are read for different reasons and with different
	strategies than are e-books. Although previous research has documented
	this lack of preference for e-textbooks, student performance and
	use of electronic texts has yet to be thoroughly investigated, especially
	in naturalistic settings. This study examines students' use and performance
	on a variety of print and electronic formats in both laboratory and
	at-home conditions. Although students scored similarly across formats
	and conditions, reading time was significantly higher in the electronic
	conditions with this difference increasing for the home conditions.
	Similarly, self-reports of multi-tasking were significantly higher
	for electronic conditions in the home condition, possibly accounting
	for the disparities in reading time. We conclude by urging caution
	in the rush to assume that electronic textbooks are equivalent substitutes
	for traditional textbooks and argue for further investigation into
	the unique ways that students may interact with electronic texts
	to promote more effective design.},
  doi =       {10.1016/j.compedu.2012.10.016},
  file =      {Daniel2013.pdf:Daniel2013.pdf:PDF},
  issn =      {0360-1315},
  keywords =  {E-book},
  owner =     {chrisbau},
  review =    {Reading time was significantly higher for electronic books compared
	to paper books.},
  timestamp = {2017.10.17}
}

@InProceedings{Datteri2004,
  author =    {Darcee L. Datteri and Jeffrey N. Howard},
  title =     {The Sound Of Color},
  booktitle = {Proc. International Conference on Music Perception and Cognition},
  year =      {2004},
  abstract =  {This study investigated the integration of auditory and visual sensory
	information. Seventy-one participants were presented sine wave tones
	along with seven on-screen colored boxes. Participants chose which
	color 'fit best' with each presented tone. Color choices from ten
	exposures to each tone across 80 trials indicated an inverse audio-visual
	sensory processing relationship between wavelength and frequency
	of light versus wavelength and frequency of sound. Analyses suggest
	a consistent and symmetrical data pattern revealing a quasi-linear
	relationship between pitch and color that suggests a natural, stable,
	and universal auditory/visuo-sensory neurological processing algorithm
	for simple tones when presented with basic colors.},
  file =      {Datteri2004.pdf:Datteri2004.pdf:PDF},
  keywords =  {visualization, multimodal, rank2},
  owner =     {Chris},
  review =    {User test which considers relationship of different octaves of C to
	colour. Only considers pure sine waves. Claims statistical significance
	of a few notes but general mapping of all notes to colour is dodgy.},
  status =    {read},
  timestamp = {2013.09.22}
}

@Article{Davis1989,
  author =    {Fred D. Davis},
  title =     {Perceived Usefulness, Perceived Ease of Use, and User Acceptance of Information Technology},
  journal =   {MIS Quarterly},
  year =      {1989},
  volume =    {13},
  number =    {3},
  pages =     {319-340},
  abstract =  {Valid measurement scales for predicting user acceptance of computers
	are in short supply. Most subjective measures used in practice are
	unvalidated, and their relationship to system usage is unknown. The
	present research develops and validates new scales for two specific
	variables, perceived usefulness and perceived ease of use, which
	are hypothesized to be fundamental determinants of user acceptance.
	Definitions for these two variables were used to develop scale items
	that were pretested for content validity and then tested for reliability
	and construct validity in two studies involving a total of 152 users
	and four application programs. The measures were refined and stream-lined,
	resulting in two six-item scales with reliabilities of.98 for usefulness
	and.94 for ease of use. The scales exhibited high convergent, discriminant,
	and factorial validity. Perceived usefulness was significantly correlated
	with both self-reported current usage (r=.63, Study 1) and self-predicted
	future usage (r=.85, Study 2). Perceived ease of use was also significantly
	correlated with current usage (r=.45, Study 1) and future usage (r=.59,
	Study 2). In both studies, usefulness had a significantly greater
	correlation with usage behavior than did ease of use. Regression
	analyses suggest that perceived ease of use may actually be a causal
	antecedent to perceived usefulness, as opposed to a parallel, direct
	determinant of system usage. Implications are drawn for future research
	on user acceptance.},
  doi =       {10.2307/249008},
  file =      {Davis1989.pdf:Davis1989.pdf:PDF},
  issn =      {02767783},
  keywords =  {rank5},
  publisher = {Management Information Systems Research Center, University of Minnesota}
}

@Book{Derry2002,
  title =     {{PC} Audio Editing, second edition},
  publisher = {Taylor \& Francis},
  year =      {2003},
  author =    {Roger Derry},
  isbn =      {0240516974},
  owner =     {chrisbau},
  timestamp = {2018.01.25}
}

@InProceedings{Dewey2016a,
  author =    {Dewey, Christopher and Wakefield, Jonathan},
  title =     {Novel Designs for the Audio Mixing Interface Based on Data Visualization First Principles},
  booktitle = {Proc. 140th Audio Engineering Society Convention},
  year =      {2016},
  month =     may,
  file =      {Dewey2016a.pdf:Dewey2016a.pdf:PDF},
  keywords =  {rank3},
  url =       {http://www.aes.org/e-lib/browse.cfm?elib=18264}
}

@InProceedings{Dewey2014,
  author =    {Christopher Dewey and Jonathan P. Wakefield},
  title =     {A guide to the design and evaluation of new user interfaces for the audio industry},
  booktitle = {Proc. Audio Engineering Society Convention},
  year =      {2014},
  file =      {Dewey2014.pdf:Dewey2014.pdf:PDF},
  keywords =  {rank5},
  owner =     {chrisbau},
  timestamp = {2015.06.01},
  url =       {http://www.aes.org/e-lib/browse.cfm?elib=17218}
}

@InProceedings{Dewey2016,
  author =    {Christopher Dewey and Jonathan P. Wakefield},
  title =     {Audio interfaces should be designed based on data visualisation first principles},
  booktitle = {Proceedings of the 2nd AES Workshop on Intelligent Music Production},
  year =      {2016},
  month =     sep,
  file =      {Dewey2016.pdf:Dewey2016.pdf:PDF},
  keywords =  {rank5},
  owner =     {chrisbau},
  timestamp = {2017.02.24}
}

@InProceedings{Dhanesha2010,
  author =    {Dhanesha, Ketki A. and Rajput, Nitendra and Srivastava, Kundan},
  title =     {User Driven Audio Content Navigation for Spoken Web},
  booktitle = {Proceedings of the International Conference on Multimedia},
  year =      {2010},
  series =    {MM '10},
  pages =     {1071--1074},
  address =   {Firenze, Italy},
  publisher = {ACM},
  acmid =     {1874152},
  doi =       {10.1145/1873951.1874152},
  file =      {Dhanesha2010.pdf:Dhanesha2010.pdf:PDF},
  isbn =      {978-1-60558-933-6},
  keywords =  {developing regions, mobile phones, navigation, voicesites, world wide telecom web, skimming, rank1},
  numpages =  {4},
  review =    {Adds voice skimming to 'spoken web' telephone service, primarily used
	by illiterate farmers.},
  status =    {printed, read}
}

@InProceedings{Diakopoulos2006,
  author =    {Diakopoulos, Nicholas and Essa, Irfan},
  title =     {Videotater: An Approach for Pen-based Digital Video Segmentation and Tagging},
  booktitle = {Proceedings of the 19th Annual ACM Symposium on User Interface Software and Technology},
  year =      {2006},
  series =    {UIST '06},
  pages =     {221--224},
  address =   {Montreux, Switzerland},
  publisher = {ACM},
  abstract =  {The continuous growth of media databases necessitates development
	of novel visualization and interaction techniques to support management
	of these collections. We present Videotater, an experimental tool
	for a Tablet PC that supports the efficient and intuitive navigation,
	selection, segmentation, and tagging of video. Our veridical representation
	immediately signals to the user where appropriate segment boundaries
	should be placed and allows for rapid review and refinement of manually
	or automatically generated segments. Finally, we explore a distribution
	of modalities in the interface by using multiple timeline representations,
	pressure sensing, and a tag painting/erasing metaphor with the pen.},
  acmid =     {1166287},
  doi =       {10.1145/1166253.1166287},
  file =      {Diakopoulos2006.pdf:Diakopoulos2006.pdf:PDF},
  isbn =      {1-59593-313-1},
  keywords =  {video segmentation, video tagging, pen, rank4},
  numpages =  {4},
  review =    {Diakopoulos and Essa (2006) created 'VideoTater', which was a digital
	ink interface for creating and annotating segments of a pre-recorded
	video. Segments could be created by drawing a vertical line on a
	video timeline, and segments could be merged by drawing a horizontal
	line between them. Each segment could be tagged by selecting it and
	hand-writing the text, and the back of the pen could be used to erase
	tags. Pen pressure was used to distinguish between selection and
	tagging, with low pressure for selecting and high for tagging. Informal
	feedback from three users found that the gestures were successful,
	and that the pressure mapping worked well.}
}

@Article{Doddington1985,
  author =   {Doddington, G.R.},
  title =    {Speaker recognition: Identifying people by their voices},
  journal =  {Proceedings of the IEEE},
  year =     {1985},
  volume =   {73},
  number =   {11},
  pages =    {1651-1664},
  month =    nov,
  abstract = {The usefulness of identifying a person from the characteristics of
	his voice is increasing with the growing importance of automatic
	information processing and telecommunications. This paper reviews
	the voice characteristics and identification techniques used in recognizing
	people by their voices. A discussion of inherent performance limitations,
	along with a review of the performance achieved by listening, visual
	examination of spectrograms, and automatic computer techniques, attempts
	to provide a perspective with which to evaluate the potential of
	speaker recognition and productive directions for research into and
	application of speaker recognition technology.},
  doi =      {10.1109/PROC.1985.13345},
  file =     {Doddington1985.pdf:Doddington1985.pdf:PDF},
  issn =     {0018-9219},
  keywords = {Access control, Application software, Automatic speech recognition, Biometrics, Character recognition, Ear, Humans, Speaker recognition, Speech recognition, Switches, rank1}
}

@Article{Downie2008,
  author =    {J. Stephen Downie},
  title =     {The music information retrieval evaluation exchange (2005--2007): A window into music information retrieval research},
  journal =   {Acoustical Science and Technology},
  year =      {2008},
  volume =    {29},
  number =    {4},
  pages =     {247-255},
  doi =       {10.1250/ast.29.247},
  owner =     {chrisbau},
  timestamp = {2018.01.12}
}

@Article{Duan2014,
  author =    {Duan, Shufei and Zhang, Jinglan and Roe, Paul and Towsey, Michael},
  title =     {A survey of tagging techniques for music, speech and environmental sound},
  journal =   {Artificial Intelligence Review},
  year =      {2014},
  volume =    {42},
  number =    {4},
  pages =     {637--661},
  month =     dec,
  abstract =  {Sound tagging has been studied for years. Among all sound types, music,
	speech, and environmental sound are three hottest research areas.
	This survey aims to provide an overview about the state-of-the-art
	development in these areas. We discuss about the meaning of tagging
	in different sound areas at the beginning of the journey. Some examples
	of sound tagging applications are introduced in order to illustrate
	the significance of this research. Typical tagging techniques include
	manual, automatic, and semi-automatic approaches. After reviewing
	work in music, speech and environmental sound tagging, we compare
	them and state the research progress to date. Research gaps are identified
	for each research area and the common features and discriminations
	between three areas are discovered as well. Published datasets, tools
	used by researchers, and evaluation measures frequently applied in
	the analysis are listed. In the end, we summarise the worldwide distribution
	of countries dedicated to sound tagging research for years.},
  day =       {01},
  doi =       {10.1007/s10462-012-9362-y},
  file =      {Duan2014.pdf:Duan2014.pdf:PDF},
  issn =      {1573-7462},
  keywords =  {rank3},
  owner =     {chrisbau},
  timestamp = {2017.10.17}
}

@Article{Dunaway2000,
  author =    {Dunaway, David King},
  title =     {Digital Radio Production - Towards an Aesthetic},
  journal =   {New Media and Society},
  year =      {2000},
  volume =    {2},
  number =    {1},
  pages =     {29--50},
  file =      {Dunaway2000.pdf:Dunaway2000.pdf:PDF},
  keywords =  {rank5},
  publisher = {SAGE Publications}
}

@Book{Ebert1994,
  title =     {Texturing and Modeling - A Procedural Approach},
  publisher = {Academic Press},
  year =      {2002},
  author =    {David S. Ebert and F. Kenton Musgrave and Darwyn Peachey and Ken Perlin and Steven Worley},
  editor =    {David S. Ebert},
  isbn =      {1558608486},
  owner =     {Chris},
  review =    {Creating textures algorithmically. Primarily uses fractals.},
  timestamp = {2013.12.05}
}

@Patent{Edgecomb2014,
  nationality = {US},
  number =      {US 20140347328},
  year =        {2014},
  yearfiled =   {2012},
  author =      {Edgecomb, Tracy L and Van Schaack, Andrew J},
  title =       {Content selection in a pen-based computing system},
  month =       may #{~23},
  file =        {Edgecomb2014.pdf:Edgecomb2014.pdf:PDF},
  publisher =   {Google Patents}
}

@InProceedings{Edmonds2004,
  author =    {Edmonds, Ernest and Martin, Andrew and Pauletto, Sandra},
  title =     {Audio-visual Interfaces in Digital Art},
  booktitle = {Proceedings of the 2004 ACM SIGCHI International Conference on Advances in Computer Entertainment Technology},
  year =      {2004},
  series =    {ACE '04},
  pages =     {331--336},
  address =   {Singapore},
  publisher = {ACM},
  acmid =     {1067392},
  doi =       {10.1145/1067343.1067392},
  file =      {Edmonds2004.pdf:Edmonds2004.pdf:PDF},
  isbn =      {1-58113-882-2},
  numpages =  {6}
}

@InProceedings{Eika2017,
  author =    {Eika, Evelyn and Sandnes, Frode E.},
  title =     {Rethinking Audio Visualizations: Towards Better Visual Search in Audio Editing Interfaces},
  booktitle = {Universal Access in Human--Computer Interaction. Human and Technological Environments},
  year =      {2017},
  editor =    {Antona, Margherita and Stephanidis, Constantine},
  pages =     {410--418},
  address =   {Cham},
  publisher = {Springer International Publishing},
  abstract =  {QueryWaveform visualization is a key tool in audio editing. However, the visualization of audio waveforms has changed little since the emergence of the first software systems for audio editing several decades ago. This paper explores how audio is visualized. This paper shows that the commonly used time-domain representation exhibits redundant information that occupies valuable display real-estate in most audio editing software. An alternative waveform visualization approach is proposed that exploits elements from the existing visualization conventions while enhancing features that are important in visual search through digital audio. Alternatively, the method is a means for making more efficient use of the display real-estate. The proposed method is discussed in terms of its suitability for various visualization situations.},
  doi =       {10.1007/978-3-319-58700-4_33},
  file =      {Eika2017.pdf:Eika2017.pdf:PDF},
  isbn =      {978-3-319-58700-4},
  owner =     {chrisbau},
  timestamp = {2018.01.25}
}

@Book{Elliott2007,
  title =     {Statistical Analysis Quick Reference Guidebook: With {SPSS} Examples},
  publisher = {Sage},
  year =      {2007},
  author =    {Alan C. Elliott and Wayne A. Woodward},
  isbn =      {1412925606},
  owner =     {chrisbau},
  timestamp = {2018.01.23}
}

@InProceedings{Engstroem2010,
  author =    {Engstr\"{o}m, Arvid and Juhlin, Oskar and Perry, Mark and Broth, Mathias},
  title =     {Temporal Hybridity: Footage with Instant Replay in Real Time},
  booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  year =      {2010},
  series =    {CHI '10},
  pages =     {1495--1504},
  address =   {Atlanta, Georgia, USA},
  publisher = {ACM},
  acmid =     {1753550},
  doi =       {10.1145/1753326.1753550},
  file =      {Engstroem2010.pdf:Engstroem2010.pdf:PDF},
  isbn =      {978-1-60558-929-9},
  keywords =  {collaborative search, control room, editing, media production, social interaction, streaming, television, video, rank2},
  numpages =  {10},
  review =    {TV ethnographic work}
}

@MastersThesis{Ericsson2009,
  author =    {Lars Ericsson},
  title =     {Automatic speech/music discrimination in audio files},
  school =    {School of Media Technology, KTH Royal Institute of Technology},
  year =      {2009},
  file =      {Ericsson2009.pdf:Ericsson2009.pdf:PDF},
  keywords =  {SMD, rank3},
  owner =     {Chris},
  review =    {Speech-music discrimination for Swedish Radio. 97% accuracte within
	1s using only RMS. Tests a small number of features using a simple
	threshold for classification. Finds low-energy ratio to perform best.
	MFCCs didn't work as only thesholding considered.},
  status =    {printed, read},
  timestamp = {2013.11.11},
  url =       {http://www.speech.kth.se/prod/publications/files/3437.pdf}
}

@InProceedings{Erol2008,
  author =    {Erol, Berna and Ant\'{u}nez, Emilio and Hull, Jonathan J.},
  title =     {{HOTPAPER}: Multimedia Interaction with Paper Using Mobile Phones},
  booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
  year =      {2008},
  series =    {MM '08},
  pages =     {399--408},
  address =   {Vancouver, British Columbia, Canada},
  publisher = {ACM},
  acmid =     {1459413},
  doi =       {10.1145/1459359.1459413},
  file =      {Erol2008.pdf:Erol2008.pdf:PDF},
  isbn =      {978-1-60558-303-7},
  keywords =  {linking paper to electronic data, markerless linking, mobile imaging, mobile interaction, rank5},
  numpages =  {10},
  review =    {Uses space between paragraphs on paper as barcode for navigating media}
}

@InProceedings{Erol2007,
  author =    {Erol, Berna and Graham, Jamey and Hull, Jonathan J. and Hart, Peter E.},
  title =     {A Modern Day Video Flip-book: Creating a Printable Representation from Time-based Media},
  booktitle = {Proceedings of the 15th ACM International Conference on Multimedia},
  year =      {2007},
  series =    {MM '07},
  pages =     {819--822},
  address =   {Augsburg, Germany},
  publisher = {ACM},
  acmid =     {1291419},
  doi =       {10.1145/1291233.1291419},
  file =      {Erol2007.pdf:Erol2007.pdf:PDF},
  isbn =      {978-1-59593-702-5},
  keywords =  {rank5},
  numpages =  {4},
  review =    {Prints media data as barcode on paper}
}

@Patent{Fahraeus2003,
  nationality = {US},
  number =      {US 6502756},
  year =        {2003},
  yearfiled =   {2000},
  author =      {F{\aa}hraeus, C.},
  title =       {Recording of information},
  note =        {US Patent 6,502,756},
  url =         {https://www.google.com/patents/US6502756},
  file =        {Fahraeus2003.pdf:Fahraeus2003.pdf:PDF},
  keywords =    {rank5},
  publisher =   {Google Patents},
  review =      {Anoto patent}
}

@PhdThesis{Fazekas2012,
  author =    {Fazekas, Gy\"{o}rgy},
  title =     {Semantic Audio Analysis Utilities and Applications},
  school =    {Queen Mary University of London},
  year =      {2012},
  abstract =  {Extraction, representation, organisation and application of metadata about audio recordings
 are in the concern of semantic audio analysis. Our broad interpretation, aligned with recent
 developments in the field, includes methodological aspects of semantic audio, such as
 those related to information management, knowledge representation and applications of the
 extracted information. In particular, we look at how Semantic Web technologies may be used
 to enhance information management practices in two audio related areas: music informatics
 and music production.
 In the first area, we are concerned with music information retrieval (MIR) and related
 research. We examine how structured data may be used to support reproducibility and
 provenance of extracted information, and aim to support multi-modality and context adaptation
 in the analysis. In creative music production, our goals can be summarised as follows:
 off-the-shelf sound editors do not hold appropriately structured information about the edited
 material, thus human-computer interaction is inefficient. We believe that recent developments
 in sound analysis and music understanding are capable of bringing about significant improvements
 in the music production workflow. Providing visual cues related to music structure can
 serve as an example of intelligent, context-dependent functionality.
 The central contributions of this work are a Semantic Web ontology for describing recording
 studios, including a model of technological artefacts used in music production, methodologies
 for collecting data about music production workflows and describing the work of
 audio engineers which facilitates capturing their contribution to music production, and finally
 a framework for creating Web-based applications for automated audio analysis. This
 has applications demonstrating how Semantic Web technologies and ontologies can facilitate
 interoperability between music research tools, and the creation of semantic audio software, for
 instance, for music recommendation, temperament estimation or multi-modal music tutoring},
  file =      {Fazekas2012.pdf:Fazekas2012.pdf:PDF},
  owner =     {chrisbau},
  timestamp = {2018.01.12},
  url =       {http://qmro.qmul.ac.uk/jspui/handle/123456789/8443}
}

@InProceedings{Fazekas2007,
  author =    {Fazekas, Gy\"{o}rgy and Sandler, Mark},
  title =     {Intelligent Editing of Studio Recordings with the Help of Automatic Music Structure Extraction},
  booktitle = {122nd Audio Engineering Society Convention},
  year =      {2007},
  month =     may,
  file =      {Fazekas2007.pdf:Fazekas2007.pdf:PDF},
  keywords =  {rank1},
  owner =     {chrisbau},
  timestamp = {2017.11.03},
  url =       {http://www.aes.org/e-lib/browse.cfm?elib=14024}
}

@Article{Feng2004,
  author =     {Feng, Jinjuan and Sears, Andrew},
  title =      {Using Confidence Scores to Improve Hands-free Speech Based Navigation in Continuous Dictation Systems},
  journal =    {ACM Trans. Comput.-Hum. Interact.},
  year =       {2004},
  volume =     {11},
  number =     {4},
  pages =      {329--356},
  month =      dec,
  acmid =      {1035576},
  address =    {New York, NY, USA},
  doi =        {10.1145/1035575.1035576},
  file =       {Feng2004.pdf:Feng2004.pdf:PDF},
  issn =       {1073-0516},
  issue_date = {December 2004},
  keywords =   {Confidence score, navigation, simulation, speech recognition},
  numpages =   {28},
  publisher =  {ACM},
  review =     {Compared "compare the confidence score of eachword to an established
	threshold" and found "that confidence scores are not likely to be
	effective if the focus is on error detection"}
}

@InProceedings{Ferguson2005,
  author =    {Sam Ferguson and Andrew Vande Moere and Densil Cabrera},
  title =     {Seeing Sound: Real-time Sound Visualisation in Visual Feedback Loops used for Training Musicians},
  booktitle = {Proc. International Conference on Information Visualisation},
  year =      {2005},
  number =    {9},
  publisher = {IEEE},
  abstract =  {Musicians in training need to understand the sound they are producing
	in order to improve its deficient aspects. Verbal feedback from musical
	masters is the usual method used for attaining this understanding.
	However, using real-time sound visualisation as a complementary form
	of feed-back allows the large amounts of data typical of real-time
	acoustic analysis to be employed within training. This im-proves
	the efficiency of the feedback loop normally present withinmusical
	training and pedagogy. The implementation and effect of such a system
	is discussed.},
  file =      {Ferguson2005.pdf:Ferguson2005.pdf:PDF},
  keywords =  {Sound visualization, real-time visualization},
  review =    {Real time visual feedback for musicians using harmonic content, fine
	pitch, noisiness and loudness. Uses a cone shaped design.},
  status =    {read}
}

@InProceedings{Fiscus2006,
  author =    {Fiscus, Jonathan G. and Ajot, Jerome and Michel, Martial and Garofolo, John S.},
  title =     {The Rich Transcription 2006 Spring Meeting Recognition Evaluation},
  booktitle = {Machine Learning for Multimodal Interaction},
  year =      {2006},
  editor =    {Renals, Steve and Bengio, Samy and Fiscus, Jonathan G.},
  pages =     {309--322},
  address =   {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract =  {We present the design and results of the Spring 2006 (RT-06S) Rich Transcription Meeting Recognition Evaluation; the fourth in a series of community-wide evaluations of language technologies in the meeting domain. For 2006, we supported three evaluation tasks in two meeting sub-domains: the Speech-To-Text (STT) transcription task, and the ``Who Spoke When'' and ``Speech Activity Detection'' diarization tasks. The meetings were from the Conference Meeting, and Lecture Meeting sub-domains. The lowest STT word error rate, with up to four simultaneous speakers, in the multiple distant microphone condition was 46.3{\%} for the conference sub-domain, and 53.4{\%} for the lecture sub-domain. For the ``Who Spoke When'' task, the lowest diarization error rates for all speech were 35.8{\%} and 24.0{\%} for the conference and lecture sub-domains respectively. For the ``Speech Activity Detection'' task, the lowest diarization error rates were 4.3{\%} and 8.0{\%} for the conference and lecture sub-domains respectively.},
  isbn =      {978-3-540-69268-3},
  owner =     {chrisbau},
  timestamp = {2018.01.27}
}

@Misc{Fisher2016,
  author =    {Tyler Fisher},
  title =     {How NPR Transcribes and Fact-Checks the Debates, Live},
  month =     oct,
  year =      {2016},
  comment =   {Accessed 02/01/2018},
  owner =     {chrisbau},
  timestamp = {2018.01.02},
  url =       {https://source.opennews.org/articles/how-npr-transcribes-and-fact-checks-debates-live/}
}

@MastersThesis{FontCorbera2010,
  author =   {Font Corbera, Frederic},
  title =    {Design and Evaluation of a Visualization Interface for Querying Large Unstructured Sound Databases},
  school =   {Universitat Pompeu Fabra, Barcelona},
  year =     {2010},
  abstract = {Search is an underestimated problem that plays a big role in any application
	dealing with large databases. The more extensive and heterogeneous
	our data is, the harder is to find exactly what we are looking for.
	This idea resembles the data availability paradox stated by Woods:
	"more and more data is available, but our ability to interpret what
	is available has not increased". Then the question arises: is it
	really useful to collect a big dataset even if we do not have the
	ability to successfully navigate among it? According to Morville
	and Callender, search is a grand challenge that can be succeeded
	with courage and vision. A good searching tool completely improves
	the exploitation we can do of our information resources. As a consequence,
	commonly used search methods must evolve. Search goal is more than
	finding, search should become a conversation process where answers
	change the questions. Having stated all that, it seems clear that
	extensive effort should be invested on the research and design of
	appropriate tools for finding our needles in the haystack. However,
	search is a problem that does not have a general solution. It must
	be adapted to the context of the information we are dealing with,
	in the case of the presnet document, unstructured sound databases.
	The aim of this thesis is the design of a visualization interface
	that let users graphically define queries for the Freesound Project
	database (http://www.freesound.org) and retrieve suitable results
	for a musical context. Music Information Retrieval (MIR) techniques
	are used to analyze all the files in the database and automatically
	extract audio features concerning four different aspects of sound
	perception: temporal envelope, timbre, tonal information and pitch.
	Users perform queries by graphically specifying a target for each
	one of these perceptual aspects, that is to say, queries are specified
	by defining the physical properties of the sound itself rather than
	indicating its source (as is usually done in common text-based search
	engines). Similarity search is performed among the whole database
	to find the most similar sound files, and returned results are represented
	as points in a two-dimensional space that users can explore.},
  file =     {FontCorbera2010.pdf:FontCorbera2010.pdf:PDF},
  keywords = {rank3},
  review =   {A visualization interface that let users graphically define queries for the Freesound Project database. Has a similar background section.},
  url =      {static/media/Font-Frederic-Master-Thesis-2010.pdf}
}

@Article{Foote1999,
  author =    {Foote, Jonathan},
  title =     {An overview of audio information retrieval},
  journal =   {Multimedia Systems},
  year =      {1999},
  volume =    {7},
  number =    {1},
  pages =     {2-10},
  file =      {Foote1999.pdf:Foote1999.pdf:PDF},
  issn =      {0942-4962},
  keywords =  {rank5},
  language =  {English},
  publisher = {Springer-Verlag}
}

@InProceedings{Foote1998,
  author =    {Foote, Jonathan and Boreczhy, John and Girgensohn, Andreas and Wilcox, Lynn},
  title =     {An Intelligent Media Browser Using Automatic Multimodal Analysis},
  booktitle = {Proceedings of the Sixth ACM International Conference on Multimedia},
  year =      {1998},
  series =    {MULTIMEDIA '98},
  pages =     {375--380},
  address =   {Bristol, United Kingdom},
  publisher = {ACM},
  acmid =     {290804},
  doi =       {10.1145/290747.290804},
  file =      {Foote1998.pdf:Foote1998.pdf:PDF},
  isbn =      {0-201-30990-4},
  keywords =  {automatic analysis, content-based retrieval, skimming, spacker identification, video, visualization, rank4},
  numpages =  {6},
  review =    {Example interface which uses pseudocolour.}
}

@Article{Foulke1969,
  author =   {Foulke, Emerson and Sticht, Thomas G.},
  title =    {Review of research on the intelligibility and comprehension of accelerated speech},
  journal =  {Psychological Bulletin},
  year =     {1969},
  volume =   {72},
  number =   {1},
  pages =    {50--62},
  abstract = {Time-compressed or accelerated speech is speech which has been reproduced in less than the original production time. Such speech may prove to be useful in a variety of situations in which people must rely upon listening to obtain the information specified by language. It may also prove to be a useful tool in studying the temporal requirements of the listener as he processes spoken language. Methods for the generation of time compressed speech are reviewed. Methods for the assessment of the effect of compression on word intelligibility and listening comprehension are discussed. Experiments dealing with the effect of time compression upon word intelligibility and upon the comprehensibility of connected discourse, and experiments concerned with the influence of stimulus variables, such as signal distortion, and organismic variables such as intelligence, are reviewed. The general finding that compression in time has a different effect upon the comprehensibility of connected discourse than},
  doi =      {10.1037/h0027575},
  issn =     {0033-2909},
  keywords = {intelligibility & comprehension of accelerated speech, review of research, Age Factors, Auditory Perception, Child, Educational Status, Female, Humans, Intelligence, Male, Methods, Psycholinguistics, Reading, Sex Factors, Speech, Time Factors, Vision, Ocular, Auditory Discrimination, Experimentation, Literature Review, Speech Perception, Thinking}
}

@InProceedings{Fouse2011,
  author =    {Fouse, Adam and Weibel, Nadir and Hutchins, Edwin and Hollan, James D.},
  title =     {{ChronoViz}: A System for Supporting Navigation of Time-coded Data},
  booktitle = {Proc. Extended Abstracts on Human Factors in Computing Systems (CHI)},
  year =      {2011},
  series =    {CHI EA '11},
  pages =     {299--304},
  address =   {Vancouver, British Columbia, Canada},
  publisher = {ACM},
  acmid =     {1979706},
  doi =       {10.1145/1979742.1979706},
  file =      {Fouse2011.pdf:Fouse2011.pdf:PDF},
  isbn =      {978-1-4503-0268-5},
  keywords =  {behavioral research, digital paper, interactive visualization, multimodal interaction, rank5},
  numpages =  {6}
}

@Misc{Friedhoff2016,
  author =       {Jane Friedhoff},
  title =        {Notes on Designing {This American Life's ``Shortcut''}},
  howpublished = {The Tow Center for Digital Journalism},
  month =        oct,
  year =         {2016},
  comment =      {Accessed 05/02/2018},
  owner =        {chrisbau},
  timestamp =    {2018.01.05},
  url =          {https://towcenter.org/notes-from-creating-this-american-lifes-shortcut/}
}

@Article{Friedland2009,
  author =     {Friedland, G. and Vinyals, O. and Huang, Yan and Muller, C.},
  title =      {Prosodic and Other Long-Term Features for Speaker Diarization},
  journal =    {Trans. Audio, Speech and Lang. Proc.},
  year =       {2009},
  volume =     {17},
  number =     {5},
  pages =      {985--993},
  month =      jul,
  abstract =   {Speaker diarization is defined as the task of determining ldquowho
	spoke whenrdquo given an audio track and no other prior knowledge
	of any kind. The following article shows how a state-of-the-art speaker
	diarization system can be improved by combining traditional short-term
	features (MFCCs) with prosodic and other long-term features. First,
	we present a framework to study the speaker discriminability of 70
	different long-term features. Then, we show how the top-ranked long-term
	features can be combined with short-term features to increase the
	accuracy of speaker diarization. The results were measured on standardized
	datasets (NIST RT) and show a consistent improvement of about 30%
	relative in diarization error rate compared to the best system presented
	at the NIST evaluation in 2007.},
  acmid =      {2209953},
  address =    {Piscataway, NJ, USA},
  doi =        {10.1109/TASL.2009.2015089},
  file =       {Friedland2009.pdf:Friedland2009.pdf:PDF},
  issn =       {1558-7916},
  issue_date = {July 2009},
  keywords =   {Long-term features, prosody, speaker diarization},
  numpages =   {9},
  publisher =  {IEEE Press},
  status =     {printed}
}

@Article{Frisson2010,
  author =    {Frisson, Christian and Ala{\c{c}}am, Sema and Coskun, Emirhan and Ertl, Dominik and Kayalar, Ceren and Lawson, Lionel and Lingenfelser, Florian and Wagner, Johannes},
  title =     {COMEDIANNOTATE: Towards more usable multimedia content annotation by adapting the use interface},
  journal =   {QPSR of the Numediart Research Program},
  year =      {2010},
  volume =    {3},
  number =    {3},
  pages =     {45--55},
  file =      {Frisson2010.pdf:Frisson2010.pdf:PDF},
  keywords =  {rank1},
  publisher = {Citeseer},
  review =    {Media annotation interface. Has a decent lit review.}
}

@InProceedings{Gomez2005,
  author =    {Emilia G\'{o}mez and Jordi Bonada},
  title =     {Tonality Visualization of Polyphonic Audio},
  booktitle = {Proc. International Computer Music Conference},
  year =      {2005},
  abstract =  {This paper presents a tool to visualize the tonal content of polyphonic
	audio signals. After a brief introduction to the problem of tonal
	analysis, we present different views that can help to analyze the
	tonal content of a piece of music in audio format and to investigate
	techniques for chord and key estimation and tonal similarity.},
  file =      {Gomez2005.pdf:Gomez2005.pdf:PDF},
  keywords =  {visualization, rank1},
  review =    {Presents a variety of ideas on presenting the key of music visually.
	Some interesting ideas including a torus representation and multi
	scaled temporal Visualization.},
  status =    {read}
}

@InProceedings{Garcia2014,
  author =    {J\'{e}r\'{e}mie Garcia and Theophanis Tsandilas and Carlos Agon and Wendy Mackay},
  title =     {PaperComposer: Creating Interactive Paper Interfaces for Music Composition},
  booktitle = {Proc. Conference of the Association Francophone d'Interaction Homme-Machine (IHM)},
  year =      {2014},
  file =      {Garcia2014.pdf:Garcia2014.pdf:PDF},
  keywords =  {rank2},
  owner =     {chrisbau},
  review =    {Interactive paper which can be cut up.},
  timestamp = {2015.01.16}
}

@InProceedings{Garcia2012,
  author =    {J\'{e}r\'{e}mie Garcia and Theophanis Tsandilas and Carlos Agon and Wendy E. Mackay},
  title =     {Interactive Paper Substrates to Support Musical Creation},
  booktitle = {Proc. ACM Conference on Human Factors in Computing Systems (CHI)},
  year =      {2012},
  file =      {Garcia2012.pdf:Garcia2012.pdf:PDF},
  keywords =  {rank2},
  owner =     {chrisbau},
  timestamp = {2015.01.16}
}

@InProceedings{Gingrich2013,
  author =    {Oliver Gingrich and Eugenia Emets and Alain Renaud},
  title =     {Enhancing Presence - Immersive Sound Environments and Presence Generating Factor},
  booktitle = {Proc. Electronic Visualisation and the Arts},
  year =      {2013},
  file =      {Gingrich2013.pdf:Gingrich2013.pdf:PDF},
  keywords =  {rank1},
  owner =     {Chris},
  review =    {Introduces concept of cymatics. Describes installation that uses very
	basic features. Doesn't go into details about visualisation.},
  status =    {printed, read},
  timestamp = {2013.11.21}
}

@InProceedings{Gohlke2010,
  author =       {Gohlke, Kristian and Hlatky, Michael and Heise, Sebastian and Black, David and Loviscach, J{\"o}rn},
  title =        {Track displays in {DAW} software: Beyond waveform views},
  booktitle =    {Proc. 128th Audio Engineering Society Convention},
  year =         {2010},
  organization = {Audio Engineering Society},
  file =         {Gohlke2010.pdf:Gohlke2010.pdf:PDF},
  keywords =     {rank5},
  url =          {http://www.aes.org/e-lib/browse.cfm?elib=15441}
}

@InProceedings{Goodwin2004,
  author =    {Goodwin, M.M. and Laroche, J.},
  title =     {A dynamic programming approach to audio segmentation and speech/music discrimination},
  booktitle = {Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).},
  year =      {2004},
  volume =    {4},
  pages =     {iv-309-iv-312 vol.4},
  abstract =  {We consider the problem of segmenting an audio signal into characteristic
	regions based on feature-set similarities. In the proposed approach,
	a feature-space representation of the signal is generated; sequences
	of these feature-space samples are then aggregated into clusters
	corresponding to distinct signal regions. The algorithm consists
	of using linear discriminant analysis (LDA) to condition the feature
	space and dynamic programming (DP) to identify data clusters. We
	consider the design of the dynamic program cost functions; we are
	able to derive effective cost functions without relying on significant
	prior information about the structure of the expected data clusters.
	We demonstrate the application of the LDA-DP segmentation algorithm
	to speech/music discrimination. Experimental results are given and
	discussed.},
  doi =       {10.1109/ICASSP.2004.1326825},
  file =      {Goodwin2004.pdf:Goodwin2004.pdf:PDF},
  issn =      {1520-6149},
  keywords =  {audio signal processing, dynamic programming, music, speech, speech processing, audio segmentation, audio signal segmentation, data clusters, dynamic program cost functions, dynamic programming, feature-space representation, linear discriminant analysis, signal representation, speech/music discrimination, Clustering algorithms, Cost function, Covariance matrix, Dynamic programming, Fingerprint recognition, Linear discriminant analysis, Multiple signal classification, Robustness, Signal generators, Speech, SMD},
  status =    {printed, read}
}

@InProceedings{Goudeseune2012,
  author =    {Goudeseune, Camille},
  title =     {Effective Browsing of Long Audio Recordings},
  booktitle = {Proceedings of the 2nd ACM International Workshop on Interactive Multimedia on Mobile and Portable Devices},
  year =      {2012},
  series =    {IMMPD '12},
  pages =     {35--42},
  address =   {Nara, Japan},
  publisher = {ACM},
  acmid =     {2390831},
  doi =       {10.1145/2390821.2390831},
  file =      {Goudeseune2012.pdf:Goudeseune2012.pdf:PDF},
  isbn =      {978-1-4503-1595-1},
  keywords =  {big data, zoom, mipmap, rank5},
  numpages =  {8},
  review =    {Highly relevant work on zooming-aware audio visualisation for browsing
	long recordings. Visualisation isn't particularly innovative, but
	the user trial uses a format (anomaly discovery) which could potentially
	be used to evaluate my work.},
  status =    {printed, read}
}

@InProceedings{Graham2003,
  author =    {J. Graham and J. J. Hull},
  title =     {A paper-based interface for video browsing and retrieval},
  booktitle = {Proc. International Conference on Multimedia and Expo (ICME)},
  year =      {2003},
  volume =    {2},
  pages =     {II-749-52 vol.2},
  month =     {July},
  abstract =  {A paper-based interface for browsing video is proposed. A paper document
	shows key frames selected from a video, a transcript for the parallel
	audio track, and bar codes that, when scanned, invoke a multimedia
	player. The paper document provides a stand-alone representation
	for a video recording that lets a user both understand the content
	of the file and replay only selected parts of the multimedia that
	are necessary to gain a better understanding. This approach applies
	the two-dimensional display characteristics of a newspaper to multimedia
	retrieval. By so doing, the user's browsing and search efficiency
	is greatly improved. This poster describes an implementation of the
	video paper system using a pocket PC with a bar code reader as the
	remote control device and an archive of video recordings on the pocket
	PC or an external server.},
  doi =       {10.1109/ICME.2003.1221725},
  file =      {Graham2003.pdf:Graham2003.pdf:PDF},
  keywords =  {bar codes, image retrieval, mark scanning equipment, multimedia systems, notebook computers, video recording, video signal processing, bar code reader, external server, multimedia player, multimedia retrieval, paper document, paper-based interface, parallel audio track, pocket PC, remote control device, search efficiency, two-dimensional display characteristics, video browsing, video recording, video retrieval, Control systems, Costs, Design engineering, Displays, Power engineering and energy, Technological innovation, Usability, Video recording, Visualization, Watches, rank4},
  review =    {Duplicate of Hull2003}
}

@InProceedings{Gravano2009,
  author =    {A. Gravano and M. Jansche and M. Bacchiani},
  title =     {Restoring punctuation and capitalization in transcribed speech},
  booktitle = {2009 IEEE International Conference on Acoustics, Speech and Signal Processing},
  year =      {2009},
  pages =     {4741--4744},
  month =     {April},
  abstract =  {Adding punctuation and capitalization greatly improves the readability of automatic speech transcripts. We discuss an approach for performing both tasks in a single pass using a purely text-based n-gram language model. We study the effect on performance of varying the n-gram order (from n = 3 to n = 6) and the amount of training data (from 58 million to 55 billion tokens). Our results show that using larger training data sets consistently improves performance, while increasing the n-gram order does not help nearly as much.},
  doi =       {10.1109/ICASSP.2009.4960690},
  issn =      {1520-6149},
  keywords =  {natural languages;speech recognition;text analysis;automatic speech transcript;capitalization restoration;punctuation restoration;speech recognition;text-based n-gram language model;Automatic speech recognition;Broadcasting;Computer science;Mars;Natural languages;Speech recognition;Testing;Text recognition;Training data;Volcanoes;Speech recognition;capitalization;punctuation}
}

@InProceedings{Grierson2009,
  author =    {Mick Grierson},
  title =     {Plundermatics: Real-time interactive media segmentation for audiovisual analysis, composition and performance},
  booktitle = {Proc. Electronic Visualisation and the Arts},
  year =      {2009},
  file =      {Grierson2009.pdf:Grierson2009.pdf:PDF},
  owner =     {Chris},
  timestamp = {2013.11.21}
}

@Misc{Griggs2007,
  author =    {Griggs, Kenneth King},
  title =     {Transcript alignment},
  month =     jun,
  year =      {2007},
  note =      {US Patent 7,231,351},
  keywords =  {rank3},
  publisher = {Google Patents},
  review =    {Technology used to power ScriptSync},
  url =       {https://www.google.com/patents/US7231351}
}

@Article{Gueorguieva2004,
  author =    {R. Gueorguieva and J. H. Krystal},
  title =     {Move over {ANOVA}: Progress in analyzing repeated-measures data and its reflection in papers published in the archives of general psychiatry},
  journal =   {Archives of General Psychiatry},
  year =      {2004},
  volume =    {61},
  number =    {3},
  pages =     {310--317},
  doi =       {10.1001/archpsyc.61.3.310},
  keywords =  {rank5},
  owner =     {chrisbau},
  timestamp = {2018.01.23}
}

@InProceedings{Guimbretiere2003,
  author =    {Guimbreti\`{e}re, Fran\c{c}ois},
  title =     {Paper Augmented Digital Documents},
  booktitle = {Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology},
  year =      {2003},
  series =    {UIST '03},
  pages =     {51--60},
  address =   {Vancouver, British Columbia, Canada},
  publisher = {ACM},
  acmid =     {964702},
  doi =       {10.1145/964696.964702},
  file =      {Guimbretiere2003.pdf:Guimbretiere2003.pdf:PDF},
  isbn =      {1-58113-636-6},
  keywords =  {PADD, anoto, digital pen, paper augmented digital document, paper based user interface, rank5},
  numpages =  {10}
}

@Article{Gupta2013,
  author =    {Shika Gupta and Jafreezal Jaafar and Wan Fatimah wan Ahmad and Arpit Bansal},
  title =     {Fetaure Extraction using MFCC},
  journal =   {Signal \& Image Processing: An International Journal},
  year =      {2013},
  volume =    {4},
  number =    {4},
  pages =     {101--108},
  file =      {Gupta2013.pdf:Gupta2013.pdf:PDF},
  owner =     {Chris},
  review =    {Using MFCCs for image recognition of hand gestures.},
  timestamp = {2013.11.28}
}

@InProceedings{Huerst2008,
  author =    {H\"{u}rst, Wolfgang and G\"{o}tz, Georg},
  title =     {Interface Designs for Pen-based Mobile Video Browsing},
  booktitle = {Proceedings of the 7th ACM Conference on Designing Interactive Systems},
  year =      {2008},
  series =    {DIS '08},
  pages =     {395--404},
  address =   {Cape Town, South Africa},
  publisher = {ACM},
  acmid =     {1394488},
  doi =       {10.1145/1394445.1394488},
  file =      {Huerst2008.pdf:Huerst2008.pdf:PDF},
  isbn =      {978-1-60558-002-9},
  keywords =  {handheld devices, interaction modes, interactive navigation, mobile video, pen-based computing, video browsing, rank2},
  numpages =  {10}
}

@InProceedings{Huerst2004,
  author =    {H\"{u}rst, Wolfgang and Lauer, Tobias and G\"{o}tz, Georg},
  title =     {Interactive Manipulation of Replay Speed While Listening to Speech Recordings},
  booktitle = {Proceedings of the 12th Annual ACM International Conference on Multimedia},
  year =      {2004},
  series =    {MULTIMEDIA '04},
  pages =     {488--491},
  address =   {New York, NY, USA},
  publisher = {ACM},
  acmid =     {1027645},
  doi =       {10.1145/1027527.1027645},
  file =      {Huerst2004.pdf:Huerst2004.pdf:PDF},
  isbn =      {1-58113-893-8},
  keywords =  {UI metaphors, elastic interfaces, multimedia interaction, speech interfaces, speech skimming, time-scaled speech replay, rank2},
  numpages =  {4},
  review =    {Elastic slider interface test which uses SOLA for speech skimming.
	Conducts unconvincing user study.},
  status =    {printed, read}
}

@InProceedings{Huerst2006,
  author =    {H\"{u}rst, Wolfgang and Lauer, Tobias and Kaschuba, Robert},
  title =     {Interfaces for Interactive Audio-visual Media Browsing},
  booktitle = {Proceedings of the 14th Annual ACM International Conference on Multimedia},
  year =      {2006},
  series =    {MULTIMEDIA '06},
  pages =     {807--808},
  address =   {Santa Barbara, CA, USA},
  publisher = {ACM},
  acmid =     {1180819},
  doi =       {10.1145/1180639.1180819},
  file =      {Huerst2006.pdf:Huerst2006.pdf:PDF},
  isbn =      {1-59593-447-2},
  keywords =  {audio interfaces, audio skimming, multimedia browsing, rank2},
  numpages =  {2}
}

@InProceedings{Hamel2010,
  author =    {Philippe Hamel and Douglas Eck},
  title =     {Learning Features from Music Audio with Deep Belief Networks},
  booktitle = {Proc. International Society for Music Information Retrieval Conference},
  year =      {2010},
  file =      {Hamel2010.pdf:Hamel2010.pdf:PDF},
  owner =     {Chris},
  status =    {printed},
  timestamp = {2014.06.19}
}

@InProceedings{Hamel2011,
  author =    {Philippe Hamel and Simon Lemieux and Yoshua Bengio and Douglas Eck},
  title =     {Temporal Pooling and Multiscale Learning for Automatic Annotation and Ranking of Music Audio},
  booktitle = {Proc. International Society for Music Information Retrieval Conference},
  year =      {2011},
  file =      {Hamel2011.pdf:Hamel2011.pdf:PDF},
  owner =     {Chris},
  status =    {printed},
  timestamp = {2014.06.19}
}

@Book{Harper2001,
  title =     {The Myth of the Paperless Office},
  publisher = {MIT Press},
  year =      {2001},
  author =    {Harper, Richard and Sellen, Abigail J},
  keywords =  {rank2},
  review =    {Bit old now.
	
	"Over the past thirty years, many people have proclaimed the imminent
	arrival of the paperless office. Yet even the World Wide Web, which
	allows almost any computer to read and display another computer's
	documents, has increased the amount of printing done. The use of
	e-mail in an organization causes an average 40 percent increase in
	paper consumption. In The Myth of the Paperless Office, Abigail Sellen
	and Richard Harper use the study of paper as a way to understand
	the work that people do and the reasons they do it the way they do.
	Using the tools of ethnography and cognitive psychology, they look
	at paper use from the level of the individual up to that of organizational
	culture. Central to Sellen and Harper's investigation is the concept
	of affordances--the activities that an object allows, or affords.
	The physical properties of paper (its being thin, light, porous,
	opaque, and flexible) afford the human actions of grasping, carrying,
	folding, writing, and so on. The concept of affordance allows them
	to compare the affordances of paper with those of existing digital
	devices. They can then ask what kinds of devices or systems would
	make new kinds of activities possible or better support current activities.
	The authors argue that paper will continue to play an important role
	in office life. Rather than pursue the ideal of the paperless office,
	we should work toward a future in which paper and electronic document
	tools work in concert and organizational processes make optimal use
	of both."}
}

@InProceedings{Hart2006,
  author =       {Hart, Sandra G},
  title =        {NASA-Task Load Index (NASA-TLX); 20 years later},
  booktitle =    {Proceedings of the Human Factors and Ergonomic Society annual meeting},
  year =         {2006},
  volume =       {50},
  number =       {9},
  pages =        {904--908},
  organization = {Sage},
  doi =          {10.1177/154193120605000909},
  file =         {Hart2006.pdf:Hart2006.pdf:PDF},
  keywords =     {rank3}
}

@InCollection{Hart1988,
  author =    {Sandra G. Hart and Lowell E. Staveland},
  title =     {{Development of NASA-TLX (Task Load Index): Results of Empirical and Theoretical Research}},
  booktitle = {Human Mental Workload},
  publisher = {North-Holland},
  year =      {1988},
  volume =    {52},
  series =    {Advances in Psychology },
  pages =     {139 --183},
  abstract =  {The results of a multi-year research program to identify the factors
	associated with variations in subjective workload within and between
	different types of tasks are reviewed. Subjective evaluations of
	10 workload-related factors were obtained from 16 different experiments.
	The experimental tasks included simple cognitive and manual control
	tasks, complex laboratory and supervisory control tasks, and aircraft
	simulation. Task-, behavior-, and subject-related correlates of subjective
	workload experiences varied as a function of difficulty manipulations
	within experiments, different sources of workload between experiments,
	and individual differences in workload definition. A multi-dimensional
	rating scale is proposed in which information about the magnitude
	and sources of six workload-related factors are combined to derive
	a sensitive and reliable estimate of workload.},
  doi =       {10.1016/S0166-4115(08)62386-9},
  file =      {Hart1988.pdf:Hart1988.pdf:PDF},
  issn =      {0166-4115},
  keywords =  {rank5}
}

@InProceedings{Hasegawa-Johnson2011,
  author =    {Mark Hasegawa-Johnson and Camille Goudeseune and Jennifer Cole and Hank Kaczmarski and Heejin Kim and Sarah King and Timothy Mahrt and Jui-Ting Huang and Xiaodan Zhuang and Kai-Hsiang Lin and Harsh Vardhan Sharma and Zhen Li and Thomas S. Huang},
  title =     {Multimodal Speech and Audio User Interfaces for K-12 Outreach},
  booktitle = {Proc. Asia-Pacific Signal and Information Processing Association Conference},
  year =      {2011},
  file =      {Hasegawa-Johnson2011.pdf:Hasegawa-Johnson2011.pdf:PDF},
  keywords =  {rank2},
  owner =     {Chris},
  timestamp = {2014.01.22}
}

@Book{Hausman2012,
  title =     {Modern Radio Production: Production Programming \& Performance},
  publisher = {Cengage Learning},
  year =      {2012},
  author =    {Hausman, C. and Messere, F. and O'Donnell, L. and Benoit, P.},
  isbn =      {9781133712244},
  keywords =  {rank5}
}

@InProceedings{Heeren2008,
  author =    {Heeren, Willemijn and de Jong, Franciska},
  title =     {Disclosing Spoken Culture: User Interfaces for Access to Spoken Word Archives},
  booktitle = {Proceedings of the 22nd British HCI Group Annual Conference on People and Computers: Culture, Creativity, Interaction},
  year =      {2008},
  volume =    {1},
  series =    {BCS-HCI '08},
  pages =     {23--32},
  address =   {Liverpool, United Kingdom},
  publisher = {British Computer Society},
  acmid =     {1531518},
  file =      {Heeren2008.pdf:Heeren2008.pdf:PDF},
  isbn =      {978-1-906124-04-5},
  keywords =  {audio transcripts, speech browsing support, spoken document retrieval, user evaluation, rank1},
  numpages =  {10},
  review =    {Interface design for accessing large speech archives using aligned
	and generated transcripts (?)},
  url =       {http://dl.acm.org/citation.cfm?id=1531514.1531518}
}

@Article{Heiman1986,
  author =    {Heiman, Gary W. and Leo, Raphael J. and Leighbody, Glenn and Bowler, Kathleen},
  title =     {Word intelligibility decrements and the comprehension of time-compressed speech},
  journal =   {Perception {\&} Psychophysics},
  year =      {1986},
  volume =    {40},
  number =    {6},
  pages =     {407--411},
  month =     nov,
  abstract =  {The extent to which decreased comprehension of time-compressed messages results from decreased word intelligibility was investigated. Experiment 1, in which 500-word messages were temporally interrupted, demonstrated that when 60{\%} of the signal is deleted, comprehension is reduced even without the temporal limitations of compression. Experiment 2, which employed a backward masking procedure, demonstrated that individually compressed words are less intelligible when presented within the limited time constraints of a compressed message. It was concluded that decreased comprehension of compressed messages is the result of decreased word intelligibility.},
  day =       {01},
  doi =       {10.3758/BF03208200},
  file =      {Heiman1986.pdf:Heiman1986.pdf:PDF},
  issn =      {1532-5962},
  keywords =  {rank3},
  owner =     {chrisbau},
  timestamp = {2017.11.27}
}

@Article{Hendley2014,
  author =    {Hendley, Robert J and Beale, Russell and Bowers, Chris P and Georgousopoulos, Christos and Vassiliou, Charalampos and Sergios, Petridis and Moeller, Ralf and Karstens, Eric and Spiliotopoulos, Dimitris},
  title =     {CASAM: collaborative human-machine annotation of multimedia},
  journal =   {Multimedia tools and applications},
  year =      {2014},
  volume =    {70},
  number =    {2},
  pages =     {1277--1308},
  file =      {Hendley2014.pdf:Hendley2014.pdf:PDF},
  keywords =  {rank2},
  publisher = {Springer},
  review =    {Computer suggested tags, confirmed by human. Computer also asks direct
	questions when it's not sure.}
}

@Book{Hines2008,
  title =     {The Story of Broadcasting House},
  publisher = {Merrell},
  year =      {2008},
  author =    {Mark Hines},
  isbn =      {978-1-8589-4421-0},
  owner =     {chrisbau},
  timestamp = {2018.01.20}
}

@InProceedings{Hirschberg1992,
  author =    {Hirschberg, Julia and Grosz, Barbara},
  title =     {Intonational Features of Local and Global Discourse Structure},
  booktitle = {Proceedings of the Workshop on Speech and Natural Language},
  year =      {1992},
  series =    {HLT '91},
  pages =     {441--446},
  address =   {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid =     {1075632},
  doi =       {10.3115/1075527.1075632},
  file =      {Hirschberg1992.pdf:Hirschberg1992.pdf:PDF},
  isbn =      {1-55860-272-0},
  location =  {Harriman, New York},
  numpages =  {6}
}

@InProceedings{Ho-Ching2003,
  author =    {F. Wailing Ho-Ching and Jennifer Mankoff and James A. Landay},
  title =     {Can you see what I hear? The Design and Evaluation of a Peripheral Sound Display for the Deaf},
  booktitle = {Proc. ACM Conference on Human Factors in Computing Systems (CHI)},
  year =      {2003},
  abstract =  {We developed two visual displays for providing awareness of environmental
	audio to deaf individuals. Based on fieldwork with deaf and hearing
	participants, we focused on supporting awareness of non-speech audio
	sounds such as ringing phones and knocking in a work environment.
	Unlike past work, our designs do not require a priori knowledge of
	sounds to be detected, support discovery of new sounds, and support
	both monitoring and notification of sounds. Our Spectrograph design
	shows pitch and amplitude, while our Positional ripples design shows
	amplitude and location of sounds. A controlled experiment involving
	deaf participants found neither display to be significantly distracting.
	However users preferred the Positional Ripples display and found
	that display easier to monitor (notification sounds were detected
	with 90% success in a laboratory setting). The Spectrograph display
	also supported successful detection in most cases, and was well received
	when deployed in the field.},
  file =      {Ho-Ching2003.pdf:Ho-Ching2003.pdf:PDF},
  keywords =  {rank2},
  owner =     {Chris},
  review =    {Windows GUI for blind people which visualised location of sound in
	room using map, and a normal spectrogram to aid content recognition.},
  status =    {printed},
  timestamp = {2014.06.19}
}

@Article{Hori2003,
  author =     {Hori, C. and Furui, S.},
  title =      {A New Approach to Automatic Speech Summarization},
  journal =    {Trans. Multi.},
  year =       {2003},
  volume =     {5},
  number =     {3},
  pages =      {368--378},
  month =      sep,
  acmid =      {2219293},
  address =    {Piscataway, NJ, USA},
  doi =        {10.1109/TMM.2003.813274},
  file =       {Hori2003.pdf:Hori2003.pdf:PDF},
  issn =       {1520-9210},
  issue_date = {September 2003},
  keywords =   {rank3},
  numpages =   {11},
  publisher =  {IEEE Press}
}

@MastersThesis{Horner1993,
  author =    {Horner, Christopher Demetri},
  title =     {NewsTime - a graphical user interface to audio news},
  school =    {Massachusetts Institute of Technology},
  year =      {1993},
  file =      {Horner1993.pdf:Horner1993.pdf:PDF},
  keywords =  {rank3},
  owner =     {Chris},
  timestamp = {2014.01.23},
  url =       {http://hdl.handle.net/1721.1/45747}
}

@Article{Hu2004,
  author =   {Hu, Guoning and DeLiang Wang},
  title =    {Monaural speech segregation based on pitch tracking and amplitude modulation},
  journal =  {Neural Networks, IEEE Transactions on},
  year =     {2004},
  volume =   {15},
  number =   {5},
  pages =    {1135-1150},
  abstract = {Segregating speech from one monaural recording has proven to be very
	challenging. Monaural segregation of voiced speech has been studied
	in previous systems that incorporate auditory scene analysis principles.
	A major problem for these systems is their inability to deal with
	the high-frequency part of speech. Psychoacoustic evidence suggests
	that different perceptual mechanisms are involved in handling resolved
	and unresolved harmonics. We propose a novel system for voiced speech
	segregation that segregates resolved and unresolved harmonics differently.
	For resolved harmonics, the system generates segments based on temporal
	continuity and cross-channel correlation, and groups them according
	to their periodicities. For unresolved harmonics, it generates segments
	based on common amplitude modulation (AM) in addition to temporal
	continuity and groups them according to AM rates. Underlying the
	segregation process is a pitch contour that is first estimated from
	speech segregated according to dominant pitch and then adjusted according
	to psychoacoustic constraints. Our system is systematically evaluated
	and compared with pervious systems, and it yields substantially better
	performance, especially for the high-frequency part of speech.},
  doi =      {10.1109/TNN.2004.832812},
  file =     {Hu2004.pdf:Hu2004.pdf:PDF},
  issn =     {1045-9227},
  keywords = {acoustic signal processing;amplitude modulation;correlation methods;harmonics;speech enhancement;amplitude modulation;auditory scene analysis;cross-channel correlation;harmonics;monaural speech segregation;pitch contour;pitch tracking;temporal continuity;voice speech segregation;Amplitude modulation;Automatic speech recognition;Hidden Markov models;Image analysis;Interference;Power harmonic filters;Psychology;Sensor arrays;Speech analysis;Speech enhancement;AM;Amplitude modulation;computational auditory scene analysis;grouping;monaural speech segregation;pitch tracking;segmentation;SMD}
}

@Misc{RQDA,
  author =    {Ronggui Huang},
  title =     {{RQDA}: R-based Qualitative Data Analysis. R package version 0.2-8.},
  year =      {2016},
  note =      {\url{http://rqda.r-forge.r-project.org/}},
  owner =     {chrisbau},
  timestamp = {2015.08.28}
}

@Article{Hubbard1996,
  author =    {Timothy L. Hubbard},
  title =     {Synesthesia-like Mappings of Lightness, Pitch, and Melodic Interval},
  journal =   {The American Journal of Psychology},
  year =      {1996},
  volume =    {109},
  number =    {2},
  pages =     {219-238},
  abstract =  {Synesthesia-like mappings between visual lightness and auditory pitch
	and between visual lightness and melodic interval were examined.
	When subjects rated how visual lightnesses and auditory pitches "fit
	together," lighter stimuli fit better with higher pitches, and darker
	stimuli fit better with lower pitches. These patterns were stronger
	against black than against white visual backgrounds; however, effects
	of visual background were eliminated when subjects had a large set
	of lightness levels from which to choose the visual lightness level
	that best fit a given auditory pitch or melodic interval. When subjects
	chose which visual lightness best fit or matched a melodic interval,
	lighter stimuli were chosen for ascending melodic intervals, and
	darker stimuli were chosen for descending melodic intervals. Larger
	melodic intervals produced more extreme (lighter or darker) choices.
	Auditory pitch exhibits meaningful synesthesia-like mappings with
	visual lightness when unidimensionally varied in frequency and when
	multidimensionally varied in interval size and direction.},
  file =      {Hubbard1996.pdf:Hubbard1996.pdf:PDF},
  keywords =  {visualization, multimodal, rank2},
  owner =     {Chris},
  review =    {Relates visual lightness to both pitch and direction of melody. Synesthetes
	and non-synesthetes experience cross-modality in the same way ->
	common neural mapping?},
  status =    {read},
  timestamp = {2013.10.08}
}

@InProceedings{Hull2003,
  author =    {J. J. Hull and B. Erol and J. Graham and Dar-Shyang Lee},
  title =     {Visualizing multimedia content on paper documents: components of key frame selection for Video Paper},
  booktitle = {Proc. Seventh International Conference on Document Analysis and Recognition},
  year =      {2003},
  pages =     {389-392 vol.1},
  month =     aug,
  abstract =  {The components of a key frame selection algorithm for a paper-based
	multimedia browsing interface called Video Paper are described. Analysis
	of video image frames is combined with the results of processing
	the closed caption to select key frames that are printed on a paper
	document together with the closed caption. Bar codes positioned near
	the key frames allow a user to play the video from the corresponding
	times. This paper describes several component techniques that are
	being investigated for key frame selection in the Video Paper system,
	including face detection and text recognition. The Video Paper system
	implementation is also discussed.},
  doi =       {10.1109/ICDAR.2003.1227695},
  file =      {Hull2003.pdf:Hull2003.pdf:PDF},
  keywords =  {content-based retrieval, face recognition, graphical user interfaces, image recognition, multimedia systems, video recording, video signal processing, visual databases, Video Paper, bar codes, face detection, key frame selection, multimedia content visualization, paper documents, paper-based interface, paper-based multimedia browsing, selection algorithm, text recognition, video browsing, video image frames, video playing, video recording, video retrieval, Automatic control, Displays, Face detection, Image analysis, Monitoring, TV, Technological innovation, Text recognition, Video recording, Visualization, rank4},
  review =    {Thumbnails and barcodes on paper}
}

@Misc{Hyperaudio2016,
  author =    {{Hyperaudio Inc.}},
  title =     {{Hyperaudio Pad: A} transcript powered audio and video editor},
  year =      {2016},
  note =      {Accessed 15.08.16.},
  keywords =  {rank3},
  owner =     {chrisbau},
  timestamp = {2016.08.05},
  url =       {http://hyperaud.io/}
}

@InProceedings{Imai2001,
  author =    {Imai, Atsushi and Seiyama, Nobumasa and Mishima, Takeshi and Takagi, Tohru and Miyasaka, Eiichi},
  title =     {Application Of Speech Rate Conversion Technology To Video Editing: Allows Up To 5 Times Normal Speed Playback While Maintaining Speech Intelligibility},
  booktitle = {Proc. Audio Engineering Society 20th International Conference on Archiving, Restoration, and New Methods of Recording},
  year =      {2001},
  month =     oct,
  file =      {Imai2001.pdf:Imai2001.pdf:PDF},
  keywords =  {rank2},
  owner =     {chrisbau},
  timestamp = {2017.11.03},
  url =       {http://www.aes.org/e-lib/browse.cfm?elib=10055}
}

@InProceedings{Imai1983,
  author =    {S. Imai},
  title =     {Cepstral analysis synthesis on the mel frequency scale},
  booktitle = {ICASSP '83. IEEE International Conference on Acoustics, Speech, and Signal Processing},
  year =      {1983},
  volume =    {8},
  pages =     {93-96},
  month =     apr,
  abstract =  {This paper presents a new technique of cepstral analysis synthesis on the mel frequency scale, the log spectrum on the mel frequency scale (the mel log spectrum) is considered to be an effective representation of the spectral envelope of speech. This analysis synthesis system uses the mel log spectrum approximation (MLSA) filter which was devised for the cepstral synthesis on the mel frequency scale. The filter coefficients are easily obtained through a simple linear transform from the mel cepstrum defined as the Fourier cosine coefficients of the mel log spectral envelope of speech. The MLSA filter has a low coefficient sensitivity and a good coefficient quantization characteristics. The spectral distortion caused by interpolation of the filter parameters of two successive frames is small. Accordingly, the data rate of this system is very low. The same quality speech is synthesized at 60-70 % of data rates in the conventional cepstral vocoder or the LPC vocoder.},
  doi =       {10.1109/ICASSP.1983.1172250},
  file =      {Imai1983.pdf:Imai1983.pdf:PDF},
  keywords =  {Cepstral analysis;Cepstrum;Fourier transforms;Frequency synthesizers;Mel frequency cepstral coefficient;Nonlinear filters;Quantization;Speech analysis;Speech synthesis;Vocoders},
  review =    {MFCC original paper}
}

@InProceedings{Ingebretsen1982,
  author =    {Ingebretsen, Robert B. and Stockham, Jr., Thomas G.},
  title =     {Random Access Editing of Digital Audio},
  booktitle = {Proc. 72nd Audio Engineering Society Convention},
  year =      {1982},
  month =     oct,
  file =      {Ingebretsen1982.pdf:Ingebretsen1982.pdf:PDF},
  keywords =  {rank5},
  owner =     {chrisbau},
  timestamp = {2017.11.03},
  url =       {http://www.aes.org/e-lib/browse.cfm?elib=11833}
}

@Article{Inkpen2004,
  author =    {Diana Inkpen and Alain D\'{e}silets},
  title =     {Extracting Semantically-Coherent Keyphrases from Speech},
  journal =   {Canadian Acoustics},
  year =      {2004},
  file =      {Inkpen2004.pdf:Inkpen2004.pdf:PDF},
  keywords =  {rank3},
  owner =     {chrisbau},
  review =    {Replacing low-confidence transcribed words with ellipses increases
	coherence. Related to confidence shading.},
  timestamp = {2015.04.07}
}

@TechReport{ISO5776,
  author =      {{ISO 5776:2016(en)}},
  title =       {Graphic technology - Symbols for text proof correction},
  institution = {International Organization for Standardization},
  year =        {2016},
  type =        {Standard},
  file =        {ISO5776.pdf:ISO5776.pdf:PDF},
  keywords =    {rank3},
  owner =       {chrisbau},
  timestamp =   {2017.08.07}
}

@Article{Jeong2012,
  author =   {Jeong,Hanho},
  title =    {A comparison of the influence of electronic books and paper books on reading comprehension, eye fatigue, and perception},
  journal =  {The Electronic Library},
  year =     {2012},
  volume =   {30},
  number =   {3},
  pages =    {390-408},
  abstract = {Purpose - This paper aims to assess the usability of electronic books
	(e-books) and paper books (p-books) with objective measures, including
	user comprehension, eye fatigue, and perception. Design/methodology/approach
	- A total of 56 sixth-year public school students participated in
	this study. This paper was conducted in the following order: pre-CFF
	measurement, p-/e-book reading, post-CFF measurement, quiz, and questionnaire.
	A standard CFF device, a computer with a monitor for reading e-books,
	p-books, desks, and chairs were provided. Findings - This paper found
	that there is a significant "book effect" on quiz scores; compared
	to e-books, p-books appear to enable better reading comprehension.
	Regarding eye fatigue, students had significantly greater eye fatigue
	after reading e-books than after reading p-books. Students were satisfied
	with the e-book, but they preferred p-books. Research limitations/implications
	- Students would show satisfaction with e-books and acknowledge their
	usefulness, but still prefer p-books. However, a clearer understanding
	of this paradox in perception is needed. Further studies should try
	to explore the students' perceptions of e-books. Practical implications
	- Surprisingly, though, Korean students studied herein, who have
	had a higher level of exposure to technology than those in other
	countries, did not show positive behavioral intentions toward e-books.
	Overall, the responses from the Korean students suggest that there
	was general satisfaction with reading e-books on screen. However,
	this study also found a discordance in the students' perceptions
	of e-books. In this study, most students grew tired of reading on
	the screen; this tiredness could have an adverse effect on both reading
	comprehension and the perception of e-books. In further analyzing
	user responses, many of the critical remarks were found to refer
	to the screen/text size or clarity rather than to the e-book itself.
	Originality/value - Although this study suggests that students in
	general are not yet ready to entirely give up p-books, e-books are
	becoming increasingly common. However, great challenges remain in
	terms of making e-book content more available and in enabling improved
	comprehension and reducing eye fatigue.},
  doi =      {10.1108/02640471211241663},
  file =     {Jeong2012.pdf:Jeong2012.pdf:PDF},
  isbn =     {02640473},
  keywords = {Library And Information Sciences--Computer Applications; Fatigue; E-books; Studies; Reading comprehension},
  review =   {Paper books have better reading comprehension and less eye fatigue
	than electronic books}
}

@Book{Junqua1995,
  title =     {Robustness in Automatic Speech Recognition: Fundamentals and Applications},
  publisher = {Kluwer Academic Publishers},
  year =      {1995},
  author =    {Junqua, Jean-Claude and Haton, Jean-Paul},
  isbn =      {0792396464},
  keywords =  {rank3}
}

@Book{Koehler1929,
  title =     {Gestalt Psychology},
  publisher = {Liveright},
  year =      {1929},
  author =    {K\"{o}hler, W},
  keywords =  {rank5},
  owner =     {Chris},
  review =    {Original bouba/kiki paper (see Ramachandran2001 for more info).},
  timestamp = {2014.07.18}
}

@InCollection{Kacprzak2013,
  author =    {Kacprzak, Stanis{\l}aw and Zi{\'o}{\l}ko, Mariusz},
  title =     {Speech/Music Discrimination via Energy Density Analysis},
  booktitle = {Statistical Language and Speech Processing},
  publisher = {Springer Berlin Heidelberg},
  year =      {2013},
  editor =    {Dediu, Adrian-Horia and Martin-Vide, Carlos and Mitkov, Ruslan and Truthe, Bianca},
  volume =    {7978},
  series =    {Lecture Notes in Computer Science},
  pages =     {135-142},
  doi =       {10.1007/978-3-642-39593-2_12},
  file =      {Kacprzak2013.pdf:Kacprzak2013.pdf:PDF},
  isbn =      {978-3-642-39592-5},
  keywords =  {speech/music discrimination, sound classification, audio content analysis, smd, rank2},
  review =    {Best energy based SMD feature - like low-energy ratio/frames but with
	fewer parameters.},
  status =    {printed}
}

@InProceedings{Kalnikaite2012,
  author =    {Vaiva Kalnikait\.{e} and Patrick Ehlen and Steve Whittaker},
  title =     {Markup as You Talk: Establishing Effective Memory Cues While Still Contributing to a Meeting},
  booktitle = {Proc. ACM Computer-Supported Cooperative Work and Social Computing},
  year =      {2012},
  file =      {Kalnikaite2012.pdf:Kalnikaite2012.pdf:PDF},
  keywords =  {rank4},
  owner =     {chrisbau},
  review =    {Live markup of speech},
  timestamp = {2015.04.07}
}

@InProceedings{Kato2015,
  author =       {Kato, Jun and Nakano, Tomoyasu and Goto, Masataka},
  title =        {TextAlive: Integrated Design Environment for Kinetic Typography},
  booktitle =    {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
  year =         {2015},
  pages =        {3403--3412},
  organization = {ACM},
  file =         {Kato2015.pdf:Kato2015.pdf:PDF},
  keywords =     {rank5}
}

@InProceedings{Katsamanis2011,
  author =    {Katsamanis, Athanasios and Black, MP and Georgiou, Panayiotis G and Goldstein, Louis and Narayanan, S},
  title =     {SailAlign: Robust long speech-text alignment},
  booktitle = {Proc. of Workshop on New Tools and Methods for Very-Large Scale Phonetics Research},
  year =      {2011},
  file =      {Katsamanis2011.pdf:Katsamanis2011.pdf:PDF}
}

@Article{Kim2003,
  author =    {Kim, Jinmook and Oard, Douglas W. and Soergel, Dagobert},
  title =     {Searching large collections of recorded speech: A preliminary study},
  journal =   {Proceedings of the American Society for Information Science and Technology},
  year =      {2003},
  volume =    {40},
  number =    {1},
  pages =     {330--339},
  abstract =  {This paper reports on an exploratory study of the criteria searchers
	use when judging the relevance of recorded speech from radio programs
	and the attributes of a recording on which those judgments are based.
	Five volunteers each performed three searches using two systems (NPR
	Online and SpeechBot) for three questions and judged the relevance
	of the results. Data were collected through observation and screen
	capture, think aloud, and interviews; coded; and analyzed by looking
	for patterns. Criteria used as a basis for selection were found to
	be similar to those observed in relevance studies with printed materials,
	but the attributes used as a basis for assessing those criteria were
	found to exhibit modality-specific characteristics. For example,
	audio replay was often found to be necessary when assessing story
	genre (e.g., report, interview, commentary) because of limitations
	in presently available metadata. Participants reported a strong preference
	for manually prepared summaries over passages extracted from automatic
	speech recognition transcripts, and consequential differences in
	search behavior were observed between the two conditions. Some important
	implications for interface and component design are drawn, such as
	the utility of summaries at multiple levels of detail in view of
	the difficulty of skimming imperfect transcripts and the potential
	utility of automatic speaker identification to support authority
	judgments in systems.},
  doi =       {10.1002/meet.1450400141},
  file =      {Kim2003.pdf:Kim2003.pdf:PDF},
  issn =      {1550-8390},
  keywords =  {rank5},
  publisher = {Wiley Subscription Services, Inc., A Wiley Company},
  review =    {HCI-type study of navigating radio recordings with different modalities.
	Tried to recruit radio producers but failed.}
}

@Article{Kinnunen2010,
  author =    {Tomi Kinnunen and Haizhou Li},
  title =     {An overview of text-independent speaker recognition: From features to supervectors},
  journal =   {Speech Communication},
  year =      {2010},
  volume =    {52},
  number =    {1},
  pages =     {12 - 40},
  abstract =  {Abstract This paper gives an overview of automatic speaker recognition technology, with an emphasis on text-independent recognition. Speaker recognition has been studied actively for several decades. We give an overview of both the classical and the state-of-the-art methods. We start with the fundamentals of automatic speaker recognition, concerning feature extraction and speaker modeling. We elaborate advanced computational techniques to address robustness and session variability. The recent progress from vectors towards supervectors opens up a new area of exploration and represents a technology trend. We also provide an overview of this recent development and discuss the evaluation methodology of speaker recognition systems. We conclude the paper with discussion on future directions.},
  doi =       {10.1016/j.specom.2009.08.009},
  file =      {Kinnunen2010.pdf:Kinnunen2010.pdf:PDF},
  issn =      {0167-6393},
  keywords =  {Speaker recognition, Text-independence, Feature extraction, Statistical models, Discriminative models, Supervectors, Intersession variability compensation},
  owner =     {chrisbau},
  timestamp = {2018.01.14}
}

@Book{Kirwan1992,
  title =        {A Guide to Task Analysis: The Task Analysis Working Group},
  publisher =    {Taylor \& Francis},
  year =         {1992},
  author =       {Kirwan, B. and Ainsworth, L. K.},
  edition =      {1},
  month =        sep,
  abstract =     {{A practical user guide, this contains real case studies demonstrating
	the impact TA has in industrial applications. Task analysis (TA)
	is the term applied to any process that identifies and examines tasks
	performed by humans as they interact with systems. This book documents
	the application of task analytical techniques and organizes them
	into a practical user guide. It enables organizations to select the
	most appropriate and cost-effective technique to meet their particular
	needs and uses real case studies to demonstrate the impact task analysis
	has in industrial applications. "A Guide to Task Analysis" shows
	how to target TA resources optimally over the life cycle of a project,
	from conceptual design through to systems operation, noting the role
	of TA in safety and quality assurance; minimizing operator error;
	enhancing productivity; maximizing staff potential; implementing
	new technologies; and translating similar tasks across different
	task environments. It is aimed at design engineers, safety assessors,
	trainers, protocol/procedure writers, and project managers, with
	or without previous TA experience.}},
  day =          {09},
  howpublished = {Paperback},
  isbn =         {0748400583},
  owner =        {chrisbau},
  posted-at =    {2010-01-16 16:13:21},
  timestamp =    {2017.11.16}
}

@InProceedings{Klemmer2003,
  author =    {Klemmer, Scott R. and Graham, Jamey and Wolff, Gregory J. and Landay, James A.},
  title =     {Books with Voices: Paper Transcripts As a Physical Interface to Oral Histories},
  booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  year =      {2003},
  series =    {CHI '03},
  pages =     {89--96},
  address =   {Ft. Lauderdale, Florida, USA},
  publisher = {ACM},
  acmid =     {642628},
  doi =       {10.1145/642611.642628},
  file =      {Klemmer2003.pdf:Klemmer2003.pdf:PDF},
  isbn =      {1-58113-630-7},
  keywords =  {augmented reality, handheld, interactive paper, oral history, reading, tangible interface, video retrieval, rank4},
  numpages =  {8},
  review =    {Includes section called 'paper prototype of a paper interface'}
}

@InProceedings{Kobayashi1997,
  author =    {Kobayashi, Minoru and Schmandt, Chris},
  title =     {Dynamic Soundscape: Mapping Time to Space for Audio Browsing},
  booktitle = {Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems},
  year =      {1997},
  series =    {CHI '97},
  pages =     {194--201},
  address =   {Atlanta, Georgia, USA},
  publisher = {ACM},
  acmid =     {258702},
  doi =       {10.1145/258549.258702},
  file =      {Kobayashi1997.pdf:Kobayashi1997.pdf:PDF},
  isbn =      {0-89791-802-9},
  keywords =  {audio browsing, selective listening, simultaneous listening, spatial memory, spatialized audio, rank4},
  numpages =  {8}
}

@InProceedings{Konstas2012,
  author =    {Soitris Konstas},
  title =     {Audiovisual Installation: Interactive imprinting of sound in water},
  booktitle = {Proc. Electronic Visualisation and the Arts},
  year =      {2012},
  file =      {Konstas2012.pdf:Konstas2012.pdf:PDF},
  keywords =  {installation},
  owner =     {Chris},
  timestamp = {2013.11.21}
}

@InProceedings{Kroos2017,
  author =    {Christian Kroos and Mark Plumbley},
  title =     {Neuroevolution for sound event detection in real life audio: A pilot study},
  booktitle = {Proc. {DCASE} 2017},
  year =      {2017},
  month =     nov,
  abstract =  {Neuroevolution techniques combine genetic algorithms with artificial
neural networks, some of them evolving network topology
along with the network weights. One of these latter techniques is
the NeuroEvolution of Augmenting Topologies (NEAT) algorithm.
For this pilot study we devised an extended variant (joint NEAT,
J-NEAT), introducing dynamic cooperative co-evolution, and applied
it to sound event detection in real life audio (Task 3) in the
DCASE 2017 challenge. Our research question was whether small
networks could be evolved that would be able to compete with the
much larger networks now typical for classification and detection
tasks. We used the wavelet-based deep scattering transform and
k-means clustering across the resulting scales (not across samples)
to provide J-NEAT with a compact representation of the acoustic
input. The results show that for the development data set J-NEAT
was capable of evolving small networks that match the performance
of the baseline system in terms of the segment-based error metrics,
while exhibiting a substantially better event-related error rate. In
the challenge, J-NEAT took first place overall according to the F1
error metric with an F1 of 44:9\% and achieved rank 15 out of 34 on
the ER error metric with a value of 0:891. We discuss the question
of evolving versus learning for supervised tasks.},
  file =      {Kroos2017.pdf:Kroos2017.pdf:PDF},
  journal =   {Detection and Classification of Acoustic Scenes and Events (DCASE 2017) Proceedings 2017},
  keywords =  {Sound event detection, neuroevolution, NEAT, deep scattering transform, wavelets, clustering, co-evolution},
  url =       {http://epubs.surrey.ac.uk/842496/}
}

@InProceedings{Kurniawan2001,
  author =    {Sri Kurniawan And and Sri H. Kurniawan and Panayiotis Zaphiris},
  title =     {Reading Online or on Paper: Which is Faster?},
  booktitle = {Proceedings of the 9th International Conference on Human Computer Interaction},
  year =      {2001},
  pages =     {5--10},
  abstract =  {Online information is often formatted in a similar fashion to printed
	information. But are they similar in their effectiveness? The present
	study investigates the effect of information format on user's preference
	and reading time when people read online information or printed information.
	The study tested one, two and three-column formats. This study involved
	forty two participants from three main adult age groups: young (18-40
	years), middleaged (40-65 years) and seniors (65+ years). The overall
	mean age was 50.0 years (S.D. = 20.44 years). Participants were divided
	into two reading groups: online and on paper. A balanced number of
	participants from each age group was assigned to each reading treatment.
	There was no significant difference in the reading speed and preference
	between different column formats. In agreement with findings from
	previous studies, reading on paper was 10-30% faster than reading
	online. The paper concludes with suggestions to designers of online
	information. As previous studies showed, some action needs to be
	taken to improve reading speed, such as using bigger font size or
	high contrast between the text and the background.},
  file =      {Kurniawan2001.pdf:Kurniawan2001.pdf:PDF},
  review =    {In agreement with findings from previous studies, reading on paper
	was 10-30% faster than reading online.}
}

@Article{Lazar2007,
  author =    {Jonathan Lazar and Aaron Allen and Jason Kleinman and Chris Malarkey},
  title =     {What Frustrates Screen Reader Users on the Web: A Study of 100 Blind Users},
  journal =   {International Journal of Human-Computer Interaction},
  year =      {2007},
  volume =    {22},
  number =    {3},
  pages =     {247--269},
  abstract =  {In previous research, the computer frustrations of student and workplace
	users have been documented. However, the challenges faced by blind
	users on the Web have not been previously examined. In this study,
	100 blind users, using time diaries, recorded their frustrations
	using the Web. The top causes of frustration reported were (a) page
	layout causing confusing screen reader feedback; (b) conflict between
	screen reader and application; (c) poorly designed/unlabeled forms;
	(d) no alt text for pictures; and (e) 3-way tie between misleading
	links, inaccessible PDF, and a screen reader crash. Most of the causes
	of frustration, such as inappropriate form and graphic labels and
	confusing page layout, are relatively simple to solve if Webmasters
	and Web designers focus on this effort. In addition, the more technically
	challenging frustrations, such as screen reader crashes and conflicts,
	need to be addressed by the screen reader developers. Blind users
	in this study were likely to repeatedly attempt to solve a frustration,
	not give up, and not reboot the computer. In this study, the blind
	users reported losing, on average, 30.4% of time due to these frustrating
	situations. Implications for Web developers, screen reader developers,
	and screen reader users are discussed in this article.},
  file =      {Lazar2007.pdf:Lazar2007.pdf:PDF},
  keywords =  {eval},
  owner =     {chrisbau},
  timestamp = {2014.11.06}
}

@Article{Leake2017,
  author =    {Mackenzie Leake and Abe Davis and Anh Truong and Maneesh Agrawala},
  title =     {Computational Video Editing for Dialogue-Driven Scenes},
  journal =   {ACM Transactions on Graphics},
  year =      {2017},
  volume =    {36},
  number =    {4},
  month =     jul,
  abstract =  {We present a system for efficiently editing video of dialogue-driven
	scenes. The input to our system is a standard film script and multiple
	video takes, each capturing a different camera framing or performance
	of the complete scene. Our system then automatically selects the
	most appropriate clip from one of the input takes, for each line
	of dialogue, based on a user-specified set of film-editing idioms.
	Our system starts by segmenting the input script into lines of dialogue
	and then splitting each input take into a sequence of clips time-aligned
	with each line. Next, it labels the script and the clips with high-level
	structural information (e.g., emotional sentiment of dialogue, camera
	framing of clip, etc.). After this pre-process, our interface offers
	a set of basic idioms that users can combine in a variety of ways
	to build custom editing styles. Our system encodes each basic idiom
	as a Hidden Markov Model that relates editing decisions to the labels
	extracted in the pre-process. For short scenes (< 2 minutes, 8-16
	takes, 6-27 lines of dialogue) applying the user-specified combination
	of idioms to the pre-processed inputs generates an edited sequence
	in 2-3 seconds. We show that this is significantly faster than the
	hours of user time skilled editors typically require to produce such
	edits and that the quick feedback lets users iteratively explore
	the space of edit designs.},
  file =      {:Leake2017.pdf:PDF},
  owner =     {chrisbau},
  timestamp = {2017.07.25}
}

@Book{Lee1999a,
  title =     {Automatic Speech and Speaker Recognition: Advanced Topics},
  publisher = {Kluwer Academic Publishers},
  year =      {1999},
  author =    {Lee, Chin-Hui and Soong, Frank K. and Paliwal, Kuldip K.},
  address =   {Norwell, MA, USA},
  isbn =      {0792397061},
  keywords =  {rank3}
}

@Article{Lee1999,
  author =    {Daniel D. Lee and H. Sebastian Seung},
  title =     {Learning the parts of objects by non-negative matrix factorization},
  journal =   {Nature},
  year =      {1999},
  volume =    {401},
  pages =     {788--791},
  file =      {Lee1999.pdf:Lee1999.pdf:PDF},
  keywords =  {rank1},
  owner =     {Chris},
  review =    {Non-negative matrix factorization (NMF)},
  timestamp = {2014.03.11}
}

@InProceedings{Lee2007,
  author =    {Lee, Eric},
  title =     {Towards a Quantitative Analysis of Audio Scrolling Interfaces},
  booktitle = {Proc. Extended Abstracts on Human Factors in Computing Systems (CHI)},
  year =      {2007},
  series =    {CHI EA '07},
  pages =     {2213--2218},
  address =   {San Jose, CA, USA},
  publisher = {ACM},
  acmid =     {1240982},
  doi =       {10.1145/1240866.1240982},
  file =      {Lee2007.pdf:Lee2007.pdf:PDF},
  isbn =      {978-1-59593-642-4},
  keywords =  {audio interfaces, audio scrolling, empirical study, rank3},
  numpages =  {6}
}

@InProceedings{Lee2006,
  author =    {Lee, Eric and Borchers, Jan},
  title =     {{DiMa\ss}: A Technique for Audio Scrubbing and Skimming Using Direct Manipulation},
  booktitle = {Proceedings of the 1st ACM Workshop on Audio and Music Computing Multimedia},
  year =      {2006},
  series =    {AMCMM '06},
  pages =     {107--114},
  address =   {Santa Barbara, CA, USA},
  publisher = {ACM},
  abstract =  {Scrubbing or skimming through an audio-only recording remains a challenge
	with today's audio interfaces. We present DiMaß, a technique for
	direct manipulation of an audio timeline with continuous, high-fidelity
	audio feedback. Building upon prior work in interactive conducting
	systems, DiMaß uses improved algorithms to (1) estimate the input
	position and velocity of low sampling rate, relative input devices
	such as a mouse or iPod scroll wheel; (2) adjust audio play rate
	to precisely track user input; and (3) interactively time-stretch
	the audio without changing the pitch at arbitrary forwards and backwards
	play rates. Early feedback showed that users are willing to tolerate
	slightly reduced responsiveness in exchange for smoother sounding
	audio. We are currently exploring how DiMaß enables users to more
	quickly and efficiently scrub and skim through audio.},
  acmid =     {1178740},
  doi =       {10.1145/1178723.1178740},
  file =      {Lee2006.pdf:Lee2006.pdf:PDF},
  isbn =      {1-59593-501-0},
  keywords =  {audio interfaces, audio scrubbing, audio skimming, direct manipulation, synchronization, time-stretching, rank3},
  numpages =  {8}
}

@InProceedings{Levkowitz1991,
  author =    {Levkowitz, H.},
  title =     {Color icons-merging color and texture perception for integrated visualization of multiple parameters},
  booktitle = {Proc. IEEE Conference on Visualization},
  year =      {1991},
  pages =     {164-170, 420},
  month =     {Oct},
  abstract =  {A technique that harnesses color and texture perception to create
	integrated displays of 2D image-like multiparameter distributions
	is presented. The power of the technique is demonstrated by an example
	of a synthesized dataset and compared with several other proposed
	techniques. The nature of studies that are required to measure objectively
	and accurately the effectiveness of such displays is discussed},
  doi =       {10.1109/VISUAL.1991.175795},
  file =      {Levkowitz1991.pdf:Levkowitz1991.pdf:PDF},
  keywords =  {visualization colour, colour vision, computer graphics, visual perception, 2D image, colour icons, colour perception, integrated visualization, multiparameter distributions, shape, synthesized dataset, texture perception, Biomedical imaging, Computer displays, Computer science, Data visualization, Geologic measurements, Geology, Image analysis, Merging, Shape, Spatial coherence, rank2}
}

@Article{Lewis1993,
  author =    {Lewis, James R.},
  title =     {Pairs of Latin squares that produce digram-balanced Greco-Latin designs: A BASIC program},
  journal =   {Behavior Research Methods, Instruments, \& Computers},
  year =      {1993},
  volume =    {25},
  number =    {3},
  pages =     {414--415},
  doi =       {10.3758/BF03204534},
  file =      {Lewis1993.pdf:Lewis1993.pdf:PDF},
  issn =      {0743-3808},
  keywords =  {eval, rank3},
  language =  {English},
  publisher = {Springer-Verlag}
}

@InProceedings{Li2005,
  author =    {Li, Yang and Hinckley, Ken and Guan, Zhiwei and Landay, James A.},
  title =     {Experimental Analysis of Mode Switching Techniques in Pen-based User Interfaces},
  booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  year =      {2005},
  series =    {CHI '05},
  pages =     {461--470},
  address =   {Portland, Oregon, USA},
  publisher = {ACM},
  acmid =     {1055036},
  doi =       {10.1145/1054972.1055036},
  file =      {Li2005.pdf:Li2005.pdf:PDF},
  isbn =      {1-58113-998-5},
  keywords =  {gestures, ink, mode errors, mode switching, pen interfaces, rank2},
  numpages =  {10}
}

@InProceedings{Liang2005,
  author =    {Bai Liang and Hu Yaali and Lao Songyang and Chen Jianyun and Wu Lingda},
  title =     {Feature analysis and extraction for audio automatic classification},
  booktitle = {Proc. IEEE International Conference on Systems, Man and Cybernetics},
  year =      {2005},
  volume =    {1},
  pages =     {767-772 Vol. 1},
  month =     oct,
  abstract =  {Feature analysis and extraction are the foundation of audio automatic
	classification. This paper divides audio streams into five classes:
	silence, noise, pure speech, speech over background sound and music.
	We present our work on audio feature analysis and extraction on the
	frame level and clip level. Four new features are proposed, including
	silence ratio, pitch frequency standard deviation, harmonicity ratio
	and smooth pitch ratio. We have presented an SVM based approach to
	classification. The effectiveness of the features is evaluated in
	experiments. Experiment results show that the features we selected
	and proposed are rational and effective.},
  doi =       {10.1109/ICSMC.2005.1571239},
  file =      {Liang2005.pdf:Liang2005.pdf:PDF},
  keywords =  {smd audio signal processing, feature extraction, pattern classification, speech processing, support vector machines, SVM classification, audio automatic classification, audio clip level, audio feature analysis, audio feature extraction, audio frame level, content based audio classification, harmonicity ratio, noise audio stream, pitch frequency standard deviation, pure speech audio stream, silence audio stream, silence ratio, smooth pitch ratio, support vector machine, Acoustic noise, Background noise, Data mining, Feature extraction, Information analysis, Music, Speech enhancement, Streaming media, Support vector machine classification, Support vector machines, Feature analysis and extraction, content-based audio classification, support vector machines, rank3}
}

@InProceedings{Liang2014,
  author =    {Liang, Yuan and Iwano, Koji and Shinoda, Koichi},
  title =     {Simple Gesture-based Error Correction Interface for Smartphone Speech Recognition},
  booktitle = {Proc. Fifteenth Annual Conference of the International Speech Communication Association},
  year =      {2014},
  file =      {Liang2014.PDF:Liang2014.PDF:PDF},
  keywords =  {rank2},
  review =    {Smartphone-based ASR correction using gestures.}
}

@InProceedings{Lin2012,
  author =    {Kai-Hsiang Lin and Xiaodan Zhuang and Goudeseune, C. and King, S. and Hasegawa-Johnson, M. and Huang, T.S.},
  title =     {Improving faster-than-real-time human acoustic event detection by saliency-maximized audio visualization},
  booktitle = {Proc. IEEE International Conference on Acoustics, Speech and Signal Processing},
  year =      {2012},
  pages =     {2277-2280},
  abstract =  {We propose a saliency-maximized audio spectrogram as a representation
	that lets human analysts quickly search for and detect events in
	audio recordings. By rendering target events as visually salient
	patterns, this representation minimizes the time and effort needed
	to examine a recording. In particular, we propose a transformation
	of a conventional spectrogram that maximizes the mutual information
	between the spectrograms of isolated target events and the estimated
	saliency of the overall visual representation. When subjects are
	shown spectrograms that are saliency-maximized, they perform significantly
	better in a 1/10-real-time acoustic event detection task.},
  doi =       {10.1109/ICASSP.2012.6288368},
  file =      {Lin2012.pdf:Lin2012.pdf:PDF},
  issn =      {1520-6149},
  keywords =  {audio recording, audio signal processing, audio-visual systems, audio recordings, human acoustic event detection, human analyst, realtime rendering, saliency maximized audio spectrogram, saliency maximized audio visualization, salient pattern, visual representation, Acoustics, Audio recording, Event detection, Humans, Spectrogram, Speech, Visualization, acoustic event detection, audio visualization, visual saliency, rank4},
  review =    {Applies saliency mapping to spectrogram. User trial in which candidates
	had to find sound effects in meeting recordings showed that salience-mapped
	version worked better than standard spectrogram.},
  status =    {printed, read}
}

@Article{Lin2013,
  author =     {Lin, Kai-Hsiang and Zhuang, X. and Goudeseune, C. and King, S. and Hasegawa-Johnson, M. and Huang, T. S.},
  title =      {Saliency-maximized Audio Visualization and Efficient Audio-visual Browsing for Faster-than-real-time Human Acoustic Event Detection},
  journal =    {ACM Transactions on Applied Perception},
  year =       {2013},
  volume =     {10},
  number =     {4},
  pages =      {26:1--26:16},
  month =      oct,
  acmid =      {2536773},
  address =    {New York, NY, USA},
  articleno =  {26},
  doi =        {10.1145/2536764.2536773},
  file =       {Lin2013.pdf:Lin2013.pdf:PDF},
  issn =       {1544-3558},
  issue_date = {October 2013},
  keywords =   {Visual salience/saliency, acoustic event detection, audio visualization, rank4},
  numpages =   {16},
  publisher =  {ACM}
}

@Book{Lindgaard1994,
  title =     {Usability Testing and System Evaluation: A Guide for Designing Useful Computer Systems},
  publisher = {Chapman \& Hall},
  year =      {1994},
  author =    {Lindgaard, G.},
  series =    {Chapman and Hall computing series},
  isbn =      {9780412461002},
  lccn =      {gb94014684}
}

@Article{Lloret2012,
  author =    {Lloret, Elena and Palomar, Manuel},
  title =     {Text summarisation in progress: a literature review},
  journal =   {Artificial Intelligence Review},
  year =      {2012},
  volume =    {37},
  number =    {1},
  pages =     {1-41},
  doi =       {10.1007/s10462-011-9216-z},
  file =      {Lloret2012.pdf:Lloret2012.pdf:PDF},
  issn =      {0269-2821},
  keywords =  {Human language technologies, Text summarisation, Intelligent systems, rank3},
  language =  {English},
  publisher = {Springer Netherlands},
  review =    {Review of text summarisation literature}
}

@InProceedings{Long2003,
  author =    {Long, A. Chris and Casares, Juan and Myers, Brad A. and Bhatnagar, Rishi and Stevens, Scott M. and Dabbish, Laura and Yocum, Dan and Corbett, Albert},
  title =     {{SILVER: Simplifying Video Editing with Metadata}},
  booktitle = {CHI '03 Extended Abstracts on Human Factors in Computing Systems},
  year =      {2003},
  series =    {CHI EA '03},
  pages =     {628--629},
  address =   {Ft. Lauderdale, Florida, USA},
  publisher = {ACM},
  abstract =  {Digital video is becoming increasingly ubiquitous. However, editing
	video remains difficult for several reasons: it is a time-based medium,
	it has dual tracks of audio and video, and current tools force users
	to work at the smallest level of detail. Based on interviews with
	professional video editors, we developed a video editor, called Silver,
	that uses metadata to make digital video editing more accessible
	to novices. To help users visualize video, Silver provides multiple
	views with different semantic content and at different levels of
	abstraction, including storyboard, editable transcript, and timeline
	views. Silver offers smart editing operations that help users resolve
	the inconsistencies that arise because of the different boundaries
	in audio and video.},
  acmid =     {765898},
  doi =       {10.1145/765891.765898},
  file =      {Long2003.pdf:Long2003.pdf:PDF},
  isbn =      {1-58113-637-4},
  keywords =  {hierarchical video editing, video metadata, rank5},
  numpages =  {2}
}

@InProceedings{Loviscach2011,
  author =       {J\"{o}rn Loviscach},
  title =        {The Quintessence of a Waveform: Focus and Context for Audio Track Displays},
  booktitle =    {Proc. 130th Audio Engineering Society Convention},
  year =         {2011},
  organization = {Audio Engineering Society},
  file =         {Loviscach2011.pdf:Loviscach2011.pdf:PDF},
  keywords =     {rank5},
  url =          {http://www.aes.org/e-lib/browse.cfm?elib=15864}
}

@InProceedings{Loviscach2011a,
  author =    {J\"{o}rn Loviscach},
  title =     {A Nimble Video Editor that Puts Audio First},
  booktitle = {Proc. 131st Audio Engineering Society Convention},
  year =      {2011},
  file =      {Loviscach2011a.pdf:Loviscach2011a.pdf:PDF},
  keywords =  {rank5},
  owner =     {chrisbau},
  timestamp = {2014.11.18},
  url =       {http://www.aes.org/e-lib/browse.cfm?elib=16023}
}

@Unpublished{Loviscach2013,
  author =    {J\"{o}rn Loviscach},
  title =     {Semantic Analysis to Help Editing Recorded Speech},
  note =      {Presented at 134th AES Convention, Rome, Italy},
  month =     may,
  year =      {2013},
  comment =   {Accessed 03/02/2018},
  file =      {Loviscach2013.pdf:Loviscach2013.pdf:PDF},
  keywords =  {rank5},
  owner =     {chrisbau},
  timestamp = {2017.12.05},
  url =       {http://www.j3l7h.de/talks/2013-05-04_Semantic_Analysis_to_Help_Editing_Recorded_Speech.pdf}
}

@InBook{Lu2009,
  chapter =   {4},
  pages =     {1--39},
  title =     {Audio Content Discovery: An Unsupervised Approach},
  publisher = {Springer US},
  year =      {2009},
  author =    {Lu, Lie and Hanjalic, Alan},
  editor =    {Divakaran, Ajay},
  address =   {Boston, MA},
  abstract =  {Automatically extracting semantic content from audio streams can be helpful in many multimedia applications. Motivated by the known limitations of traditional supervised approaches to content extraction, which are hard to generalize and require suitable training data, we propose in this chapter a completely unsupervised approach to content discovery in composite audio signals. The approach adopts the ideas from text analysis to find the fundamental and representative audio segments (analog to words and keywords), and to employ them for parsing a general audio document into meaningful ``paragraphs'' and ``paragraphs'' clusters. In our approach, we first employ spectral clustering to discover natural semantic sound clusters (e.g. speech, music, noise, applause, speech              mixed with              music). These clusters are referred to as audio elements, and analog to words in text analysis. Based on the obtained set of audio elements, the key audio elements, which are most prominent in characterizing the content of input audio data, are selected. The obtained (key) audio elements are then used to detect potential boundaries of semantic audio ``paragraphs'' denoted as auditory scenes, which are finally clustered in terms of the audio elements appearing therein, by investigating the relations between audio elements and auditory scenes with an information-theoretic co-clustering scheme. Evaluations of the proposed approach performed on 5 hours of diverse audio data indicate that promising results can be achieved, both regarding audio element discovery and auditory scene segmentation/clustering.},
  booktitle = {Multimedia Content Analysis: Theory and Applications},
  doi =       {10.1007/978-0-387-76569-3_4},
  file =      {Lu2009.pdf:Lu2009.pdf:PDF},
  isbn =      {978-0-387-76569-3},
  keywords =  {rank4},
  owner =     {chrisbau},
  timestamp = {2017.12.07}
}

@Book{Luff2000,
  title =     {Workplace Studies: Recovering Work Practice and Informing System Design},
  publisher = {Cambridge University Press},
  year =      {2000},
  author =    {Luff, P. and Hindmarsh, J. and Heath, C.},
  doi =       {10.1017/CBO9780511628122},
  file =      {Luff2000.pdf:Luff2000.pdf:PDF},
  keywords =  {rank5},
  place =     {Cambridge}
}

@InProceedings{Mackay1998,
  author =    {Mackay, Wendy E. and Fayard, Anne-Laure and Frobert, Laurent and M{\'e}dini, Lionel},
  title =     {Reinventing the Familiar: Exploring an Augmented Reality Design Space for Air Traffic Control},
  booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  year =      {1998},
  series =    {CHI '98},
  pages =     {558--565},
  address =   {Los Angeles, California, USA},
  publisher = {ACM Press/Addison-Wesley Publishing Co.},
  acmid =     {274719},
  doi =       {10.1145/274644.274719},
  file =      {Mackay1998.pdf:Mackay1998.pdf:PDF},
  isbn =      {0-201-30987-4},
  keywords =  {augmented reality, design space, interactive paper, participatory design, video prototyping},
  numpages =  {8},
  review =    {Paper can be spatially organised in ways rarely supported by digital
	tools}
}

@Article{Mangen2013,
  author =   {Anne Mangen and Bente R. Walgermo and Kolbj{\o}rn Br{\o}nnick},
  title =    {Reading linear texts on paper versus computer screen: Effects on reading comprehension},
  journal =  {International Journal of Educational Research },
  year =     {2013},
  volume =   {58},
  pages =    {61 - 68},
  abstract = {Objective To explore effects of the technological interface on reading
	comprehension in a Norwegian school context. Participants 72 tenth
	graders from two different primary schools in Norway. Method The
	students were randomized into two groups, where the first group read
	two texts (1400-2000 words) in print, and the other group read the
	same texts as \{PDF\} on a computer screen. In addition pretests
	in reading comprehension, word reading and vocabulary were administered.
	A multiple regression analysis was carried out to investigate to
	what extent reading modality would influence the students' scores
	on the reading comprehension measure. Conclusion Main findings show
	that students who read texts in print scored significantly better
	on the reading comprehension test than students who read the texts
	digitally. Implications of these findings for policymaking and test
	development are discussed.},
  doi =      {10.1016/j.ijer.2012.12.002},
  file =     {Mangen2013.pdf:Mangen2013.pdf:PDF},
  issn =     {0883-0355},
  keywords = {Reading comprehension, rank5},
  review =   {Comprehension of text is better on paper than on screen.}
}

@InBook{Maragos2008,
  pages =     {1--46},
  title =     {Cross-Modal Integration for Performance Improving in Multimedia: A Review},
  publisher = {Springer US},
  year =      {2008},
  author =    {Maragos, Petros and Gros, Patrick and Katsamanis, Athanassios and Papandreou, George},
  editor =    {Maragos, Petros and Potamianos, Alexandros and Gros, Patrick},
  address =   {Boston, MA},
  booktitle = {Multimodal Processing and Interaction: Audio, Video, Text},
  doi =       {10.1007/978-0-387-76316-3_1},
  file =      {Maragos2008.pdf:Maragos2008.pdf:PDF},
  isbn =      {978-0-387-76316-3},
  keywords =  {rank3},
  review =    {Contains some useful bits on audio features and cross-modality. Useful for background chapter.}
}

@Article{Margolin2013,
  author =   {Margolin, Sara J. and Driscoll, Casey and Toland, Michael J. and Kegler, Jennifer Little},
  title =    {E-readers, Computer Screens, or Paper: Does Reading Comprehension Change Across Media Platforms?},
  journal =  {Applied Cognitive Psychology},
  year =     {2013},
  volume =   {27},
  number =   {4},
  pages =    {512--519},
  abstract = {The present research examined the impact of technology on reading
	comprehension. While previous research has examined memory for text,
	and yielded mixed results of the impact technology has on one's ability
	to remember what they have read, the reading literature has not yet
	examined comprehension. In comparing paper, computers, and e-readers,
	results from this study indicated that these three different presentation
	modes do not differentially affect comprehension of narrative or
	expository text. Additionally, readers were not consistently compensating
	for difficulties with comprehension by engaging in different reading
	behaviors when presented with text in different formats. These results
	suggest that reading can happen effectively in a variety of presentation
	formats. Copyright Â© 2013 John Wiley & Sons, Ltd.},
  doi =      {10.1002/acp.2930},
  file =     {Margolin2013.pdf:Margolin2013.pdf:PDF},
  issn =     {1099-0720},
  review =   {Paper, computers, and e-readers do not differentially affect comprehension
	of narrative or expository text.}
}

@Article{Marks2003,
  author =   {Lawrence E Marks and Elisheva Ben-Artzi and Stephen Lakatos},
  title =    {Cross-modal interactions in auditory and visual discrimination},
  journal =  {International Journal of Psychophysiology },
  year =     {2003},
  volume =   {50},
  number =   {1-2},
  pages =    {125--145},
  abstract = {Three experiments examined auditory-visual interactions using two
	sensory discrimination paradigms. Experiments 1 and 2 used a one-interval
	confidence-rating procedure and found modest effects of concurrent
	visual stimulation on auditory pitch and loudness discrimination,
	but little effect of auditory stimulation on visual brightness discrimination.
	The cross-modal interactions could have either a sensory or decisional
	basis. Experiment 3 used a two-interval same-different procedure
	and found no effect of visual stimulation on auditory sensitivity
	in pitch discrimination, and very little effect of auditory stimulation
	on visual sensitivity in brightness discrimination. Although the
	ensemble of results could be explained by sensory facilitation and/or
	inhibition that varies with the behavioral task, the pattern of these
	and related findings suggests instead that the cross-modal interactions
	result primarily from relatively late decisional processes (e.g.
	shifts in response criterion or 'bias').},
  doi =      {10.1016/S0167-8760(03)00129-6},
  file =     {Marks2003.pdf:Marks2003.pdf:PDF},
  issn =     {0167-8760},
  keywords = {Auditory and visual discrimination, rank3},
  review =   {Psychology study into auditory/visual links using user ratings.},
  status =   {printed}
}

@InProceedings{Marshall1997,
  author =    {Marshall, Catherine C.},
  title =     {Annotation: From Paper Books to the Digital Library},
  booktitle = {Proceedings of the Second ACM International Conference on Digital Libraries},
  year =      {1997},
  series =    {DL '97},
  pages =     {131--140},
  address =   {Philadelphia, Pennsylvania, USA},
  publisher = {ACM},
  acmid =     {263806},
  doi =       {10.1145/263690.263806},
  file =      {Marshall1997.pdf:Marshall1997.pdf:PDF},
  isbn =      {0-89791-868-1},
  keywords =  {annotation, annotation systems design, digital library, markings, reading tools, study, rank3},
  numpages =  {10},
  review =    {Looks at paper annotations for marking assignments.}
}

@InProceedings{Mason2007,
  author =    {Mason, Andrew and Evans, Michael J. and Sheikh, Alia},
  title =     {Music Information Retrieval in Broadcasting: Some Visual Applications},
  booktitle = {Proc. 123rd Audio Engineering Society Convention},
  year =      {2007},
  month =     oct,
  file =      {Mason2007.pdf:Mason2007.pdf:PDF},
  keywords =  {rank5},
  url =       {http://www.aes.org/e-lib/browse.cfm?elib=14296}
}

@InProceedings{Masoodian2006,
  author =    {Masoodian, M. and Rogers, B. and Ware, D. and McKoy, S.},
  title =     {{TRAED}: Speech Audio Editing using Imperfect Transcripts},
  booktitle = {Proc. 12th International Multi-Media Modelling Conference},
  year =      {2006},
  abstract =  {Although digital recording of speech is widespread, and an increasing
	range of applications allow recording and inclusion of speech data
	in documents, editing and retrieval of speech audio remains generally
	a challenging task. We have previously developed a speech audio editing
	and browsing application which utilizes imperfect transcripts of
	speech as a mechanism for text-based editing and retrieval of speech
	audio documents. This paper presents a second prototype, called TRAED,
	which enhances the functionality provided by our earlier prototype,
	and further facilitates the task of speech audio editing and access},
  doi =       {10.1109/MMMC.2006.1651371},
  file =      {Masoodian2006.pdf:Masoodian2006.pdf:PDF},
  keywords =  {audio signal processing, information retrieval, speech processing, text editing, TRAED, digital recording, imperfect transcripts, speech audio access, speech audio browsing, speech audio documents, speech audio editing, speech audio retrieval, text-based editing, Audio recording, Auditory displays, Automatic speech recognition, CD recording, Digital audio broadcasting, Digital recording, Information retrieval, Music, Prototypes, Speech enhancement, rank5}
}

@InProceedings{Massie1985,
  author =    {Dana C. Massie and Evan Brooks and Peter Gotcher},
  title =     {Software vs. Hardware Synthesis; A Reconciliation},
  booktitle = {Proc. 78th Audio Engineering Society Convention},
  year =      {1985},
  abstract =  {An integrated computer music system is described which combines an
	Emulator II polyphonic digital sampling keyboard and a Macintosh
	personal computer to permit flexible analysis, modification, and
	synthesis of musical signals. System capabilities include algorithms
	such as weighted overlap-add phase vocoder, non-linear waveshaping,
	time-varying additive synthesis, Karplus Strong, graphical editing
	of synthesis parameters and waveforms, and efficient real-time performance
	of sounds.},
  file =      {Massie1985.pdf:Massie1985.pdf:PDF},
  keywords =  {rank5},
  owner =     {Chris},
  timestamp = {2014.08.05}
}

@Article{Matsuo2004,
  author =   {Matsuo, Y. and Ishizuka, M.},
  title =    {Keyword extraction from a single document using word co-occurrence statistical information},
  journal =  {International Journal on Artificial Intelligence Tools},
  year =     {2004},
  volume =   {13},
  number =   {01},
  pages =    {157-169},
  doi =      {10.1142/S0218213004001466},
  file =     {Matsuo2004.pdf:Matsuo2004.pdf:PDF},
  keywords = {rank3}
}

@Article{McGill1978,
  author =              {McGill, Robert and Tukey, John W. and Larsen, Wayne A.},
  title =               {Variations of Box Plots},
  journal =             {The American Statistician},
  year =                {1978},
  volume =              {32},
  number =              {1},
  pages =               {pp. 12-16},
  abstract =            {Box plots display batches of data. Five values from a set of data
	are conventionally used; the extremes, the upper and lower hinges
	(quartiles), and the median. Such plots are becoming a widely used
	tool in exploratory data analysis and in preparing visual summaries
	for statisticians and nonstatisticians alike. Three variants of the
	basic display, devised by the authors, are described. The first visually
	incorporates a measure of group size; the second incorporates an
	indication of rough significance of differences between medians;
	the third combines the features of the first two. These techniques
	are displayed by examples.},
  copyright =           {Copyright © 1978 American Statistical Association},
  file =                {McGill1978.pdf:McGill1978.pdf:PDF},
  issn =                {00031305},
  jstor_articletype =   {research-article},
  jstor_formatteddate = {Feb., 1978},
  keywords =            {stats},
  language =            {English},
  publisher =           {American Statistical Association},
  url =                 {http://www.jstor.org/stable/2683468}
}

@Book{McLeish2015,
  title =     {Radio production},
  publisher = {Focal Press},
  year =      {2015},
  author =    {McLeish, Robert and Link, Jeff},
  isbn =      {1138819972},
  keywords =  {rank5}
}

@Article{Metatla2016,
  author =   {Metatla, Oussama and Martin, Fiore and Parkinson, Adam and Bryan-Kinns, Nick and Stockman, Tony and Tanaka, Atau},
  title =    {Audio-haptic interfaces for digital audio workstations},
  journal =  {Journal on Multimodal User Interfaces},
  year =     {2016},
  volume =   {10},
  number =   {3},
  pages =    {247--258},
  abstract = {We examine how auditory displays, sonification and haptic interaction
	design can support visually impaired sound engineers, musicians and
	audio production specialists access to digital audio workstation.
	We describe a user-centred approach that incorporates various participatory
	design techniques to help make the design process accessible to this
	population of users. We also outline the audio-haptic designs that
	results from this process and reflect on the benefits and challenges
	that we encountered when applying these techniques in the context
	of designing support for audio editing.},
  doi =      {10.1007/s12193-016-0217-8},
  file =     {Metatla2016.pdf:Metatla2016.pdf:PDF},
  issn =     {1783-8738},
  keywords = {rank4}
}

@InProceedings{Milota2015,
  author =    {Milota, Andre D.},
  title =     {The Application of Word Processor UI Paradigms to Audio and Animation Editing},
  booktitle = {Proceedings of the 2015 ACM on International Conference on Multimodal Interaction},
  year =      {2015},
  series =    {ICMI '15},
  pages =     {363--364},
  address =   {Seattle, Washington, USA},
  publisher = {ACM},
  abstract =  {This demonstration showcases Quixotic, an audio editor, and Quintessence,
	an animation editor. Both appropriate many of the interaction techniques
	found in word processors, and allow users to more quickly create
	time-variant media. Our different approach to the interface aims
	to make recorded speech and simple animation into media that can
	be efficiently used for one-to-one asynchronous communications, quick
	note taking and documentation, as well as for idea refinement.},
  acmid =     {2823292},
  doi =       {10.1145/2818346.2823292},
  file =      {Milota2015.pdf:Milota2015.pdf:PDF},
  isbn =      {978-1-4503-3912-4},
  keywords =  {animation, audio editing, word processing, rank3},
  numpages =  {2},
  review =    {Interface for inserting audio clips into a word processor and mixing
	it with/using it like text.}
}

@InProceedings{Misra2004,
  author =    {H. Misra and S. Ikbal and H. Bourlard and H. Hermansky},
  title =     {Spectral entropy based feature for robust ASR},
  booktitle = {Proc. IEEE International Conference on Acoustics, Speech and Signal Processing},
  year =      {2004},
  abstract =  {In general, entropy gives us a measure of the number of bits required
	to represent some information. When applied to probability mass function
	(PMF), entropy can also be used to measure the "peakiness" of a distribution.
	In this paper, we propose using the entropy of short time Fourier
	transform spectrum, normalised as PMF, as an additional feature for
	automatic speech recognition (ASR). It is indeed expected that a
	peaky spectrum, representation of clear formant structure in the
	case of voiced sounds, will have low entropy, while a flatter spectrum
	corresponding to non-speech or noisy regions will have higher entropy.
	Extending this reasoning further, we introduce the idea of multi-band/multi-resolution
	entropy feature where we divide the spectrum into equal size sub-bands
	and compute entropy in each sub-band. The results presented in this
	paper show that multi-band entropy features used in conjunction with
	normal cepstral features improve the performance of ASR system.},
  file =      {Misra2004.pdf:Misra2004.pdf:PDF},
  keywords =  {SMD},
  owner =     {Chris},
  review =    {Defines spectral entropy which is a measure of the peakiness of the
	spectrum. Used for finding presence of formants in noisy speech.},
  status =    {printed, read},
  timestamp = {2013.11.15}
}

@InProceedings{Moreland2009,
  author =    {Kenneth Moreland},
  title =     {Diverging Color Maps for Scientific Visualization},
  booktitle = {Proceedings of the 5th International Symposium on Visual Computing},
  year =      {2009},
  abstract =  {One of the most fundamental features of scientific visualization is
	the process of mapping scalar values to colors. This process allows
	us to view scalar fields by coloring surfaces and volumes. Unfortunately,
	the majority of scientific visualization tools still use a color
	map that is famous for its ineffectiveness: the rainbow color map.
	This color map, which naively sweeps through the most saturated colors,
	is well known for its ability to obscure data, introduce artifacts,
	and confuse users. Although many alternate color maps have been proposed,
	none have achieved widespread adoption by the visualization community
	for scientific visualization. This paper explores the use of diverging
	color maps (sometimes also called ratio, bipolar, or double-ended
	color maps) for use in scientific visualization, provides a diverging
	color map that generally performs well in scientific visualization
	applications, and presents an algorithm that allows users to easily
	generate their own customized color maps.},
  doi =       {10.1007/978-3-642-10520-3_9},
  file =      {Moreland2009.pdf:Moreland2009.pdf:PDF},
  keywords =  {visualization, rank2},
  owner =     {Chris},
  review =    {\url{http://www.sandia.gov/~kmorel/documents/ColorMaps/}},
  timestamp = {2014.01.14}
}

@Misc{Morgan2015,
  author =       {Josh Morgan},
  title =        {How Podcasts Have Changed in Ten Years: By the Numbers},
  howpublished = {Medium},
  month =        sep,
  year =         {2015},
  owner =        {chrisbau},
  timestamp =    {2018.01.27},
  url =          {https://medium.com/@monarchjogs/how-podcasts-have-changed-in-ten-years-by-the-numbers-720a6e984e4e}
}

@Article{Moss2015,
  author =   {Moss,Brett},
  title =    {The Evolving Face of Radio Production},
  journal =  {Radio World},
  year =     {2015},
  volume =   {39},
  number =   {13},
  pages =    {22-24},
  month =    {May 20},
  note =     {Copyright - Copyright New Bay Media LLC May 20, 2015; Document feature - Photographs; Last updated - 2015-05-31},
  abstract = {In an interview, Pete Presnel, creative director for Crawford broadcasting's
	Detroit cluster, WMUZ(FM), WEXL(AM) and WRDT(AM); and Greg Clancy,
	GM/VP of creative at TM Studios, talked about the evolution of radio
	production. Presnel said there are things you can do with a piece
	of software you might pay a few hundred dollars for that runs on
	an off-the-shelf computer that simply could only be imagined in a
	room full of analog boxes that might have cost hundreds of thousands
	of dollars 20 years ago. According to Clancy, with recent technology
	advancements, TM Studios has unlimited capacity to work from its
	Dallas headquarters with musicians in Nashville and New York or singers
	in LA and London. Writers, producers and talent are trading charts
	and creating music as if they are all actually working together in
	its renowned Studio A, when in fact there might be 5,000 miles of
	geographic separation.},
  file =     {Moss2015.pdf:Moss2015.pdf:PDF},
  keywords = {Communications--Radio, rank3},
  language = {English},
  url =      {https://search.proquest.com/docview/1684295613?accountid=17256}
}

@Article{Mueller2014,
  author =   {Mueller, Pam A. and Oppenheimer, Daniel M.},
  title =    {The Pen Is Mightier Than the Keyboard: Advantages of Longhand Over Laptop Note Taking},
  journal =  {Psychological Science},
  year =     {2014},
  volume =   {25},
  number =   {6},
  pages =    {1159-1168},
  abstract = {Taking notes on laptops rather than in longhand is increasingly common.
	Many researchers have suggested that laptop note taking is less effective
	than longhand note taking for learning. Prior studies have primarily
	focused on students' capacity for multitasking and distraction when
	using laptops. The present research suggests that even when laptops
	are used solely to take notes, they may still be impairing learning
	because their use results in shallower processing. In three studies,
	we found that students who took notes on laptops performed worse
	on conceptual questions than students who took notes longhand. We
	show that whereas taking more notes can be beneficial, laptop note
	takers' tendency to transcribe lectures verbatim rather than processing
	information and reframing it in their own words is detrimental to
	learning.},
  doi =      {10.1177/0956797614524581},
  eprint =   {http://pss.sagepub.com/content/25/6/1159.full.pdf+html},
  file =     {Mueller2014.pdf:Mueller2014.pdf:PDF},
  keywords = {rank1}
}

@Article{Muller2013,
  author =   {Muller, M. and Nanzhu Jiang and Grosche, P.},
  title =    {A Robust Fitness Measure for Capturing Repetitions in Music Recordings With Applications to Audio Thumbnailing},
  journal =  {Audio, Speech, and Language Processing, IEEE Transactions on},
  year =     {2013},
  volume =   {21},
  number =   {3},
  pages =    {531-543},
  month =    {March},
  abstract = {The automatic extraction of structural information from music recordings
	constitutes a central research topic. In this paper, we deal with
	a subproblem of audio structure analysis called audio thumbnailing
	with the goal to determine the audio segment that best represents
	a given music recording. Typically, such a segment has many (approximate)
	repetitions covering large parts of the recording. As the main technical
	contribution, we introduce a novel fitness measure that assigns a
	fitness value to each segment that expresses how much and how well
	the segment "explains" the repetitive structure of the entire recording.
	The thumbnail is then defined to be the fitness-maximizing segment.
	To compute the fitness measure, we describe an optimization scheme
	that jointly performs two error-prone steps, path extraction and
	grouping, which are usually performed successively. As a result,
	our approach is even able to cope with strong musical and acoustic
	variations that may occur within and across related segments. As
	a further contribution, we introduce the concept of fitness scape
	plots that reveal global structural properties of an entire recording.
	Finally, to show the robustness and practicability of our thumbnailing
	approach, we present various experiments based on different audio
	collections that comprise popular music, classical music, and folk
	song field recordings.},
  doi =      {10.1109/TASL.2012.2227732},
  file =     {Muller2013.pdf:Muller2013.pdf:PDF},
  issn =     {1558-7916},
  keywords = {audio recording;audio signal processing;music;optimisation;audio segment;audio structure analysis;audio thumbnailing;automatic extraction;classical music;fitness-maximizing segment;folk song field recordings;grouping method;music recording;optimization scheme;path extraction;repetitive structure;robust fitness measure;Abstracts;Educational institutions;Instruments;Music;Robustness;Speech;Speech processing;Structure analysis;alignment;audio;fitness;music;path;repetition;thumbnail}
}

@Misc{NIST2016,
  author =      {{National Institute of Standards and Technology (NIST)}},
  title =       {Rich Transcription Evaluation},
  year =        {2016},
  note =        {Accessed 27/01/18},
  owner =       {chrisbau},
  shortauthor = {NIST},
  timestamp =   {2018.01.27},
  url =         {https://www.nist.gov/itl/iad/mig/rich-transcription-evaluation}
}

@Article{Nielsen1997,
  author =   {Nielsen, J.},
  title =    {The use and misuse of focus groups},
  journal =  {Software, IEEE},
  year =     {1997},
  volume =   {14},
  number =   {1},
  pages =    {94-95},
  month =    {Jan},
  abstract = {Focus groups are a somewhat informal technique that can help you assess
	user needs and feeling both before interface design and long after
	implementation. In a focus group, you bring together six to nine
	users to discuss issues and concerns about the features of a user
	interface. The group typically lasts about two hours and is run by
	a moderator who maintains the group's focus. Focus groups often bring
	out users' spontaneous reactions and ideas and let you observe some
	group dynamics and organizational issues. The paper discusses the
	use and misuse of focus groups},
  doi =      {10.1109/52.566434},
  file =     {Nielsen1997.pdf:Nielsen1997.pdf:PDF},
  issn =     {0740-7459},
  keywords = {software development management;user centred design;user interfaces;focus groups;group dynamics;informal technique;organizational issues;system implementation;user interface design;user needs;Advertising;Interactive systems;Market research;Proposals;Rain;Snow;Usability;User interfaces;Web design;World Wide Web}
}

@InProceedings{Nielsen1993,
  author =    {Nielsen, Jakob and Landauer, Thomas K.},
  title =     {A Mathematical Model of the Finding of Usability Problems},
  booktitle = {Proc. INTERACT '93 and SIGCHI Conference on Human Factors in Computing Systems},
  year =      {1993},
  series =    {CHI '93},
  pages =     {206--213},
  address =   {Amsterdam, The Netherlands},
  publisher = {ACM},
  acmid =     {169166},
  doi =       {10.1145/169059.169166},
  file =      {Nielsen1993.pdf:Nielsen1993.pdf:PDF},
  isbn =      {0-89791-575-5},
  keywords =  {Poisson models, cost-benefit analysis, heuristic evaluation, iterative design, usability engineering, usability problems, user testing, rank4},
  numpages =  {8},
  review =    {Says that only 5 people are needed for usability studies.}
}

@Article{Noll1967,
  author =    {A. Michael Noll},
  title =     {Cepstrum Pitch Determination},
  journal =   {The Journal of the Acoustical Society of America},
  year =      {1967},
  volume =    {41},
  number =    {293},
  abstract =  {The cepstrum, defined as the power spectrum of the logarithm of the power spectrum, has a strong peak corresponding to the pitch period of the voiced-speech segment being analyzed. Cepstra were calculated on a digital computer and were automatically plotted on microfilm. Algorithms were developed heuristically for picking those peaks corresponding to voiced-speech segments and the vocal pitch periods. This information was then used to derive the excitation for a computer-simulated channel vocoder. The pitch quality of the vocoded speech was judged by experienced listeners in informal comparison tests to be indistinguishable from the original speech.},
  doi =       {10.1121/1.1910339},
  file =      {Noll1967.pdf:Noll1967.pdf:PDF},
  owner =     {chrisbau},
  review =    {Original cepstrum paper},
  timestamp = {2018.01.13}
}

@Article{Noyes2008,
  author =   { Jan M. Noyes and Kate J. Garland },
  title =    {Computer- vs. paper-based tasks: Are they equivalent?},
  journal =  {Ergonomics},
  year =     {2008},
  volume =   {51},
  number =   {9},
  pages =    {1352-1375},
  note =     {PMID: 18802819},
  abstract = { In 1992, Dillon published his critical review of the empirical literature
	on reading from paper vs. screen. However, the debate concerning
	the equivalence of computer- and paper-based tasks continues, especially
	with the growing interest in online assessment. The current paper
	reviews the literature over the last 15 years and contrasts the results
	of these more recent studies with Dillon's findings. It is concluded
	that total equivalence is not possible to achieve, although developments
	in computer technology, more sophisticated comparative measures and
	more positive user attitudes have resulted in a continuing move towards
	achieving this goal. Many paper-based tasks used for assessment or
	evaluation have been transferred directly onto computers with little
	regard for any implications. This paper considers equivalence issues
	between the media by reviewing performance measures. While equivalence
	seems impossible, the importance of any differences appears specific
	to the task and required outcomes. },
  doi =      {10.1080/00140130802170387},
  eprint =   { http://dx.doi.org/10.1080/00140130802170387 },
  file =     {Noyes2008.pdf:Noyes2008.pdf:PDF},
  keywords = {rank5}
}

@InProceedings{OHara1997,
  author =    {O'Hara, Kenton and Sellen, Abigail},
  title =     {A Comparison of Reading Paper and On-line Documents},
  booktitle = {Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems},
  year =      {1997},
  series =    {CHI '97},
  pages =     {335--342},
  address =   {Atlanta, Georgia, USA},
  publisher = {ACM},
  abstract =  {We report on a laboratory study that compares reading from paper to
	reading on-line. Critical differences have to do with the major advantages
	paper offers in supporting annotation while reading, quick navigation,
	and flexibility of spatial layout. These, in turn, allow readers
	to deepen their understanding of the text, extract a sense of its
	structure, create a plan for writing, cross-refer to other documents,
	and interleave reading and writing. We discuss the design implications
	of these findings for the development of better reading technologies.},
  acmid =     {258787},
  doi =       {10.1145/258549.258787},
  file =      {OHara1997.pdf:OHara1997.pdf:PDF},
  isbn =      {0-89791-802-9},
  keywords =  {Web, design, digital documents, digital libraries, hypertext, paper, reading, rank5},
  numpages =  {8},
  review =    {Working on paper rather than on screen allows users to deepen their
	understanding of the text, extract a sense of its structure, create
	a plan for writing, cross-refer to other documents, and interleave
	reading and writing.}
}

@InProceedings{Obin2014,
  author =    {Nicolas Obin and Axel Roebel and Gregoire Bachman},
  title =     {On Automatic Voice Casting for Expressive Speech: Speaker Recognition vs. Speech Classification},
  booktitle = {Proc. IEEE International Conference on Acoustic, Speech and Signal Processing (ICASSP)},
  year =      {2014},
  abstract =  {This paper presents the first large-scale automatic voice casting
	system, and explores the adaptation of speaker recognition techniques
	to measure voice similarities. The proposed system is based on the
	representation of a voice by classes (e.g., age/gender, voice quality,
	emotion). First, a multi-label system is used to classify speech
	into classes. Then, the output probabilities for each class are concatenated
	to form a vector that represents the vocal signature of a speech
	recording. Finally, a similarity search is performed on the vocal
	signatures to determine the set of target actors that are the most
	similar to a speech recording of a source actor. In a subjective
	experiment conducted in the real-context of voice casting for video
	games, the multi-label system clearly outperforms standard speaker
	recognition systems. This indicates evidence that speech classes
	successfully capture the principal directions that are used in the
	perception of voice similarity.},
  file =      {Obin2014.pdf:Obin2014.pdf:PDF},
  keywords =  {smd},
  owner =     {Chris},
  timestamp = {2014.05.28}
}

@Misc{Ofcom2017,
  author =      {{Office of Communications}},
  title =       {The Communications Market: UK},
  month =       aug,
  year =        {2017},
  file =        {Ofcom2017.pdf:Ofcom2017.pdf:PDF},
  owner =       {chrisbau},
  shortauthor = {Ofcom},
  timestamp =   {2017.12.15},
  url =         {https://www.ofcom.org.uk/research-and-data/multi-sector-research/cmr/cmr-2017/uk}
}

@Article{Olberding2010,
  author =   {Olberding, Simon and Steimle, J{\"u}rgen},
  title =    {Towards Understanding Erasing-based Interactions: Adding Erasing Capabilities to Anoto Pens},
  journal =  {Proceedings of Paper Computing},
  year =     {2010},
  file =     {Olberding2010.pdf:Olberding2010.pdf:PDF},
  keywords = {rank4}
}

@Article{Oviatt2000,
  author =        {Oviatt, Sharon and Cohen, Phil and Wu, Lizhong and Vergo, John and Duncan, Lisbeth and Suhm, Bernhard and Bers, Josh and Holzman, Thomas and Winograd, Terry and Landay, James and Larson, Jim and Ferro, David},
  title =         {Designing the User Interface for Multimodal Speech and Pen-based Gesture Applications: State-of-the-art Systems and Future Research Directions},
  journal =       {Hum.-Comput. Interact.},
  year =          {2000},
  volume =        {15},
  number =        {4},
  pages =         {263--322},
  month =         dec,
  __markedentry = {[chrisbau:]},
  acmid =         {1463023},
  address =       {Hillsdale, NJ, USA},
  doi =           {10.1207/S15327051HCI1504_1},
  file =          {Oviatt2000.pdf:Oviatt2000.pdf:PDF},
  issn =          {0737-0024},
  issue_date =    {December 2000},
  keywords =      {rank5},
  numpages =      {60},
  publisher =     {L. Erlbaum Associates Inc.}
}

@Article{Panagiotakis2005,
  author =   {Panagiotakis, C. and Tziritas, G.},
  title =    {A speech/music discriminator based on {RMS} and zero-crossings},
  journal =  {IEEE Transactions on Multimedia},
  year =     {2005},
  volume =   {7},
  number =   {1},
  pages =    {155-166},
  abstract = {Over the last several years, major efforts have been made to develop
	methods for extracting information from audiovisual media, in order
	that they may be stored and retrieved in databases automatically,
	based on their content. In this work we deal with the characterization
	of an audio signal, which may be part of a larger audiovisual system
	or may be autonomous, as for example in the case of an audio recording
	stored digitally on disk. Our goal was to first develop a system
	for segmentation of the audio signal, and then classification into
	one of two main categories: speech or music. Among the system's requirements
	are its processing speed and its ability to function in a real-time
	environment with a small responding delay. Because of the restriction
	to two classes, the characteristics that are extracted are considerably
	reduced and moreover the required computations are straightforward.
	Experimental results show that efficiency is exceptionally good,
	without sacrificing performance. Segmentation is based on mean signal
	amplitude distribution, whereas classification utilizes an additional
	characteristic related to the frequency. The classification algorithm
	may be used either in conjunction with the segmentation algorithm,
	in which case it verifies or refutes a music-speech or speech-music
	change, or autonomously, with given audio segments. The basic characteristics
	are computed in 20 ms intervals, resulting in the segments' limits
	being specified within an accuracy of 20 ms. The smallest segment
	length is one second. The segmentation and classification algorithms
	were benchmarked on a large data set, with correct segmentation about
	97% of the time and correct classification about 95%.},
  doi =      {10.1109/TMM.2004.840604},
  file =     {Panagiotakis2005.pdf:Panagiotakis2005.pdf:PDF},
  issn =     {1520-9210},
  keywords = {audio databases, audio recording, audio signal processing, audio-visual systems, feature extraction, information retrieval, music, signal classification, speech processing, statistical analysis, audio recording, audio signal segmentation, audiovisual media, classification algorithm, information extraction, mean signal amplitude distribution, music classification, speech classification, speech/music discriminator, zero-crossing rate, Audio databases, Audio recording, Audio-visual systems, Classification algorithms, Content based retrieval, Data mining, Multiple signal classification, Music information retrieval, Real time systems, Speech, SMD, rank2},
  review =   {Good overview of SMD using only RMS and ZCR. Claims that RMS and ZCR
	have little correlation. Easy to read.},
  status =   {printed, read}
}

@InProceedings{Park2009,
  author =    {Tae Hong Park and Zhiye Li and Wen Wu},
  title =     {Easy Does It: The Electro-Acoustic Music Analysis Toolbox},
  booktitle = {Proc. International Society for Music Information Retrieval Conference},
  year =      {2009},
  file =      {Park2009.pdf:Park2009.pdf:PDF},
  owner =     {Chris},
  timestamp = {2014.02.04}
}

@InProceedings{Paterno2004,
  author =    {Fabio Patern\`{o} and Enrico Zini},
  title =     {Applying Information Visualization Techniques to Visual Representations of Task Models},
  booktitle = {Proc. 3rd annual conference on Task models and diagrams (TAMODIA)},
  year =      {2004},
  abstract =  {This paper shows how information visualization techniques can be used
	to improve the effectiveness of task model representations. In particular,
	we discuss how fisheye and semantic-zoom representations have been
	used to improve the effectiveness of the ConcurTaskTrees notation.
	The approach can also be useful for improving other visual modelling
	languages. We also report on a first evaluation of the proposed representations.},
  file =      {Paterno2004.pdf:Paterno2004.pdf:PDF},
  keywords =  {task},
  owner =     {Chris},
  timestamp = {2014.09.01}
}

@Book{Patton1990,
  title =     {Qualitative Evaluation and Research Methods},
  publisher = {Sage},
  year =      {1990},
  author =    {Michael Quinn Patton},
  isbn =      {0803937792},
  owner =     {chrisbau},
  timestamp = {2017.11.14}
}

@InProceedings{Pavel2016,
  author =    {Pavel, Amy and Goldman, Dan B. and Hartmann, Bj\"{o}rn and Agrawala, Maneesh},
  title =     {VidCrit: Video-based Asynchronous Video Review},
  booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
  year =      {2016},
  series =    {UIST '16},
  pages =     {517--528},
  address =   {Tokyo, Japan},
  publisher = {ACM},
  acmid =     {2984552},
  doi =       {10.1145/2984511.2984552},
  file =      {Pavel2016.pdf:Pavel2016.pdf:PDF},
  isbn =      {978-1-4503-4189-9},
  keywords =  {feedback, video editing, video review, rank2},
  numpages =  {12}
}

@InProceedings{Pavel2014,
  author =    {Pavel, Amy and Reed, Colorado and Hartmann, Bj\"{o}rn and Agrawala, Maneesh},
  title =     {Video Digests: A Browsable, Skimmable Format for Informational Lecture Videos},
  booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
  year =      {2014},
  series =    {UIST '14},
  pages =     {573--582},
  address =   {Honolulu, Hawaii, USA},
  publisher = {ACM},
  acmid =     {2647400},
  doi =       {10.1145/2642918.2647400},
  file =      {Pavel2014.pdf:Pavel2014.pdf:PDF},
  isbn =      {978-1-4503-3069-5},
  keywords =  {education, video digests, video presentation interfaces, rank4},
  numpages =  {10}
}

@Misc{Perraudin2014,
  author =       {Frances Perraudin},
  title =        {Cassetteboy: ``David Cameron won't be pleased by our video''},
  howpublished = {The Guardian},
  month =        oct,
  year =         {2014},
  comment =      {Accessed 05/02/2018},
  owner =        {chrisbau},
  timestamp =    {2018.01.05},
  url =          {https://www.theguardian.com/media/2014/oct/10/cassetteboy-david-cameron-mashup-copyright}
}

@InProceedings{Perry2009,
  author =    {Perry, Mark and Juhlin, Oskar and Esbj\"{o}rnsson, Mattias and Engstr\"{o}m, Arvid},
  title =     {Lean Collaboration Through Video Gestures: Co-ordinating the Production of Live Televised Sport},
  booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  year =      {2009},
  series =    {CHI '09},
  pages =     {2279--2288},
  address =   {Boston, MA, USA},
  publisher = {ACM},
  abstract =  {This paper examines the work and interactions between camera operators
	and a vision mixer during an ice hockey match, and presents an interaction
	analysis using video data. We analyze video-mediated indexical gestures
	in the collaborative production of live sport on television between
	distributed team members. The findings demonstrate how video forms
	the topic, resource and product of collabora-tion: whilst it shapes
	the nature of the work (editing), it is simultaneously also the primary
	resource for supporting mutual orientation and negotiating shot transitions
	between remote participants (co-ordination), as well as its end prod-uct
	(broadcast). Our analysis of current professional activi-ties is
	used to develop implications for the design of future services for
	live collaborative video production.},
  acmid =     {1519051},
  doi =       {10.1145/1518701.1519051},
  file =      {Perry2009.pdf:Perry2009.pdf:PDF},
  isbn =      {978-1-60558-246-7},
  keywords =  {task, live tv collaboration communication indexical gestures mobile technology video production sport, rank3},
  numpages =  {10}
}

@InProceedings{Peus2011,
  author =    {Peus, Stephan},
  title =     {The ``Digital Solution'': The Answer to a Lot of Challenges within New Production Routines at Today's Broadcasting Stations},
  booktitle = {Proc. 130th Audio Engineering Society Convention},
  year =      {2011},
  month =     may,
  file =      {Peus2011.pdf:Peus2011.pdf:PDF},
  keywords =  {rank5},
  owner =     {chrisbau},
  timestamp = {2017.11.03},
  url =       {http://www.aes.org/e-lib/browse.cfm?elib=15812}
}

@InProceedings{Pikrakis2006,
  author =    {Aggelos Pikrakis and Theodoros Giannakopoulos and Sergios Theodoridis},
  title =     {A computationally efficient speech/music discriminator for radio recordings},
  booktitle = {Proc. of of the 7th International Conference on Music Information Retrieval},
  year =      {2006},
  pages =     {107--110},
  file =      {Pikrakis2006.pdf:Pikrakis2006.pdf:PDF},
  keywords =  {SMD, rank2},
  status =    {printed, read}
}

@InProceedings{Pikrakis2006a,
  author =    {A. Pikrakis and T. Giannakopoulos and S. Theodoridis},
  title =     {Speech/Music Discrimination for radio broadcasts using a hybrid HMM-Bayesian Network architecture},
  booktitle = {Proceedings of the 14th European Signal Processing Conference},
  year =      {2006},
  address =   {Florence, Italy},
  month =     sep,
  file =      {Pikrakis2006a.pdf:Pikrakis2006a.pdf:PDF},
  keywords =  {SMD, rank2},
  owner =     {Chris},
  timestamp = {2013.11.12}
}

@Article{Pikrakis2008,
  author =   {Pikrakis, A. and Giannakopoulos, T. and Theodoridis, S.},
  title =    {A Speech/Music Discriminator of Radio Recordings Based on Dynamic Programming and Bayesian Networks},
  journal =  {Multimedia, IEEE Transactions on},
  year =     {2008},
  volume =   {10},
  number =   {5},
  pages =    {846-857},
  abstract = {This paper presents a multistage system for speech/music discrimination
	which is based on a three-step procedure. The first step is a computationally
	efficient scheme consisting of a region growing technique and operates
	on a 1-D feature sequence, which is extracted from the raw audio
	stream. This scheme is used as a preprocessing stage and yields segments
	with high music and speech precision at the expense of leaving certain
	parts of the audio recording unclassified. The unclassified parts
	of the audio stream are then fed as input to a more computationally
	demanding scheme. The latter treats speech/music discrimination of
	radio recordings as a probabilistic segmentation task, where the
	solution is obtained by means of dynamic programming. The proposed
	scheme seeks the sequence of segments and respective class labels
	(i.e., speech/music) that maximize the product of posterior class
	probabilities, given the data that form the segments. To this end,
	a Bayesian Network combiner is embedded as a posterior probability
	estimator. At a final stage, an algorithm that performs boundary
	correction is applied to remove possible errors at the boundaries
	of the segments (speech or music) that have been previously generated.
	The proposed system has been tested on radio recordings from various
	sources. The overall system accuracy is approximately 96%. Performance
	results are also reported on a musical genre basis and a comparison
	with existing methods is given.},
  doi =      {10.1109/TMM.2008.922870},
  file =     {Pikrakis2008.pdf:Pikrakis2008.pdf:PDF},
  issn =     {1520-9210},
  keywords = {Bayes methods, audio signal processing, dynamic programming, radio networks, 1D feature sequence, Bayesian networks, audio recording, dynamic programming, radio recordings, raw audio stream, speech/music discriminator, Bayesian networks, dynamic programming, speech-music discrimination, SMD, rank2},
  review =   {Complicated three-stage SMD system. First stage uses 'chromatic entropy'
	based on spectral entropy (see Misra2004) and works for about half
	the audio. Second stage fills in the gaps using five feats and a
	bayesian network. The features are short-term energy, mean standard
	deviation of chroma, minimum standard deviation of chroma, and the
	first two MFCCs. Third stage if fine-grained boundary correction.
	Claims computational efficiency but uses the frequency domain.},
  status =   {printed, read}
}

@InProceedings{Pizzi1989,
  author =    {Pizzi, Skip},
  title =     {Digital Audio Applications in Radio Broadcasting},
  booktitle = {Proc. Audio Engineering Society 7th International Conference on Audio in Digital Times},
  year =      {1989},
  month =     may,
  file =      {Pizzi1989.pdf:Pizzi1989.pdf:PDF},
  owner =     {chrisbau},
  timestamp = {2017.11.03},
  url =       {http://www.aes.org/e-lib/browse.cfm?elib=5446}
}

@InProceedings{Popescu-Belis2006,
  author =    {Popescu-Belis,Andréi and Georgescul,Maria},
  title =     {TQB: Accessing Multimodal Data Using a Transcript-based Query and Browsing Interface},
  booktitle = {Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC)},
  year =      {2006},
  pages =     {1560-1565},
  note =      {ID: unige:3443},
  file =      {Popescu-Belis2006.pdf:Popescu-Belis2006.pdf:PDF},
  keywords =  {rank2},
  language =  {eng},
  review =    {Meeting browser interface with - segmentation of individual channels
	into utterances - labelling of utterances with dialogue act tags
	(e.g. statement, question, command, or politeness mark) - segmentation
	of the meeting into thematic episodes - labelling of episodes with
	salient keywords - document-speech alignment using explicit references
	to documents},
  url =       {http://archive-ouverte.unige.ch/unige:3443}
}

@Article{Pras2013,
  author =   {Pras, Amandine and Guastavino, Catherine and Lavoie, Maryse},
  title =    {The impact of technological advances on recording studio practices},
  journal =  {Journal of the American Society for Information Science and Technology},
  year =     {2013},
  volume =   {64},
  number =   {3},
  pages =    {612--626},
  abstract = {Since the invention of sound reproduction in the late 19th century,
	studio practices in musical recording evolved in parallel with technological
	improvements. Recently, digital technology and Internet file sharing
	led to the delocalization of professional recording studios and the
	decline of traditional record companies. A direct consequence of
	this new paradigm is that studio professions found themselves in
	a transitional phase, needing to be reinvented. To understand the
	scope of these recent technological advances, we first offer an overview
	of musical recording culture and history and show how studio recordings
	became a sophisticated form of musical artwork that differed from
	concert representations. We then trace the economic evolution of
	the recording industry through technological advances and present
	positive and negative impacts of the decline of the traditional business
	model on studio practices and professions. Finally, we report findings
	from interviews with six world-renowned record producers reflecting
	on their recording approaches, the impact of recent technological
	advances on their careers, and the future of their profession. Interviewees
	appreciate working on a wider variety of projects than they have
	in the past, but they all discuss trade-offs between artistic expectations
	and budget constraints in the current paradigm. Our investigations
	converge to show that studio professionals have adjusted their working
	settings to the new economic situation, although they still rely
	on the same aesthetic approaches as in the traditional business model
	to produce musical recordings.},
  doi =      {10.1002/asi.22840},
  file =     {Pras2013.pdf:Pras2013.pdf:PDF},
  issn =     {1532-2890},
  keywords = {music, recording, skills, rank2}
}

@Misc{ProducerSpot2015,
  author =    {{Producer Spot}},
  title =     {Top Ten Best DAW},
  year =      {2015},
  comment =   {Accessed 23/2/2017},
  keywords =  {rank1},
  owner =     {chrisbau},
  review =    {"We created a poll that was shared on our social networks and sent
	to our subscribers. More than 10,000 people voted"},
  timestamp = {2017.02.23},
  url =       {http://www.producerspot.com/top-best-daw-2015-best-music-software}
}

@Article{Rabiner1989,
  author =   {Rabiner, L.},
  title =    {A tutorial on hidden Markov models and selected applications in speech recognition},
  journal =  {Proceedings of the IEEE},
  year =     {1989},
  volume =   {77},
  number =   {2},
  pages =    {257-286},
  abstract = {This tutorial provides an overview of the basic theory of hidden Markov
	models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and
	gives practical details on methods of implementation of the theory
	along with a description of selected applications of the theory to
	distinct problems in speech recognition. Results from a number of
	original sources are combined to provide a single source of acquiring
	the background required to pursue further this area of research.
	The author first reviews the theory of discrete Markov chains and
	shows how the concept of hidden states, where the observation is
	a probabilistic function of the state, can be used effectively. The
	theory is illustrated with two simple examples, namely coin-tossing,
	and the classic balls-in-urns system. Three fundamental problems
	of HMMs are noted and several practical techniques for solving these
	problems are given. The various types of HMMs that have been studied,
	including ergodic as well as left-right models, are described},
  doi =      {10.1109/5.18626},
  file =     {Rabiner1989.pdf:Rabiner1989.pdf:PDF},
  issn =     {0018-9219},
  keywords = {Markov processes, speech recognition, balls-in-urns system, coin-tossing, discrete Markov chains, ergodic models, hidden Markov models, hidden states, left-right models, probabilistic function, speech recognition, Distortion, Hidden Markov models, Mathematical model, Multiple signal classification, Signal processing, Speech recognition, Statistical analysis, Stochastic processes, Temperature measurement, Tutorial, rank1},
  review =   {Concise but not always easy to read overview of HMMs.},
  status =   {printed, read}
}

@Article{Raimond2014,
  author =   {Yves Raimond and Tristan Ferne and Michael Smethurst and Gareth Adams},
  title =    {The {BBC} World Service Archive prototype},
  journal =  {Web Semantics: Science, Services and Agents on the World Wide Web},
  year =     {2014},
  volume =   {27-28},
  number =   {0},
  pages =    {2 - 9},
  note =     {Semantic Web Challenge 2013 },
  abstract = {Abstract Most broadcasters have accumulated large audio and video
	archives stretching back over many decades. For example the \{BBC\}
	World Service radio archive includes around 70,000 English-language
	programmes from over 45 years. This amounts to about three years
	of continuous audio and around 15TB of data. The metadata around
	this archive is sparse and sometimes wrong, but the full audio content
	is available in digital form. We have built a system to process the
	existing audio and text and automatically annotate programmes within
	the archive with Linked Data web identifiers. The resulting interlinks
	are used to bootstrap search and navigation within this archive and
	expose it to users. Automated data will never be entirely accurate
	so we built crowdsourcing mechanisms for users to correct and add
	data. The resulting crowdsourced data is then used to improve search
	and navigation within the archive, as well as evaluate and improve
	our algorithms. As a result of this feedback cycle, the interlinks
	between our archive and the Semantic Web are continuously improving.
	This unique combination of Semantic Web technologies, automation
	and crowdsourcing has dramatically reduced the amount of time and
	effort required to publish this rich archive online. The \{BBC\}
	World Service archive prototype is available online at http://worldservice.prototyping.bbc.co.uk,
	last accessed March 2014.},
  doi =      {10.1016/j.websem.2014.07.005},
  file =     {Raimond2014.pdf:Raimond2014.pdf:PDF},
  issn =     {1570-8268},
  keywords = {Crowdsourcing, rank4}
}

@Misc{RAJAR2017a,
  author =    {{RAJAR}},
  title =     {Quarterly summary of radio listening, Q3 2017},
  month =     sep,
  year =      {2017},
  owner =     {chrisbau},
  timestamp = {2017.11.25},
  url =       {http://www.rajar.co.uk/docs/2017_09/2017_Q3_Quarterly_Summary_Figures.pdf}
}

@Misc{RAJAR2017,
  author =    {{RAJAR} and {IpsosMori}},
  title =     {{MIDAS} audio survey, summer 2017},
  year =      {2017},
  owner =     {chrisbau},
  timestamp = {2017.11.25},
  url =       {http://www.rajar.co.uk/docs/news/MIDAS_Summer_v2.pdf}
}

@Article{Ramachandran2001,
  author =    {V.S. Ramachandran and E.M. Hubbard},
  title =     {Synaesthesia - A Window Into Perception, Thought and Language},
  journal =   {Journal of Consciousness Studies},
  year =      {2001},
  volume =    {8},
  number =    {12},
  pages =     {3--34},
  file =      {Ramachandran2001.pdf:Ramachandran2001.pdf:PDF},
  keywords =  {rank4},
  owner =     {Chris},
  review =    {Good overview of synaesthesia in general. Includes bit on Bouba/Kiki.},
  status =    {printed},
  timestamp = {2014.01.08}
}

@InProceedings{Ramos2003,
  author =    {Ramos, Gonzalo and Balakrishnan, Ravin},
  title =     {Fluid Interaction Techniques for the Control and Annotation of Digital Video},
  booktitle = {Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology},
  year =      {2003},
  series =    {UIST '03},
  pages =     {105--114},
  address =   {Vancouver, British Columbia, Canada},
  publisher = {ACM},
  acmid =     {964708},
  doi =       {10.1145/964696.964708},
  file =      {Ramos2003.pdf:Ramos2003.pdf:PDF},
  isbn =      {1-58113-636-6},
  keywords =  {annotations, fluid interaction techniques, pen-based interfaces, video},
  numpages =  {10}
}

@InProceedings{Ramos2005,
  author =    {Ramos, Gonzalo and Balakrishnan, Ravin},
  title =     {Zliding: Fluid Zooming and Sliding for High Precision Parameter Manipulation},
  booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
  year =      {2005},
  series =    {UIST '05},
  pages =     {143--152},
  address =   {Seattle, Washington, USA},
  publisher = {ACM},
  acmid =     {1095059},
  doi =       {10.1145/1095034.1095059},
  file =      {Ramos2005.pdf:Ramos2005.pdf:PDF},
  isbn =      {1-59593-271-2},
  keywords =  {input, multi-scale navigation, pen-based interfaces, pressure widgets},
  numpages =  {10}
}

@InProceedings{Ranjan2006,
  author =    {Ranjan, Abhishek and Balakrishnan, Ravin and Chignell, Mark},
  title =     {Searching in Audio: The Utility of Transcripts, Dichotic Presentation, and Time-compression},
  booktitle = {Proc. SIGCHI Conference on Human Factors in Computing Systems},
  year =      {2006},
  series =    {CHI '06},
  pages =     {721--730},
  address =   {Montr\'{e}al, Qu\'{e}bec, Canada},
  publisher = {ACM},
  abstract =  {Searching audio data can potentially be facilitated by the use of
	automatic speech recognition (ASR) technology to generate text transcripts
	which can then be easily queried. However, since current ASR technology
	cannot reliably generate 100% accurate transcripts, additional techniques
	for fluid browsing and searching of the audio itself are required.
	We explore the impact of transcripts of various qualities, dichotic
	presentation, and time-compression on an audio search task. Results
	show that dichotic presentation and reasonably accurate transcripts
	can assist in the search process, but suggest that time-compression
	and low accuracy transcripts should be used carefully.},
  acmid =     {1124879},
  doi =       {10.1145/1124772.1124879},
  file =      {Ranjan2006.pdf:Ranjan2006.pdf:PDF},
  isbn =      {1-59593-372-7},
  keywords =  {audio time-compression, dichotic listening, transcripts, skimming, rank5},
  numpages =  {10}
}

@Misc{Rice2001,
  author =    {Rice, S.V. and Patten, M.D.},
  title =     {Waveform display utilizing frequency-based coloring and navigation},
  month =     feb,
  year =      {2001},
  note =      {US Patent 6,184,898},
  keywords =  {rank4},
  owner =     {chrisbau},
  publisher = {Google Patents},
  timestamp = {2017.12.04},
  url =       {https://www.google.com/patents/US6184898}
}

@Patent{Rice2001a,
  year =      {2001},
  yearfiled = {1998},
  author =    {Rice, S.V. and Patten, M.D.},
  title =     {Waveform display utilizing frequency-based coloring and navigation},
  month =     feb,
  note =      {US Patent 6,184,898},
  file =      {Rice2001a.pdf:Rice2001a.pdf:PDF},
  owner =     {chrisbau},
  publisher = {Google Patents},
  timestamp = {2018.01.25}
}

@InProceedings{Rice2005,
  author =    {Stephen V. Rice},
  title =     {Frequency-Based Coloring of the Waveform Display to Facilitate Audio Editing and Retrieval},
  booktitle = {Proc. 119th Audio Engineering Society Convention},
  year =      {2005},
  number =    {119},
  address =   {New York, NY, USA},
  month =     oct,
  abstract =  {The audio waveform display provides the visual focus in audio-editing
	systems yet sounds are difficult to see in the display. Using a new
	technique, the display is colored torepresent the frequency content
	to make sounds more visible. This requires extraction of frequency
	information from the audio signal and an appropriate mapping of this
	information to the color space. Ideally, the coloring is independent
	of recording level, and similar sounds are represented by similar
	colors. Audio-editing systems are enhanced by the improved userinterface.
	Audio-retrieval systems can present colored waveform displays as
	visual 'thumbnails' in a list of sound search results.},
  file =      {Rice2005.pdf:Rice2005.pdf:PDF},
  keywords =  {visualization, rank4},
  owner =     {Chris},
  review =    {Coloured waveforms based on spectrum},
  status =    {read},
  timestamp = {2013.09.22}
}

@InProceedings{Robert-Ribes1997,
  author =       {Robert-Ribes, Jordi and Mukhtar, Rami G and others},
  title =        {Automatic generation of hyperlinks between audio and transcript.},
  booktitle =    {Proc. EUROSPEECH},
  year =         {1997},
  organization = {Citeseer},
  file =         {Robert-Ribes1997.pdf:Robert-Ribes1997.pdf:PDF},
  keywords =     {rank1},
  review =       {Speech alignment}
}

@Book{Rogers2011,
  title =     {Interaction Design: Beyond Human Computer Interaction (3rd edition)},
  publisher = {Wiley},
  year =      {2011},
  author =    {Yvonne Rogers and Helen Sharp and Jenny Preece},
  keywords =  {rank1},
  owner =     {Chris},
  status =    {read},
  timestamp = {2013.09.28}
}

@Article{Rouanet1970,
  author =    {Rouanet, H. and L\'{e}pine, D.},
  title =     {Comparison between treatments in a repeated-measurement design: {ANOVA} and multivariate methods},
  journal =   {British Journal of Mathematical and Statistical Psychology},
  year =      {1970},
  volume =    {23},
  number =    {2},
  pages =     {147--163},
  doi =       {10.1111/j.2044-8317.1970.tb00440.x},
  issn =      {2044-8317},
  publisher = {Blackwell Publishing Ltd}
}

@TechReport{Rouncefield1997,
  author =      {Mark Rouncefield and John A Hughes and Jon O'Brien},
  title =       {Ethnography - Some Practicalities of Ethnographic Analysis},
  institution = {Cooperative Systems Engineering Group, Lancaster University},
  year =        {1997},
  number =      {CSEG/27/1997},
  abstract =    {The aims of this section are: 1. To outline some practical advice
	in describing ethnography to clients. 2. To outline what ethnography
	can and cannot do. 3. To present some practical advice on `doing'
	ethnography - to provide some `things to look out for'. 4. To identify
	some practical matters in conducting an ethnography: 5. To give some
	indication of what kind of materials should be collected. 6. To present
	some general principles for writing the report},
  file =        {Rouncefield1997.pdf:Rouncefield1997.pdf:PDF},
  keywords =    {task, rank3},
  owner =       {chrisbau},
  review =      {Honest, useful and practical guide to conducting field studies.},
  timestamp =   {2014.11.10}
}

@PhdThesis{Rubin2015,
  author =    {Steven Rubin},
  title =     {Tools for Creating Audio Stories},
  school =    {Electrical Engineering and Computer Sciences, University of California at Berkeley},
  year =      {2015},
  keywords =  {rank5},
  owner =     {chrisbau},
  timestamp = {2016.03.10},
  url =       {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2015/EECS-2015-237.html}
}

@InProceedings{Rubin2013,
  author =    {Rubin, Steve and Berthouzoz, Floraine and Mysore, Gautham J. and Li, Wilmot and Agrawala, Maneesh},
  title =     {Content-based Tools for Editing Audio Stories},
  booktitle = {Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology},
  year =      {2013},
  series =    {UIST '13},
  pages =     {113--122},
  address =   {St. Andrews, Scotland, United Kingdom},
  publisher = {ACM},
  acmid =     {2501993},
  doi =       {10.1145/2501988.2501993},
  file =      {Rubin2013.pdf:Rubin2013.pdf:PDF},
  isbn =      {978-1-4503-2268-3},
  keywords =  {audio editing, music browsing, music retargeting, storytelling, transcript-based editing, rank5},
  numpages =  {10}
}

@Misc{Sailer2013,
  author =       {Martin Oliver Sailer},
  title =        {{crossdes}: Construction of Crossover Designs},
  howpublished = {R project package},
  year =         {2013},
  owner =        {chrisbau},
  timestamp =    {2018.01.23},
  url =          {https://cran.r-project.org/package=crossdes}
}

@MastersThesis{Sampaio2016,
  author =    {Sampaio, Vitor Hugo Mendes},
  title =     {Produ\c{c}\~{a}o \'{a}udio: impacto do som em produ\c{c}\~{o}es audiovisuais},
  school =    {Universidade de Tr\'{a}s-os-Montes e Alto Douro},
  year =      {2016},
  file =      {Sampaio2016.pdf:Sampaio2016.pdf:PDF},
  keywords =  {rank2},
  owner =     {chrisbau},
  timestamp = {2017.02.23}
}

@InProceedings{Saunders1996,
  author =    {Saunders, J.},
  title =     {Real-time discrimination of broadcast speech/music},
  booktitle = {Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
  year =      {1996},
  volume =    {2},
  pages =     {993-996 vol. 2},
  abstract =  {We describe a technique which is successful at discriminating speech
	from music on broadcast FM radio. The computational simplicity of
	the approach could lend itself to wide application including the
	ability to automatically change channels when commercials appear.
	The algorithm provides the capability to robustly distinguish the
	two classes and runs easily in real time. Experimental results to
	date show performance approaching 98% correct classification},
  doi =       {10.1109/ICASSP.1996.543290},
  file =      {Saunders1996.pdf:Saunders1996.pdf:PDF},
  issn =      {1520-6149},
  keywords =  {acoustic signal processing, frequency modulation, music, radio broadcasting, speech processing, broadcast FM radio, broadcast speech/music, commercials, computational simplicity, correct classification, experimental results, performance, real-time discrimination, Automatic control, Bandwidth, Cost function, Frequency, Instruments, Monitoring, Multiple signal classification, Radio broadcasting, Speech, Surveillance, SMD, rank5},
  review =    {Early experiments in broadcast SMD using ZCR distribution on 2.4s
	blocks. 90% accurate with ZCR alone, increased to 98% with low-energy
	ratio style feature. Not much information given about data and evaluation.},
  status =    {printed, read}
}

@Book{Sauro2016,
  title =     {Quantifying the user experience: Practical statistics for user research},
  publisher = {Morgan Kaufmann},
  year =      {2016},
  author =    {Sauro, Jeff and Lewis, James R},
  isbn =      {0128023082}
}

@InProceedings{Scheirer1997,
  author =    {Eric Scheirer and Malcolm Slaney},
  title =     {Construction and evaluation of a robust multifeature speech/music discriminator},
  booktitle = {Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year =      {1997},
  file =      {Scheirer1997.pdf:Scheirer1997.pdf:PDF},
  keywords =  {rank1},
  owner =     {Chris},
  timestamp = {2014.07.15}
}

@InProceedings{Schilit1998,
  author =    {Schilit, Bill N. and Golovchinsky, Gene and Price, Morgan N.},
  title =     {Beyond Paper: Supporting Active Reading with Free Form Digital Ink Annotations},
  booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  year =      {1998},
  series =    {CHI '98},
  pages =     {249--256},
  address =   {Los Angeles, California, USA},
  publisher = {ACM Press/Addison-Wesley Publishing Co.},
  acmid =     {274680},
  doi =       {10.1145/274644.274680},
  file =      {Schilit1998.pdf:Schilit1998.pdf:PDF},
  isbn =      {0-201-30987-4},
  keywords =  {affordances of paper, document metaphor, dynamic hypertext, information retrieval, paper-like user interface, pen computing, reading online, rank1},
  numpages =  {8}
}

@InProceedings{Schmandt1995,
  author =    {Schmandt, Chris and Mullins, Atty},
  title =     {AudioStreamer: Exploiting Simultaneity for Listening},
  booktitle = {Conference Companion on Human Factors in Computing Systems},
  year =      {1995},
  series =    {CHI '95},
  pages =     {218--219},
  publisher = {ACM},
  acmid =     {223533},
  doi =       {10.1145/223355.223533},
  file =      {Schmandt1995.pdf:Schmandt1995.pdf:PDF},
  isbn =      {0-89791-755-3},
  keywords =  {rank2},
  numpages =  {2}
}

@Article{Schoeffmann2015,
  author =     {Schoeffmann, Klaus and Hudelist, Marco A. and Huber, Jochen},
  title =      {Video Interaction Tools: A Survey of Recent Work},
  journal =    {ACM Comput. Surv.},
  year =       {2015},
  volume =     {48},
  number =     {1},
  pages =      {14:1--14:34},
  month =      sep,
  abstract =   {Digital video enables manifold ways of multimedia content interaction.
	Over the last decade, many proposals for improving and enhancing
	video content interaction were published. More recent work particularly
	leverages on highly capable devices such as smartphones and tablets
	that embrace novel interaction paradigms, for example, touch, gesture-based
	or physical content interaction. In this article, we survey literature
	at the intersection of Human-Computer Interaction and Multimedia.
	We integrate literature from video browsing and navigation, direct
	video manipulation, video content visualization, as well as interactive
	video summarization and interactive video retrieval. We classify
	the reviewed works by the underlying interaction method and discuss
	the achieved improvements so far. We also depict a set of open problems
	that the video interaction community should address in future.},
  acmid =      {2808796},
  address =    {New York, NY, USA},
  articleno =  {14},
  doi =        {10.1145/2808796},
  file =       {Schoeffmann2015.pdf:Schoeffmann2015.pdf:PDF},
  issn =       {0360-0300},
  issue_date = {September 2015},
  keywords =   {Video search and retrieval, human-computer interaction, mobile devices, rank5},
  numpages =   {34},
  publisher =  {ACM},
  review =     {Useful for background section.}
}

@InProceedings{Schubert2004,
  author =    {Schubert, Emery and Wolfe, Joe and Tarnopolsky, Alex},
  title =     {Spectral centroid and timbre in complex, multiple instrumental textures},
  booktitle = {Proc. 8th International Conference on Music Perception \& Cognition},
  year =      {2004},
  month =     {08},
  file =      {Schubert2004.pdf:Schubert2004.pdf:PDF},
  owner =     {chrisbau},
  timestamp = {2018.01.14}
}

@Unpublished{SCISYS2015,
  author =    {{SCISYS}},
  title =     {dira! Radio},
  note =      {Accessed 20/01/2018},
  year =      {2015},
  owner =     {chrisbau},
  timestamp = {2015.02.12},
  url =       {http://www.scisys.co.uk/where-we-work/media-broadcast/our-solutions/dira-radio.html}
}

@InProceedings{Sell2014,
  author =    {Gregory Sell and Pascal Clark},
  title =     {Music Tonality Features for Speech/Music Discrimination},
  booktitle = {Proc. IEEE International Conference on Acoustic, Speech and Signal Processing (ICASSP)},
  year =      {2014},
  abstract =  {We introduce a novel set of features for speech/music discrimination
	derived from chroma vectors, a feature that represents musical tonality.
	These features are shown to out-perform other commonly used features
	in multiple conditions and corpora. Even when trained on mismatched
	data, the new features perform well on their own and also combine
	with existing features for further improvement. We report 97.1% precision
	on speech and 93.0% precision on music for the Broadcast News corpus
	using a simple classifier trained on a mismatched corpus.},
  doi =       {10.1109/ICASSP.2014.6854048},
  file =      {Sell2014.pdf:Sell2014.pdf:PDF},
  keywords =  {smd, rank3},
  owner =     {Chris},
  review =    {Introduces chroma-based feature for SMD which looks promising. No
	implemention available.},
  status =    {printed, read},
  timestamp = {2014.05.28}
}

@InProceedings{Seyerlehner2007,
  author =    {Klaus Seyerlehner and Tim Pohle and Markus Schedl and Gerhard Widmer},
  title =     {Automatic Music Detection in Television Productions},
  booktitle = {Proc. of the 10th International Conference on Digital Audio Effects (DAFx)},
  year =      {2007},
  abstract =  {This paper presents methods for the automatic detection of music within
	audio streams, in the fore- or background. The problem occurs in
	the context of a real-world application, namely, the analysis of
	TV productions w.r.t. the use of music. In contrast to plain speech/music
	discrimination, the problem of detecting music in TV productions
	is extremely difficult, since music is often used to accentuate scenes
	while concurrently speech and any kind of noise signals might be
	present. We present results of extensive ex periments with a set
	of standard machine learning algorithms and standard features, investigate
	the difference between frame-level and clip-level features, and demonstrate
	the importance of the application of smoothing functions as a post-processing
	step. Finally, we propose a new feature, called Continuous Frequency
	Activation (CFA), especially designed for music detection, and show
	experimentally that this feature is more precise than the other approaches
	in identifying segments with music in audio streams.},
  file =      {Seyerlehner2007.pdf:Seyerlehner2007.pdf:PDF},
  keywords =  {smd},
  owner =     {Chris},
  review =    {Introduces continuous frequency activation for speech music discrimination},
  status =    {printed, read},
  timestamp = {2014.05.29},
  url =       {http://dafx.labri.fr/main/papers/p221.pdf}
}

@Book{Shalabh2009,
  title =     {Statistical Analysis of Designed Experiments, Third Edition},
  publisher = {Springer},
  year =      {2009},
  author =    {Helge Toutenburg Shalabh},
  doi =       {10.1007/978-1-4419-1148-3},
  file =      {Shalabh2009.pdf:Shalabh2009.pdf:PDF},
  isbn =      {9781489983398},
  owner =     {chrisbau},
  timestamp = {2018.01.23}
}

@InProceedings{Shin2016,
  author =    {Shin, Hijung Valentina and Li, Wilmot and Durand, Fr{\'e}do},
  title =     {Dynamic Authoring of Audio with Linked Scripts},
  booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
  year =      {2016},
  series =    {UIST '16},
  pages =     {509--516},
  address =   {Tokyo, Japan},
  publisher = {ACM},
  abstract =  {Speech recordings are central to modern media from podcasts to audio
	books to e-lectures and voice-overs. Authoring these recordings involves
	an iterative back and forth process between script writing/editing
	and audio recording/editing. Yet, most existing tools treat the script
	and the audio separately, making the back and forth workflow very
	tedious. We present Voice Script, an interface to support a dynamic
	workflow for script writing and audio recording/editing. Our system
	integrates the script with the audio such that, as the user writes
	the script or records speech, edits to the script are translated
	to the audio and vice versa. Through informal user studies, we demonstrate
	that our interface greatly facilitates the audio authoring process
	in various scenarios.},
  acmid =     {2984561},
  doi =       {10.1145/2984511.2984561},
  file =      {Shin2016.pdf:Shin2016.pdf:PDF},
  isbn =      {978-1-4503-4189-9},
  keywords =  {audio recording, dynamic workflows, scripting, transcript-based editing, voice-over},
  numpages =  {8},
  review =    {Semantic speech editing system that integrates smoothly with pre-written
	notes and scripts.
	
	Informal study of 4 users found that it was flexible for different
	workflows, and the 'master script' view was helpful.
	
	A comparitive study with Rubin's system of 4 users found that it was
	faster, and there was value in the 'master script' and 'compare view'
	features.}
}

@InProceedings{Signer2010,
  author =    {Signer, Beat and Norrie, Moira C},
  title =     {Interactive paper: past, present and future},
  booktitle = {Proc. UbiComp},
  year =      {2010},
  file =      {Signer2010.pdf:Signer2010.pdf:PDF},
  keywords =  {rank3},
  review =    {Review of digital pen technology from 2010}
}

@InProceedings{Sigtia2014,
  author =    {Siddharth Sigtia and Simon Dixon},
  title =     {Improved Music Feature Learning with Deep Neural Networks},
  booktitle = {Proc. IEEE International Conference on Acoustic, Speech and Signal Processing (ICASSP)},
  year =      {2014},
  abstract =  {Recent advances in neural network training provide a way to efficiently
	learn representations from raw data. Good representations are an
	important requirement for Music Information Retrieval (MIR) tasks
	to be performed successfully. However, a major problem with neural
	networks is that training time becomes prohibitive for very large
	datasets and the learning algorithm can get stuck in local minima
	for very deep and wide network architectures. In this paper we examine
	3 ways to improve feature learning for audio data using neural networks:
	1.using Rectified Linear Units (ReLUs) instead of standard sigmoid
	units; 2.using a powerful regularisation technique called Dropout;
	3.using Hessian-Free (HF) optimisation to improve training of sigmoid
	nets. We show that these methods provide significant improvements
	in training time and the features learnt are better than state of
	the art hand crafted features, with a genre classification accuracy
	of 83 ± 1.1% on the Tzanetakis (GTZAN) dataset. We found that the
	rectifier networks learnt better features than the sigmoid networks.
	We also demonstrate the capacity of the features to capture relevant
	information from audio data by applying them to genre classification
	on the ISMIR 2004 dataset.},
  file =      {Sigtia2014.pdf:Sigtia2014.pdf:PDF},
  keywords =  {deep},
  owner =     {Chris},
  review =    {QMUL paper on learning audio features using deep neural nets with
	Rectified Linear Units (ReLUs), Dropout regularisation and Hessian-Free
	optimisation. Uses FFT input (1024 frame, 22050Hz sampling, 50% overlap).
	Gives results for 50-unit and 500-unit layers.},
  status =    {printed, read},
  timestamp = {2014.05.28}
}

@Book{Silverman2016,
  title =     {Qualitative research},
  publisher = {Sage},
  year =      {2016},
  author =    {Silverman, David},
  isbn =      {9781473916579}
}

@Article{Singer2017,
  author =   {Lauren M. Singer and Patricia A. Alexander},
  title =    {Reading Across Mediums: Effects of Reading Digital and Print Texts on Comprehension and Calibration},
  journal =  {The Journal of Experimental Education},
  year =     {2017},
  volume =   {85},
  number =   {1},
  pages =    {155-172},
  abstract = {This study explored differences that might exist in comprehension
	when students read digital and print texts. Ninety undergraduates
	read both digital and print versions of newspaper articles and book
	excerpts on topics of childhood ailments. Prior to reading texts
	in counterbalanced order, topic knowledge was assessed and students
	were asked to state medium preferences. After reading, students were
	asked to judge under which medium they comprehended best. Results
	demonstrated a clear preference for digital texts, and students typically
	predicted better comprehension when reading digitally. However, performance
	was not consistent with students' preferences and outcome predictions.
	While there were no differences across mediums when students identified
	the main idea of the text, students recalled key points linked to
	the main idea and other relevant information better when engaged
	with print. No differences in reading outcomes or calibration were
	found for newspaper or book excerpts.},
  doi =      {10.1080/00220973.2016.1143794},
  file =     {Singer2017.pdf:Singer2017.pdf:PDF},
  keywords = {rank5},
  review =   {Students recalled key points linked to the main idea and other relevant
	information better when engaged with printed text, compared to digital
	text.}
}

@InProceedings{Sivaraman2016,
  author =    {Sivaraman, Venkatesh and Yoon, Dongwook and Mitros, Piotr},
  title =     {Simplified Audio Production in Asynchronous Voice-Based Discussions},
  booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
  year =      {2016},
  series =    {CHI '16},
  pages =     {1045--1054},
  address =   {Santa Clara, California, USA},
  publisher = {ACM},
  abstract =  {Voice communication adds nuance and expressivity to virtual discussions,
	but its one-shot nature tends to discourage collaborators from utilizing
	it. However, text-based interfaces have made voice editing much easier,
	especially with recent advancements enabling live, time-aligned speech
	transcription. We introduce SimpleSpeech, an easy-to-use platform
	for asynchronous audio communication (AAC) with lightweight tools
	for inserting content, adjusting pauses, and correcting transcript
	errors. Qualitative and quantitative results suggest that novice
	audio producers, such as high school students, experience decreased
	mental workload when using SimpleSpeech to produce audio messages
	than without editing. The linguistic formality in SimpleSpeech messages
	was also studied, and found to form a middle ground between oral
	and written media. Our findings on editable voice messages show new
	implications for the optimal design and use cases of AAC systems.},
  acmid =     {2858416},
  doi =       {10.1145/2858036.2858416},
  file =      {Sivaraman2016.pdf:Sivaraman2016.pdf:PDF},
  isbn =      {978-1-4503-3362-7},
  keywords =  {asynchronous audio communication, speech editing, transcription-based editing, rank5},
  numpages =  {10}
}

@InProceedings{Sjolander2000,
  author =    {Sj{\"o}lander, K{\aa}re and Beskow, Jonas},
  title =     {Wavesurfer-an open source speech tool.},
  booktitle = {Proc. INTERSPEECH},
  year =      {2000},
  pages =     {464--467},
  file =      {Sjolander2000.pdf:Sjolander2000.pdf:PDF},
  keywords =  {interface, rank2}
}

@InBook{Smaragdis2009,
  chapter =   {1},
  pages =     {1--34},
  title =     {Context Extraction Through Audio Signal Analysis},
  publisher = {Springer},
  year =      {2009},
  author =    {Smaragdis, Paris and Radhakrishnan, Regunathan and Wilson, Kevin W.},
  editor =    {Divakaran, Ajay},
  abstract =  {A lot of multimedia content comes with a soundtrack which is often not taken advantage of by content analysis applications. In this chapter we cover some of the essential techniques for performing context analysis from audio signals. We describe the most popular approaches in representing audio signals, learning their structure and constructing classifiers that recognize specific sounds, as well as algorithms for locating where sounds are coming from. All these tools when used in the context of content analysis can provide powerful descriptors that can help us find various events which would be hard to locate otherwise.},
  booktitle = {Multimedia Content Analysis: Theory and Applications},
  doi =       {10.1007/978-0-387-76569-3_1},
  file =      {Smaragdis2009.pdf:Smaragdis2009.pdf:PDF},
  isbn =      {978-0-387-76569-3},
  owner =     {chrisbau},
  timestamp = {2017.12.07}
}

@Article{Smith1978,
  author =     {Smith, Alvy Ray},
  title =      {Color Gamut Transform Pairs},
  journal =    {SIGGRAPH Comput. Graph.},
  year =       {1978},
  volume =     {12},
  number =     {3},
  pages =      {12--19},
  month =      aug,
  acmid =      {807361},
  address =    {New York, NY, USA},
  doi =        {10.1145/965139.807361},
  issn =       {0097-8930},
  issue_date = {August 1978},
  keywords =   {Brightness, Color, Color transform, Gamut, Hue, Luminance, NTSC, Saturation, Value},
  numpages =   {8},
  publisher =  {ACM}
}

@Book{Smith2007,
  title =     {Mathematics of the Discrete Fourier Transform ({DFT}), with Audio Applications --- Second Edition'},
  publisher = {W3K Publishing},
  year =      {2007},
  author =    {Smith, III, Julius O.},
  isbn =      {978-0-9745607-4-8},
  owner =     {chrisbau},
  timestamp = {2018.01.25},
  url =       {https://ccrma.stanford.edu/~jos/mdft/}
}

@Article{Smith1931,
  author =   {T Smith and J Guild},
  title =    {The C.I.E. colorimetric standards and their use},
  journal =  {Transactions of the Optical Society},
  year =     {1931},
  volume =   {33},
  number =   {3},
  pages =    {73},
  abstract = {The new international standards, which define a standard observer,
	three standard illuminants, standard conditions of illuminating and
	viewing opaque specimens, a standard for evaluating the brightness
	factor of opaque specimens, and a standard trichromatic system for
	the expression of colour measurements, are stated and their origin
	explained. In addition to the numerical tables which are appended
	to the resolutions setting up these standards, there are given a
	table specifying the trichromatic coordinates for the standard observer
	of all spectral colours at wave-length intervals of 1 mu, tables
	to facilitate the calculation of the standard coordinates and the
	brightness factor of a material illuminated by any one of the three
	standard illuminants from spectrophotometric measurements on the
	material, and a table giving the coordinates of some stimuli of special
	importance on the N.P.L. system, the standard system, and another
	system which occurs in the resolutions. Some new colorimetric terms
	are proposed, partly to avoid misinterpretation and partly to meet
	new needs. The theory of colour transformations, and points which
	arise in the application of the system and in the calibration of
	instruments, are discussed.},
  url =      {http://stacks.iop.org/1475-4878/33/i=3/a=301}
}

@Article{Spence2011,
  author =    {Charles Spence},
  title =     {Crossmodal correspondences: A tutorial review},
  journal =   {Attention, Perception, \& Psychophysics},
  year =      {2011},
  volume =    {73},
  number =    {4},
  pages =     {971--975},
  doi =       {10.3758/s13414-010-0073-7},
  file =      {Spence2011.pdf:Spence2011.pdf:PDF},
  keywords =  {rank5},
  owner =     {Chris},
  review =    {Review paper on cross-modal links (or 'correspondences') between vision
	and audition. Looked at from a psychophysical perspective, so experiments
	are usually based on 'speeded' test (performing a task as quickly
	as possible and measuring reaction times), or 'unspeeded' (judging
	which visual or acoustic event came first). In the former, reaction
	times are shorter when the sound and vision are 'congruent' and in
	the latter, the JND time is higher for congruent stimuli (they appear
	to merge together). The paper references studies which show the following
	cross-modal links: - Loudness/brightness - Pitch/elevation - Pitch/size
	- Loudness/size - Pitch/spatial frequency},
  status =    {printed, read},
  timestamp = {2014.01.07}
}

@InProceedings{Stark2000,
  author =    {Stark, Litza and Whittaker, Steve and Hirschberg, Julia},
  title =     {{ASR} satisficing: The effects of {ASR} accuracy on speech retrieval},
  booktitle = {Proc. Sixth International Conference on Spoken Language Processing},
  year =      {2000},
  file =      {Stark2000.pdf:Stark2000.pdf:PDF},
  keywords =  {rank4},
  owner =     {chrisbau},
  timestamp = {2017.11.29},
  url =       {http://www.isca-speech.org/archive/archive_papers/icslp_2000/i00_3a69.pdf}
}

@Article{Stevens1937,
  author =    {S. S. Stevens and J. Volkmann},
  title =     {A Scale for the Measurement of the Psychological Magnitude Pitch},
  journal =   {The Journal of the Acoustical Society of America},
  year =      {1937},
  volume =    {8},
  number =    {185},
  doi =       {10.1121/1.1915893},
  file =      {Stevens1937.pdf:Stevens1937.pdf:PDF},
  owner =     {chrisbau},
  review =    {Original Mel-scale paper},
  timestamp = {2018.01.13}
}

@InProceedings{Stifelman2001,
  author =    {Stifelman, Lisa and Arons, Barry and Schmandt, Chris},
  title =     {{The Audio Notebook}: Paper and Pen Interaction with Structured Speech},
  booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  year =      {2001},
  series =    {CHI '01},
  pages =     {182--189},
  address =   {Seattle, Washington, USA},
  publisher = {ACM},
  acmid =     {365096},
  doi =       {10.1145/365024.365096},
  file =      {Stifelman2001.pdf:Stifelman2001.pdf:PDF},
  isbn =      {1-58113-327-8},
  keywords =  {acoustic structuring, audio, paper, pen interaction, speech, speech as data, speech interfaces, user structuring, rank5},
  numpages =  {8}
}

@InProceedings{Stowell2008,
  author =        {Stowell, D. and Plumbley, M. D.},
  title =         {Robustness and independence of voice timbre features under live performance acoustic degradations},
  booktitle =     {Proceedings of the 11th Conference on Digital Audio Effects (DAFx-08)},
  year =          {2008},
  pages =         {325--332},
  month =         {Sep},
  bdsk-url-1 =    {http://www.acoustics.hut.fi/dafx08/papers/dafx08_58.pdf},
  date-added =    {2008-03-19 11:44:12 +0000},
  date-modified = {2009-01-26 10:53:14 +0000},
  file =          {Stowell2008.pdf:Stowell2008.pdf:PDF},
  keywords =      {voice, vocal, timbre, robust, features, entropy, singing, speech, beatboxing},
  read =          {Yes},
  status =        {printed},
  url =           {http://www.acoustics.hut.fi/dafx08/papers/dafx08_58.pdf}
}

@Article{Stowell2009,
  author =    {D. Stowell and A. Robertson and N. Bryan-Kinns and M. D. Plumbley},
  title =     {Evaluation of live human-computermusic-making: quantitative and qualitative approaches},
  journal =   {International Journal of Human-Computer Studies},
  year =      {2009},
  volume =    {67},
  number =    {11},
  pages =     {960-975},
  month =     {November},
  abstract =  {Live music-making using interactive systems is not completely amenable
	to traditional HCI evaluation metrics such as task-completion rates.
	In this paper we discuss quantitative and qualitative approaches
	which provide opportunities to evaluate the music-making interaction,
	accounting for aspects which cannot be directly measured or expressed
	numerically, yet which may be important for participants. We present
	case studies in the application of a qualitative method based on
	Discourse Analysis, and a quantitative method based on the Turing
	Test. We compare and contrast these methods with each other, and
	with other evaluation approaches used in the literature, and discuss
	factors aecting which evaluation methods are appropriate in a given
	context.},
  file =      {Stowell2009.pdf:Stowell2009.pdf:PDF},
  keywords =  {rank2},
  owner =     {Chris},
  review =    {Discusses the use of discourse analysis and Turing test in evaluating
	musical instrument interfaces. Useful introduction.},
  status =    {printed, read},
  timestamp = {2013.09.22}
}

@Article{Suhm2001,
  author =     {Suhm, Bernhard and Myers, Brad and Waibel, Alex},
  title =      {Multimodal Error Correction for Speech User Interfaces},
  journal =    {ACM Trans. Comput.-Hum. Interact.},
  year =       {2001},
  volume =     {8},
  number =     {1},
  pages =      {60--98},
  month =      mar,
  acmid =      {371166},
  address =    {New York, NY, USA},
  doi =        {10.1145/371127.371166},
  file =       {Suhm2001.pdf:Suhm2001.pdf:PDF},
  issn =       {1073-0516},
  issue_date = {March 2001},
  keywords =   {dictation systems, interactive error correction, multimodal interfaces, pen input, performance model, speech input, speech user interfaces, rank5},
  numpages =   {39},
  publisher =  {ACM},
  review =     {pp. 82 suggests confidence shading slows down correction, but not
	significantly. Success obviously depends on reliability of confidence
	scores.}
}

@InProceedings{Sun2004,
  author =    {Sun, David and Xia, Steven and Sun, Chengzheng and Chen, David},
  title =     {Operational Transformation for Collaborative Word Processing},
  booktitle = {Proceedings of the 2004 ACM Conference on Computer Supported Cooperative Work},
  year =      {2004},
  series =    {CSCW '04},
  pages =     {437--446},
  publisher = {ACM},
  acmid =     {1031681},
  doi =       {10.1145/1031607.1031681},
  file =      {Sun2004.pdf:Sun2004.pdf:PDF},
  isbn =      {1-58113-810-5},
  keywords =  {group undo, multi-versioning, operational transformation, rank3},
  numpages =  {10}
}

@InProceedings{Towsey2014,
  author =    {Michael Towsey and Liang Zhang and Mark Cottman-Fields and Jason Wimmer and Jinglan Zhang and Paul Roe},
  title =     {Visualization of Long-duration Acoustic Recordings of the Environment},
  booktitle = {Proc. International Conference on Computational Science},
  year =      {2014},
  pages =     {703 - 712},
  abstract =  {Abstract Acoustic recordings of the environment are an important aid
	to ecologists monitoring biodiversity and environmental health. However,
	rapid advances in recording technology, storage and computing make
	it possible to accumulate thousands of hours of recordings, of which,
	ecologists can only listen to a small fraction. The big-data challenge
	addressed in this paper is to visualize the content of long-duration
	audio recordings on multiple scales, from hours, days, months to
	years. The visualization should facilitate navigation and yield ecologically
	meaningful information. Our approach is to extract (at one minute
	resolution) acoustic indices which reflect content of ecological
	interest. An acoustic index is a statistic that summarizes some aspect
	of the distribution of acoustic energy in a recording. We combine
	indices to produce false-color images that reveal acoustic content
	and facilitate navigation through recordings that are months or even
	years in duration.},
  doi =       {10.1016/j.procs.2014.05.063},
  file =      {Towsey2014.pdf:Towsey2014.pdf:PDF},
  issn =      {1877-0509},
  journal =   {Proc. International Conference on Computational Science},
  keywords =  {rank4},
  review =    {Taking month- or year-long environmental recordings and using false
	colour to visualize them. Uncovered interesting things like how the
	time of the birds' morning call moves throughout the year.},
  status =    {printed}
}

@Article{Tranter2006,
  author =   {Tranter, S.E. and Reynolds, D.A.},
  title =    {An overview of automatic speaker diarization systems},
  journal =  {Audio, Speech, and Language Processing, IEEE Transactions on},
  year =     {2006},
  volume =   {14},
  number =   {5},
  pages =    {1557-1565},
  abstract = {Audio diarization is the process of annotating an input audio channel
	with information that attributes (possibly overlapping) temporal
	regions of signal energy to their specific sources. These sources
	can include particular speakers, music, background noise sources,
	and other signal source/channel characteristics. Diarization can
	be used for helping speech recognition, facilitating the searching
	and indexing of audio archives, and increasing the richness of automatic
	transcriptions, making them more readable. In this paper, we provide
	an overview of the approaches currently used in a key area of audio
	diarization, namely speaker diarization, and discuss their relative
	merits and limitations. Performances using the different techniques
	are compared within the framework of the speaker diarization task
	in the DARPA EARS Rich Transcription evaluations. We also look at
	how the techniques are being introduced into real broadcast news
	systems and their portability to other domains and tasks such as
	meetings and speaker verification},
  doi =      {10.1109/TASL.2006.878256},
  file =     {Tranter2006.pdf:Tranter2006.pdf:PDF},
  issn =     {1558-7916},
  keywords = {audio signal processing, speaker recognition, DARPA EARS Rich Transcription evaluations, audio archives, audio diarization, automatic speaker diarization systems, automatic transcriptions, broadcast news systems, input audio channel annotation, speaker verification, speech recognition, temporal signal energy region, Acoustic noise, Broadcasting, Colored noise, Data mining, Indexing, Loudspeakers, Multiple signal classification, Music, Signal processing, Speech recognition, Speaker diarization, speaker segmentation and clustering, rank3},
  status =   {printed, read}
}

@InProceedings{Truong2016,
  author =    {Truong, Anh and Berthouzoz, Floraine and Li, Wilmot and Agrawala, Maneesh},
  title =     {QuickCut: An Interactive Tool for Editing Narrated Video},
  booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
  year =      {2016},
  series =    {UIST '16},
  pages =     {497--507},
  address =   {Tokyo, Japan},
  publisher = {ACM},
  acmid =     {2984569},
  doi =       {10.1145/2984511.2984569},
  file =      {Truong2016.pdf:Truong2016.pdf:PDF},
  isbn =      {978-1-4503-4189-9},
  keywords =  {video annotation, video editing, video segmentation, rank4},
  numpages =  {11}
}

@InProceedings{Tsiros2013,
  author =    {Augoustinos Tsiros},
  title =     {The Dimensions and Complexities of Audio-Visual Association},
  booktitle = {Proc. Electronic Visualisation and the Arts},
  year =      {2013},
  file =      {Tsiros2013.pdf:Tsiros2013.pdf:PDF},
  keywords =  {multimodal, rank3},
  owner =     {Chris},
  review =    {Comparison of the principles behind vision and audition. Contains
	lots of useful references on perception, psychology and philosophy.
	Concludes that more work needs to be done.},
  status =    {printed, read},
  timestamp = {2013.11.21}
}

@InProceedings{Tsiros2014,
  author =    {Tsiros, A.},
  title =     {Evaluating the Perceived Similarity Between Audio-Visual Features Using Corpus-Based Concatenative Synthesis},
  booktitle = {Proceedings of the Internation Conference on New Interfaces for Musical Expression},
  year =      {2014},
  file =      {Tsiros2014.pdf:Tsiros2014.pdf:PDF},
  keywords =  {rank5},
  owner =     {Chris},
  review =    {Two-stage user study of audio-visual similarity by (a) manually matching
	visual features to audio features and (b) generating images from
	sound.},
  status =    {printed},
  timestamp = {2014.07.15},
  url =       {http://www.nime.org/proceedings/2014/nime2014_484.pdf}
}

@Article{Tucker2003,
  author =    {Tucker, Roger CF and Hickey, Marianne and Haddock, Nick},
  title =     {Speech-as-data technologies for personal information devices},
  journal =   {Personal and Ubiquitous Computing},
  year =      {2003},
  volume =    {7},
  number =    {1},
  pages =     {22--29},
  file =      {Tucker2003.pdf:Tucker2003.pdf:PDF},
  keywords =  {rank2},
  publisher = {Springer-Verlag},
  review =    {Suggested by David Frolich who knows the author.}
}

@InCollection{Tucker2005,
  author =    {Tucker, Simon and Whittaker, Steve},
  title =     {Accessing Multimodal Meeting Data: Systems, Problems and Possibilities},
  booktitle = {Machine Learning for Multimodal Interaction},
  publisher = {Springer Berlin Heidelberg},
  year =      {2005},
  editor =    {Bengio, Samy and Bourlard, HervÃ©},
  volume =    {3361},
  series =    {Lecture Notes in Computer Science},
  pages =     {1-11},
  doi =       {10.1007/978-3-540-30568-2_1},
  file =      {Tucker2005.pdf:Tucker2005.pdf:PDF},
  isbn =      {978-3-540-24509-4},
  keywords =  {rank5},
  language =  {English},
  review =    {Chapter on meeting data}
}

@InProceedings{Tucker2006,
  author =    {Tucker, Simon and Whittaker, Steve},
  title =     {Time is of the Essence: An Evaluation of Temporal Compression Algorithms},
  booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  year =      {2006},
  series =    {CHI '06},
  pages =     {329--338},
  address =   {Montr\'{e}al, Qu\'{e}bec, Canada},
  publisher = {ACM},
  acmid =     {1124822},
  doi =       {10.1145/1124772.1124822},
  file =      {Tucker2006.pdf:Tucker2006.pdf:PDF},
  isbn =      {1-59593-372-7},
  keywords =  {audio interfaces, evaluation methods, excision, meetings interfaces, speech manipulation, speech summary, speech-as-data, speed-up, summarization, temporal compression, rank3},
  numpages =  {10},
  review =    {Compares sped-up speech with two transcription-based speech skimmers:
	utterance removal and word removal. Shows that speed-up is effective
	for long (\textasciitilde 30min) recordings, but is 'too fast'.},
  status =    {printed, read}
}

@Book{Tufte2001,
  title =     {The Visual Display of Quantitative Information: Second Edition},
  publisher = {Graphics Press},
  year =      {2001},
  author =    {Edward Rolf Tufte},
  keywords =  {rank3},
  owner =     {Chris},
  review =    {Oulines principles of visual design},
  status =    {read},
  timestamp = {2013.12.05}
}

@Misc{TVLicensing2017,
  author =    {{TV Licensing}},
  title =     {TV Licensing Annual Review 2016/2017},
  year =      {2017},
  owner =     {chrisbau},
  timestamp = {2018.01.20},
  url =       {http://www.tvlicensing.co.uk/about/our-performance-AB6}
}

@Article{Tzanetakis1999,
  author =     {Tzanetakis, George and Cook, Perry},
  title =      {MARSYAS: A Framework for Audio Analysis},
  journal =    {Org. Sound},
  year =       {1999},
  volume =     {4},
  number =     {3},
  pages =      {169--175},
  month =      dec,
  acmid =      {972857},
  address =    {New York, NY, USA},
  doi =        {10.1017/S1355771800003071},
  file =       {Tzanetakis1999.pdf:Tzanetakis1999.pdf:PDF},
  issn =       {1355-7718},
  issue_date = {December 1999},
  keywords =   {rank5},
  numpages =   {7},
  publisher =  {Cambridge University Press}
}

@InProceedings{Tzanetakis2000,
  author =    {George Tzanetakis and Perry Cook},
  title =     {Audio Information Retrieval (AIR) Tools},
  booktitle = {Proc. International Society for Music Information Retrieval (ISMIR)},
  year =      {2000},
  file =      {Tzanetakis2000.pdf:Tzanetakis2000.pdf:PDF},
  keywords =  {rank5},
  owner =     {Chris},
  review =    {TimbreGrams},
  timestamp = {2014.02.04},
  url =       {http://ismir2000.ismir.net/papers/tzanetakis_abs.pdf}
}

@InProceedings{Tzanetakis2001,
  author =    {Tzanetakis, George and Cook, Perry},
  title =     {{MARSYAS3D}: A prototype audio browser-editor using a large scale immersive visual and audio display},
  booktitle = {Proceedings of the 2001 International Conference on Auditory Display},
  year =      {2001},
  pages =     {250--254},
  address =   {Espoo, Finland},
  month =     jul,
  file =      {Tzanetakis2001.pdf:Tzanetakis2001.pdf:PDF},
  owner =     {chrisbau},
  timestamp = {2017.12.05},
  url =       {http://hdl.handle.net/1853/50625}
}

@InProceedings{Vemuri2004,
  author =    {Vemuri, Sunil and DeCamp, Philip and Bender, Walter and Schmandt, Chris},
  title =     {Improving Speech Playback Using Time-compression and Speech Recognition},
  booktitle = {Proc. SIGCHI Conference on Human Factors in Computing Systems},
  year =      {2004},
  series =    {CHI '04},
  pages =     {295--302},
  address =   {Vienna, Austria},
  publisher = {ACM},
  acmid =     {985730},
  doi =       {10.1145/985692.985730},
  file =      {Vemuri2004.pdf:Vemuri2004.pdf:PDF},
  isbn =      {1-58113-702-8},
  keywords =  {information retrieval, speech recognition, time-compressed audio, rank5},
  numpages =  {8},
  review =    {Found that confidence shading improved comprehension of ASR, but not
	significantly.}
}

@InProceedings{Waibel2003,
  author =    {Waibel, Alex and Schultz, T. and Bett, M. and Denecke, M. and Malkin, R. and Rogina, I. and Stiefelhagen, R. and Jie Yang},
  title =     {SMaRT: the Smart Meeting Room Task at ISL},
  booktitle = {Proc, IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
  year =      {2003},
  volume =    {4},
  pages =     {IV-752-5 vol.4},
  month =     {April},
  abstract =  {As computational and communications systems become increasingly smaller,
	faster, more powerful, and more integrated, the goal of interactive,
	integrated meeting support rooms is slowly becoming reality. It is
	already possible, for instance, to rapidly locate task-related information
	during a meeting, filter it, and share it with remote users. Unfortunately,
	the technologies that provide such capabilities are as obstructive
	as they are useful - they force humans to focus on the tool rather
	than the task. Thus the veneer of utility often hides the true costs
	of use, which are longer, less focused human interactions. To address
	this issue, we present our current research efforts towards SMaRT:
	the Smart Meeting Room Task. The goal of SMaRT is to provide meeting
	support services that do not require explicit human-computer interaction.
	Instead, by monitoring the activities in the meeting room using both
	video and audio analysis, the room is able to react appropriately
	to users' needs and allow the users to focus on their own goals.},
  doi =       {10.1109/ICASSP.2003.1202752},
  file =      {Waibel2003.pdf:Waibel2003.pdf:PDF},
  issn =      {1520-6149},
  keywords =  {business data processing, interactive systems, user interfaces, ISL, SMaRT, Smart Meeting Room Task, integrated meeting support rooms, interactive meeting support rooms, monitoring, Computer interfaces, Costs, Humans, Information filtering, Information filters, Interactive systems, Laboratories, Merging, Monitoring, Switches, rank1},
  review =    {Summarisation of meeting room analysis technology}
}

@Article{Wald2007,
  author =    {Mike Wald and John-Mark Bell and Philip Boulain and Karl Doody and Jim Gerrard},
  title =     {Correcting automatic speech recognition captioning errors in real time},
  journal =   {International Journal of Speech Technology},
  year =      {2007},
  volume =    {10},
  number =    {1},
  doi =       {10.1007/s10772-008-9014-4},
  file =      {Wald2007.pdf:Wald2007.pdf:PDF},
  keywords =  {rank2},
  owner =     {chrisbau},
  review =    {Real-time ASR correction},
  timestamp = {2015.04.07}
}

@Article{Wanderley2002,
  author =    {Marcelo Mortensen Wanderley and Nicola Orio},
  title =     {Evaluation of Input Devices for Musical Expression: Borrowing Tools from HCI},
  journal =   {Computer Music Journal},
  year =      {2002},
  volume =    {23},
  number =    {3},
  pages =     {62--76},
  file =      {Wanderley2002.pdf:Wanderley2002.pdf:PDF},
  keywords =  {eval},
  owner =     {Chris},
  timestamp = {2014.03.03}
}

@Misc{Ward,
  author =       {Simon Ward},
  title =        {Reshaping the {BBC} Newsroom},
  howpublished = {EBU},
  month =        dec,
  year =         {2012},
  file =         {:Ward2012.ppt:PowerPoint},
  owner =        {chrisbau},
  timestamp =    {2018.01.19}
}

@InProceedings{Weher1994,
  author =    {Weher, Karon and Poon, Alex},
  title =     {Marquee: A Tool for Real-time Video Logging},
  booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  year =      {1994},
  series =    {CHI '94},
  pages =     {58--64},
  address =   {Boston, MA, USA},
  publisher = {ACM},
  acmid =     {191697},
  doi =       {10.1145/191666.191697},
  file =      {Weher1994.pdf:Weher1994.pdf:PDF},
  isbn =      {0-89791-650-6},
  keywords =  {gestural interfaces, multimedia, penbased computing, user interfaces, user studies, video annotation, video indexing, rank5},
  numpages =  {7},
  review =    {Weher and Poon (1994) presented 'Marquee', which was a system for
	supporting the task of logging during a live video recording. Users
	could make synchronised handwritten notes by drawing a horizontal
	line to mark a timestamp, then writing their notes below. Additionally,
	they could create a list of keywords, and add them to their notes
	by pressing them at the right moment. An evaluation of the system
	with three users found that users did not partake in discussions
	while logging, which may prevent such a system being used by a radio
	presenter during an interview. The freehand nature of the notes meant
	that it could easily handle the different styles of each user. Users
	reported that they didn't feel they had to make as many notes as
	they normally would, because they could easily refer back to the
	video recording. The authors also noted that videos can be further
	annotated during replay, allowing for an iterative logging process.}
}

@InProceedings{Weibel2012,
  author =    {Weibel, Nadir and Fouse, Adam and Emmenegger, Colleen and Friedman, Whitney and Hutchins, Edwin and Hollan, James},
  title =     {Digital Pen and Paper Practices in Observational Research},
  booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  year =      {2012},
  series =    {CHI '12},
  pages =     {1331--1340},
  acmid =     {2208590},
  doi =       {10.1145/2207676.2208590},
  file =      {Weibel2012.pdf:Weibel2012.pdf:PDF},
  isbn =      {978-1-4503-1015-4},
  keywords =  {activity visualization, annotations, digital ethnography, interactive navigation, paper-digital notes, video analysis},
  numpages =  {10},
  review =    {User study of ChronoViz}
}

@InProceedings{Weibel2008,
  author =       {Weibel, Nadir and Ispas, Adriana and Signer, Beat and Norrie, Moira C},
  title =        {{PaperProof}: a paper-digital proof-editing system},
  booktitle =    {CHI'08 Extended Abstracts on Human Factors in Computing Systems},
  year =         {2008},
  pages =        {2349--2354},
  organization = {ACM},
  file =         {Weibel2008.pdf:Weibel2008.pdf:PDF},
  keywords =     {rank5},
  review =       {Annotation of paper documents using digital pens}
}

@InProceedings{Whittaker2004,
  author =    {Whittaker, Steve and Amento, Brian},
  title =     {Semantic Speech Editing},
  booktitle = {Proc. SIGCHI Conference on Human Factors in Computing Systems},
  year =      {2004},
  series =    {CHI '04},
  pages =     {527--534},
  address =   {Vienna, Austria},
  publisher = {ACM},
  acmid =     {985759},
  doi =       {10.1145/985692.985759},
  file =      {Whittaker2004.pdf:Whittaker2004.pdf:PDF},
  isbn =      {1-58113-702-8},
  keywords =  {acoustic representations, speech browsing, speech editing, speech recognition, speech retrieval, transcripts, rank5},
  numpages =  {8}
}

@InProceedings{Whittaker2000,
  author =    {Whittaker, Steve and Davis, Richard and Hirschberg, Julia and Muller, Urs},
  title =     {Jotmail: A Voicemail Interface That Enables You to See What Was Said},
  booktitle = {Proc. SIGCHI Conference on Human Factors in Computing Systems},
  year =      {2000},
  series =    {CHI '00},
  pages =     {89--96},
  address =   {The Hague, The Netherlands},
  publisher = {ACM},
  acmid =     {332411},
  doi =       {10.1145/332040.332411},
  file =      {Whittaker2000.pdf:Whittaker2000.pdf:PDF},
  isbn =      {1-58113-216-6},
  keywords =  {``speech as data'', annotation, asynchronous communication, empirical evaluation, note-taking, speech access, voicemail, rank4},
  numpages =  {8},
  review =    {Suggested by David Frohlich. Precursor to SCANmail.}
}

@Article{Whittaker2007,
  author =    {Whittaker, Steve and Hirschberg, Julia},
  title =     {Accessing speech data using strategic fixation},
  journal =   {Computer Speech \& Language},
  year =      {2007},
  volume =    {21},
  number =    {2},
  pages =     {296--324},
  doi =       {10.1016/j.csl.2006.06.004},
  file =      {Whittaker2007.pdf:Whittaker2007.pdf:PDF},
  keywords =  {rank4},
  publisher = {Elsevier}
}

@InProceedings{Whittaker2002,
  author =    {Whittaker, Steve and Hirschberg, Julia and Amento, Brian and Stark, Litza and Bacchiani, Michiel and Isenhour, Philip and Stead, Larry and Zamchick, Gary and Rosenberg, Aaron},
  title =     {{SCANMail}: A Voicemail Interface That Makes Speech Browsable, Readable and Searchable},
  booktitle = {Proc. SIGCHI Conference on Human Factors in Computing Systems},
  year =      {2002},
  series =    {CHI '02},
  pages =     {275--282},
  address =   {Minneapolis, Minnesota, USA},
  publisher = {ACM},
  acmid =     {503426},
  doi =       {10.1145/503376.503426},
  file =      {Whittaker2002.pdf:Whittaker2002.pdf:PDF},
  isbn =      {1-58113-453-3},
  keywords =  {"speech as data", asynchronous communication, empirical evaluation, speech access, voicemail, what you see is almost what you hear, rank5},
  numpages =  {8}
}

@InProceedings{Whittaker1999,
  author =    {Whittaker, Steve and Hirschberg, Julia and Choi, John and Hindle, Don and Pereira, Fernando and Singhal, Amit},
  title =     {{SCAN}: Designing and Evaluating User Interfaces to Support Retrieval from Speech Archives},
  booktitle = {Proc. 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
  year =      {1999},
  series =    {SIGIR '99},
  pages =     {26--33},
  address =   {Berkeley, California, USA},
  publisher = {ACM},
  acmid =     {312639},
  doi =       {10.1145/312624.312639},
  file =      {Whittaker1999.pdf:Whittaker1999.pdf:PDF},
  isbn =      {1-58113-096-1},
  keywords =  {comparing interfaces for information access, field/empirical studies of the information seeking process, speech indexing and retrieval, user studies, rank4},
  numpages =  {8}
}

@InProceedings{Wieser2014,
  author =    {Wieser, E. and Husinsky, M. and Seidl, M.},
  title =     {Speech/music discrimination in a large database of radio broadcasts from the wild},
  booktitle = {Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year =      {2014},
  pages =     {2134-2138},
  month =     may,
  abstract =  {This paper describes the development, implementation and evaluation
	of a speech/music detector. We aim at audio from different sources
	with different qualities - i.e. audio from ``the wild''. We examine
	existing approaches for audio classification and select a recent
	feature. We modify the feature and evaluate the classification accuracy
	on a random test set of more than 60 hours of audio material against
	a standard speech/music detection approach. With our approach, we
	reach a classification accuracy of 96,6%. We provide a performant
	open source implementation of our detector.},
  doi =       {10.1109/ICASSP.2014.6853976},
  file =      {Wieser2014.pdf:Wieser2014.pdf:PDF},
  keywords =  {audio signal processing, music, radio networks, speech processing, audio classification, audio material, large database, radio broadcasts, speech-music detector, speech-music discrimination, Accuracy, Conferences, Feature extraction, Materials, Speech, Speech processing, Support vector machines, Audio classification, radio broadcast, speech/music discrimination, rank4},
  owner =     {Chris},
  review =    {Very concise and recent review of SMD. Uses continuous frequency activation as basis for SMD system. Finds that transient activation may be required to detect rhythmic-heavy music. Points out that user-adjustible slider an increase accuracy to 100%. Has open-source implementation.},
  status =    {printed, read}
}

@InProceedings{Wilcox1997,
  author =    {Wilcox, Lynn D. and Schilit, Bill N. and Sawhney, Nitin},
  title =     {Dynomite: A Dynamically Organized Ink and Audio Notebook},
  booktitle = {Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems},
  year =      {1997},
  series =    {CHI '97},
  pages =     {186--193},
  address =   {Atlanta, Georgia, USA},
  acmid =     {258700},
  doi =       {10.1145/258549.258700},
  file =      {Wilcox1997.pdf:Wilcox1997.pdf:PDF},
  isbn =      {0-89791-802-9},
  keywords =  {PDA, audio interfaces, electronic notebook, hand-writing, ink properties, keyword indexing, note-taking, paper-like interfaces, pen computing, retrieval, rank3},
  numpages =  {8}
}

@Article{Williams1949,
  author =   {{Williams}, E.J.},
  title =    {Experimental Designs Balanced for the Estimation of Residual Effects of Treatments},
  journal =  {Australian Journal of Scientific Research A Physical Sciences},
  year =     {1949},
  volume =   {2},
  pages =    {149},
  month =    {June},
  adsnote =  {Provided by the SAO/NASA Astrophysics Data System},
  adsurl =   {http://adsabs.harvard.edu/abs/1949AuSRA...2..149W},
  doi =      {10.1071/PH490149},
  file =     {Williams1949.pdf:Williams1949.pdf:PDF},
  keywords = {rank2},
  review =   {Williams design Latin Square}
}

@Book{Wolcott2009,
  title =     {Writing Up Qualitative Research (3rd ed.)},
  publisher = {SAGE Publications, Inc.},
  year =      {2009},
  author =    {Harry F. Wolcott},
  file =      {Wolcott2009.pdf:Wolcott2009.pdf:PDF},
  owner =     {chrisbau},
  timestamp = {2015.07.31}
}

@Article{Wolfe2004,
  author =    {Wolfe, J.M. and Horowitz, T.S.},
  title =     {What attributes guide the deployment of visual attention and how do they do it?},
  journal =   {Nature Reviews Neuroscience},
  year =      {2004},
  volume =    {5},
  number =    {6},
  pages =     {495--501},
  month =     jun,
  doi =       {10.1038/nrn1411},
  file =      {Wolfe2004.pdf:Wolfe2004.pdf:PDF},
  keywords =  {rank4},
  owner =     {Chris},
  review =    {Lists visual attributes in order of importance in visual search.},
  status =    {printed, read},
  timestamp = {2014.02.10}
}

@Article{Yoo2011,
  author =   {Min-Joon Yoo and In-Kwon Lee},
  title =    {Affecticon: Emotion-Based Icons for Music Retrieval},
  journal =  {Computer Graphics and Applications, IEEE},
  year =     {2011},
  volume =   {31},
  number =   {3},
  pages =    {89-95},
  abstract = {Digital audio is easy to record, play, process, and manage. Its ubiquity
	means that devices for handling it are cheap, letting more people
	record and play music and speech. In addition, the Internet has improved
	access to re corded audio. So, the amount of recorded music that
	people own has rapidly increased. Most current audio players compress
	audio files and store them in internal memory. Because stor age costs
	have consistently declined, the amount of music that can be stored
	has rapidly increased. A player with 16 Gbytes of memory can hold
	ap proximately 3,200 songs if each song is stored in compressed format
	and occupies 5 Mbytes. Effectively organizing such large volumes
	of music is difficult. People often listen repeatedly to a small
	number of favorite songs, while others remain un justifiably neglected.
	We've developed Affecticon, an efficient system for managing music
	collections. Affecticon groups pieces of music that convey similar
	emotions and labels each group with a corresponding icon. These icons
	let listeners easily select music according to its emotional content.
	Experiments have demonstrated Affecticon's effectiveness.},
  doi =      {10.1109/MCG.2011.36},
  file =     {Yoo2011.pdf:Yoo2011.pdf:PDF},
  issn =     {0272-1716},
  keywords = {audio recording;emotion recognition;music;Internet;affecticon;audio file compression;audio players;audio recording;digital audio;emotion-based icons;music retrieval;Computational modeling;Emotion recognition;Image color analysis;Mood;Music information retrieval;Speech processing;System-on-a-chip;computer graphics;graphics and multimedia;harmonograph;human-computer interaction;music emotional retrieval;music retrieval;user interfaces; visualization},
  review =   {Music track icons representing arousal and valence. Brightness depends
	on valence and saturation on arousal. Very basic user test showed
	icons sped up playlist selection.},
  status =   {read}
}

@InProceedings{Yoon2015a,
  author =    {Yoon, Dongwook},
  title =     {Enriching Online Classroom Communication with Collaborative Multi-Modal Annotations},
  booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software \& Technology},
  year =      {2015},
  series =    {UIST '15 Adjunct},
  pages =     {21--24},
  address =   {Daegu, Kyungpook, Republic of Korea},
  publisher = {ACM},
  abstract =  {In massive open online courses, peer discussion is a scalable solution
	for offering interactive and engaging learning experiences to a large
	number of students. On the other hand, the quality of communication
	mediated through online discussion tools, such as discussion forums,
	is far less expressive than that of face-to-face communication. As
	a solution, I present RichReview, a multi-modal annotation system
	through which distant students can exchange ideas using versatile
	combinations of voice, text, and pointing gestures. A series of lab
	and deployment studies of RichReview promised that the expressive
	multimedia mixture and lightweight audio browsing feature help students
	better understand commentators? intention. For the large-scale deployment,
	I redesigned RichReview as a web applet in edX?s courseware framework.
	By deploying the system at scale, I will investigate (1) the optimal
	group assignment scheme that maximizes overall diversities of group
	members, (2) educational data mining applications based on user-generated
	rich discussion data, and (3) the impact of the rich discussion to
	students? retention of knowledge. Throughout these studies, I will
	argue that a multi-modal anchored digital document annotation system
	enables rich online peer discussion at scale.},
  acmid =     {2815591},
  doi =       {10.1145/2815585.2815591},
  file =      {Yoon2015a.pdf:Yoon2015a.pdf:PDF},
  isbn =      {978-1-4503-3780-9},
  keywords =  {instructor feedback, massive open online courses., multi-modal annotation, online education, peer discussion, rank2},
  numpages =  {4},
  review =    {RichReview. Plans to test upcoming system on edX including semantic
	speech editing.}
}

@InProceedings{Yoon2014,
  author =    {Yoon, Dongwook and Chen, Nicholas and Guimbreti\`{e}re, Fran\c{c}ois and Sellen, Abigail},
  title =     {RichReview: Blending Ink, Speech, and Gesture to Support Collaborative Document Review},
  booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
  year =      {2014},
  series =    {UIST '14},
  pages =     {481--490},
  address =   {Honolulu, Hawaii, USA},
  publisher = {ACM},
  acmid =     {2647390},
  doi =       {10.1145/2642918.2647390},
  file =      {Yoon2014.pdf:Yoon2014.pdf:PDF},
  isbn =      {978-1-4503-3069-5},
  keywords =  {annotation, asynchronous communication, collaborative authoring, multi-modal input, pen interaction, pointing gesture, speech, voice, rank5},
  numpages =  {10}
}

@InProceedings{Yoon2016,
  author =    {Yoon, Dongwook and Chen, Nicholas and Randles, Bernie and Cheatle, Amy and L\"{o}ckenhoff, Corinna E. and Jackson, Steven J. and Sellen, Abigail and Guimbreti\`{e}re, Fran\c{c}ois},
  title =     {RichReview++: Deployment of a Collaborative Multi-modal Annotation System for Instructor Feedback and Peer Discussion},
  booktitle = {Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work \& Social Computing},
  year =      {2016},
  series =    {CSCW '16},
  pages =     {195--205},
  address =   {San Francisco, California, USA},
  publisher = {ACM},
  acmid =     {2819951},
  doi =       {10.1145/2818048.2819951},
  file =      {Yoon2016.pdf:Yoon2016.pdf:PDF},
  isbn =      {978-1-4503-3592-8},
  keywords =  {Multi-modal annotation, anchored comment, collaborative annotation, instructor feedback, online peer-discussion., rank5},
  numpages =  {11}
}

@InProceedings{Yoon2015,
  author =    {Yoon, Dongwook and Mitros, Piotr},
  title =     {Multi-Modal Peer Discussion with RichReview on edX},
  booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software \& Technology},
  year =      {2015},
  series =    {UIST '15 Adjunct},
  pages =     {55--56},
  address =   {Daegu, Kyungpook, Republic of Korea},
  publisher = {ACM},
  abstract =  {In this demo, we present RichReview, a multi-modal peer discussion
	system, implemented as an XBlock in the edX courseware platform.
	The system brings richness similar to face-to-face communication
	into online learning at scale. With this demonstration, we discuss
	the system?s scalable back-end architecture, semantic voice editing
	user interface, and a future research plan for the profile based
	group-assignment scheme.},
  acmid =     {2817809},
  doi =       {10.1145/2815585.2817809},
  file =      {Yoon2015.pdf:Yoon2015.pdf:PDF},
  isbn =      {978-1-4503-3780-9},
  keywords =  {massive open online courses, multi-modal annotation, peer discussion, peer group assignment., voice user interface, rank2},
  numpages =  {2},
  review =    {Plans to include semantic speech editing in edX}
}

@Article{Yu2010,
  author =     {Yu, Zhiwen and Nakamura, Yuichi},
  title =      {Smart Meeting Systems: A Survey of State-of-the-art and Open Issues},
  journal =    {ACM Comput. Surv.},
  year =       {2010},
  volume =     {42},
  number =     {2},
  pages =      {8:1--8:20},
  month =      mar,
  acmid =      {1667065},
  address =    {New York, NY, USA},
  articleno =  {8},
  doi =        {10.1145/1667062.1667065},
  file =       {Yu2010.pdf:Yu2010.pdf:PDF},
  issn =       {0360-0300},
  issue_date = {February 2010},
  keywords =   {Smart meeting system, evaluation, meeting capture, meeting recognition, semantic processing, rank3},
  numpages =   {20},
  publisher =  {ACM}
}

@Article{Yuan2008,
  author =    {Yuan, Jiahong and Liberman, Mark},
  title =     {Speaker identification on the SCOTUS corpus},
  journal =   {Journal of the Acoustical Society of America},
  year =      {2008},
  volume =    {123},
  number =    {5},
  pages =     {3878},
  file =      {Yuan2008.pdf:Yuan2008.pdf:PDF},
  publisher = {[New York: Acoustical Society of America]},
  review =    {Paper for the Penn Phonetics Lab Forced Aligner (P2FA).}
}

@Article{Zangenehpour2010,
  author =   {Shahin Zangenehpour and Robert J. Zatorre},
  title =    {Crossmodal recruitment of primary visual cortex following brief exposure to bimodal audiovisual stimuli},
  journal =  {Neuropsychologia },
  year =     {2010},
  volume =   {48},
  number =   {2},
  pages =    {591 - 600},
  abstract = {Several lines of evidence suggest that exposure to only one component
	of typically audiovisual events can lead to crossmodal cortical activation.
	These effects are likely explained by long-term associations formed
	between the auditory and visual components of such events. It is
	not certain whether such crossmodal recruitment can occur in the
	absence of explicit conditioning, semantic factors, or long-term
	association; nor is it clear whether primary sensory cortices can
	be recruited in such paradigms. In the present study we tested the
	hypothesis that crossmodal cortical recruitment would occur even
	after a brief exposure to bimodal stimuli without semantic association.
	We used positron emission tomography, and an apparatus allowing presentation
	of spatially and temporally congruous audiovisual stimuli (noise
	bursts and light flashes). When presented with only the auditory
	or visual components of the bimodal stimuli, naïve subjects showed
	only modality-specific cortical activation, as expected. However,
	subjects who had previously been exposed to the audiovisual stimuli
	showed increased cerebral blood flow in the primary visual cortex
	when presented with sounds alone. Functional connectivity analysis
	suggested that the auditory cortex was the source of visual cortex
	activity. This crossmodal activation appears to be the result of
	implicit associations of the two stimuli, likely driven by their
	spatiotemporal characteristics; it was observed after a relatively
	short period of exposure (\textasciitilde 45 min), and lasted for
	a relatively long period after the initial exposure (\textasciitilde
	1 day). The findings indicate that auditory and visual cortices interact
	with one another to a larger degree than typically assumed.},
  doi =      {10.1016/j.neuropsychologia.2009.10.022},
  file =     {Zangenehpour2010.pdf:Zangenehpour2010.pdf:PDF},
  issn =     {0028-3932},
  keywords = {Crossmodal recruitment, rank2},
  review =   {Shows that cross-modal links exist in the brain using MRI scans.},
  status =   {printed, read}
}

@Book{Zhang2001,
  title =     {Content-based Audio Classification and Retrieval for Audiovisual Data Parsing},
  publisher = {Springer},
  year =      {2001},
  author =    {Tony Zhang and C. C. Jay Kuo},
  volume =    {606},
  doi =       {10.1007/978-1-4757-3339-6},
  file =      {Zhang2001.pdf:Zhang2001.pdf:PDF},
  isbn =      {978-1-4419-4878-6},
  keywords =  {SMD, rank4},
  owner =     {Chris},
  status =    {read},
  timestamp = {2013.11.25}
}

@InProceedings{Zue1986,
  author =    {Zue, V.W. and Lamel, L.},
  title =     {An expert spectrogram reader: A knowledge-based approach to speech recognition},
  booktitle = {Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
  year =      {1986},
  volume =    {11},
  pages =     {1197-1200},
  month =     apr,
  abstract =  {Human experts can determine the phonetic identity of unknown utterances
	from a visual examination of the spectrogram with performance better
	than available computer systems. The spectrogram-reading process
	involves the use of multiple sources of knowledge, including articulatory
	movements, acoustic phonetics, phonotactics, and linguistics. In
	addition, the experts' performance can be attributed to their ability
	to deal with partial and/or conflicting information, as well as multiple
	cues. This paper investigates the feasibility of constructing a knowledge-based
	system that mimics the process of spectrogram reading by humans.
	In a task of identifying stop consonants extracted from continuous
	speech, the system achieved performance that is comparable to that
	of the experts.},
  doi =       {10.1109/ICASSP.1986.1168798},
  file =      {Zue1986.pdf:Zue1986.pdf:PDF},
  keywords =  {multimodal, rank3}
}

@InProceedings{Zue1979,
  author =    {Victor W. Zue and Ronald A. Cole},
  title =     {Experiments on spectrogram reading},
  booktitle = {Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
  year =      {1979},
  doi =       {10.1109/ICASSP.1979.1170735},
  file =      {Zue1979.pdf:Zue1979.pdf:PDF},
  keywords =  {rank3},
  owner =     {chrisbau},
  timestamp = {2017.12.05}
}

@Standard{AES2008,
  title =        {AES31--3},
  organization = {Audio Engineering Society},
  year =         {2008},
  note =         {AES standard for network and file transfer of audio - Audio-file transfer and exchange - Part 3: Simple project interchange},
  file =         {AES2008.pdf:AES2008.pdf:PDF},
  owner =        {chrisbau},
  timestamp =    {2018.01.29}
}

@comment{jabref-meta: fileDirectory:/Users/chrisbau/Dropbox/PhD/refs;}

@comment{jabref-meta: fileDirectory-Chris:./;}

@comment{jabref-meta: fileDirectory-chrisbau:./;}

@comment{jabref-meta: fileDirectory-chrisbau-rd000727.rd.bbc.co.uk:/Users/chrisbau/Dropbox/PhD/refs;}

@comment{jabref-meta: groupsversion:3;}

@comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 ExplicitGroup:Chapter 1 (Intro)\;0\;;
1 ExplicitGroup:Chapter 2 (Background)\;0\;Bouamrane2007\;Dewey2014\;Dewey2016\;Dewey2016a\;Foote1999\;Schoeffmann2015\;Tucker2005\;Waibel2003\;Yu2010\;;
2 ExplicitGroup:Vision\;0\;Hubbard1996\;Koehler1929\;Levkowitz1991\;Maragos2008\;Marks2003\;Ramachandran2001\;Tsiros2013\;Tsiros2014\;Tufte2001\;Wolfe2004\;Zangenehpour2010\;;
2 ExplicitGroup:Radio production\;0\;Barbour2004\;Engstroem2010\;Hausman2012\;Kim2003\;McLeish2015\;Moss2015\;Perry2009\;Pras2013\;SCISYS2015\;Sampaio2016\;;
2 ExplicitGroup:Waveforms\;0\;Gohlke2010\;Loviscach2011\;Mason2007\;Massie1985\;Moreland2009\;Rice2005\;Sjolander2000\;Towsey2014\;Tzanetakis1999\;Tzanetakis2000\;;
2 ExplicitGroup:Spectrograms\;0\;Boogaart2006\;Gomez2005\;Goudeseune2012\;Lin2012\;Lin2013\;RAJAR2017\;Zue1986\;;
2 ExplicitGroup:Segmentation\;0\;Hendley2014\;Kacprzak2013\;Liang2005\;Lloret2012\;Panagiotakis2005\;Pikrakis2006\;Pikrakis2006a\;Pikrakis2008\;Popescu-Belis2006\;Raimond2014\;Scheirer1997\;Sell2014\;Tranter2006\;Wieser2014\;;
2 ExplicitGroup:Transcripts\;0\;Bell2015\;Berthouzoz2012\;Boas2011\;Burke2006\;Chandrasegaran2017\;Griggs2007\;Heeren2008\;Horner1993\;Hyperaudio2016\;Inkpen2004\;Kalnikaite2012\;Kato2015\;Kim2003\;Liang2014\;Long2003\;Loviscach2011a\;Masoodian2006\;Milota2015\;Nielsen1993\;Pavel2014\;Pavel2016\;Popescu-Belis2006\;RQDA\;Ranjan2006\;Robert-Ribes1997\;Rubin2013\;Rubin2015\;Shin2016\;Sivaraman2016\;Sjolander2000\;Suhm2001\;Truong2016\;Vemuri2004\;Wald2007\;Whittaker1999\;Whittaker2000\;Whittaker2002\;Whittaker2004\;Whittaker2007\;;
2 ExplicitGroup:Skimming\;0\;Christel2008\;Huerst2004\;Huerst2006\;Kobayashi1997\;Lee2006\;Lee2007\;Pavel2014\;Ranjan2006\;Tucker2003\;Tucker2006\;Vemuri2004\;;
2 ExplicitGroup:Haptics\;0\;Carter2013\;Metatla2016\;;
1 ExplicitGroup:Chapter 3 (Waveform)\;0\;Ericsson2009\;Hart1988\;Hart2006\;Kacprzak2013\;Lewis1993\;Saunders1996\;Williams1949\;;
1 ExplicitGroup:Chapter 4 (Ethnography)\;0\;SCISYS2015\;;
1 ExplicitGroup:Chapter 5 (Screen interface)\;0\;Bell2015\;Berthouzoz2012\;Boas2011\;Burke2006\;Chandrasegaran2017\;Hart1988\;Hart2006\;Hyperaudio2016\;Inkpen2004\;Kalnikaite2012\;Kato2015\;Liang2014\;Long2003\;Loviscach2011a\;Masoodian2006\;Milota2015\;Nielsen1993\;Pavel2016\;RQDA\;Rubin2013\;Rubin2015\;Shin2016\;Sivaraman2016\;Suhm2001\;Truong2016\;Wald2007\;Whittaker1999\;Whittaker2000\;Whittaker2002\;Whittaker2004\;Whittaker2007\;;
1 ExplicitGroup:Chapter 6 (Paper interface)\;0\;Brooke1996\;Cabral2016\;Cattelan2008\;Conroy2004\;Davis1989\;Dewey2016\;Diakopoulos2006\;Erol2007\;Erol2008\;Fahraeus2003\;Fouse2011\;Graham2003\;Guimbretiere2003\;Harper2001\;Hull2003\;Inkpen2004\;Kalnikaite2012\;Klemmer2003\;Li2005\;Mangen2013\;Marshall1997\;Mueller2014\;Nielsen1993\;Noyes2008\;OHara1997\;Olberding2010\;RQDA\;Schilit1998\;Signer2010\;Singer2017\;Stifelman2001\;Suhm2001\;Vemuri2004\;Weher1994\;Weibel2008\;Wilcox1997\;Yoon2014\;Yoon2015\;Yoon2015a\;Yoon2016\;;
}

