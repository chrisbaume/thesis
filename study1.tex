% !TeX root = main.tex

\chapter{Navigation of colourised waveforms}\label{sec:study1}

Previous work has mapped waveform colour to data, which is expected to assist
users in navigating the audio content more efficiently. However we could not
find any studies which tested the effectiveness of this approach.

Our hypothesis is that by mapping the colour of a waveform to a relevant variable,
we can complete tasks using the waveform with:
\begin{itemize}
  \item Reduce the time required
  \item Reduce the effort required
  \item Complete with same performance
\end{itemize}

Hypothesis: waveforms can be improved by linking colour to data

\section{Goals}

The aims of this experiment was to discover whether enhancing a waveform with
colour allows for faster navigation.

The primary objective of this initial experiment was to verify that conducting
a task-based evaluation of an audio interface online is viable and produces
significant results. It was designed to have a narrow scope and low requirement
for participants so that any problems could be identified without having to
involve large amounts of people.

The context of the experiment was to evaluate the performance of the most
common method of visualising audio -- the waveform -- and investigate whether
there is potential to improve upon it.

\section{Experimental design}

\subsection{Task}\label{sec:studytask}
The experiment was designed to simulate a typical task encountered by radio
producers. Creation of podcasts is a common but often tedious exercise for
producers which primarily involves finding each piece of music played in a
programme and editing them down to 30 seconds in length. This is in order to
comply with rights agreements for downloadable content.

Although it is possible for experienced producers to distinguish between music
and speech using a waveform, this representation doesn't scale very well so it
becomes very difficult when the display is zoomed-out. As such, finding music
at the programme level can be challenging.

The task for the experiment was to find and select the piece of music content
in a recording of a radio programme.

\subsection{Platform}
Conducting experiments online brings a range of benefits as they can be run in
parallel, taken at any time at any place, and are easily shared. Recent
advances in web technology standards (notably
HTML5\footnote{\url{http://www.w3.org/TR/html5/}}) mean that it is now
possible to control playback of audio on a wide range of browsers without requiring
additional plugins, making large scale evaluation achievable.

\subsection{Duration}
In order to attract a sufficient number of participants while collecting enough
data, the experiment was designed so it could be completed in 10--15 mins.

\subsection{Conditions}
Three different methods were used to visualize the audio: a normal waveform, a
task-enhanced waveform and a blank display (no waveform). The blank
visualization acts as a baseline where participants are operating `blind' with
no visual assistance. The enhanced waveform (see
Section~\ref{sec:studywaveform}) represents a nominal improvement using a
simple algorithm designed for the task.

The expected result is that having a waveform is better than having nothing,
and that an enhanced waveform performs better than a normal waveform.

\subsection{Test data}
The radio programme recordings were selected to represent the wide variety of
programme and music genres broadcast on the radio, as well as covering both
male and female speakers and a range of different radio stations.

The audio content was sourced from internal BBC recordings of transmission
(ROT) and were edited to be approximately five mins in length, and to have only
one section that could be categorised as music.

The programmes are described in Table~\ref{tab:clips}.

\begin{table}[htbp]
  \begin{center}
    {\small
    \begin{tabular}{|r|l|l|l|l|l|}
      \hline
      \multicolumn{1}{|l|}{\textbf{Clip}} & \textbf{Network} & \textbf{Title} &
      \textbf{Voices} & \textbf{Prog genre} & \textbf{Music genre} \\ \hline
      Train & Radio 4 & Desert Island Discs & F & Interview & Ambient \\ \hline
      1 & 1 Xtra & Sian Anderson & M, F & Breakfast & Dance \\ \hline
      2 & 6 Music & Lauren Laverne & F & Single & Indie \\ \hline
      3 & Radio 2 & Ken Bruce & M & Phone quiz & Lounge \\ \hline
      4 & Radio 3 & Breakfast show & M & Single & Classical \\ \hline
      5 & 5 Live & Sports report & M & Sports & Band \\ \hline
      6 & Radio 1 & Zane Lowe & M & Interview & Rap \\ \hline
      7 & Radio 2 & Jo Whiley & M, F & Review & Pop \\ \hline
      8 & Radio 4 & Afternoon drama & M, F & Drama & Classical \\ \hline
      9 & Radio 4 & Front Row & M & Interview & Alternative \\ \hline
    \end{tabular}
    }
  \end{center}
  \caption{Descriptions of the radio programmes used for the evaluation}
  \label{tab:clips}
\end{table}

\subsection{Metrics}\label{sec:metrics}
\paragraph{Demographics}
Participants were asked about their gender, age and a number of question which
attempted to gauge their level of experience. The questions and options are
listed below:

{\singlespacing
\begin{itemize}
  \item Do you understand what an audio waveform is? [Yes/No]
  \item Have you previously used any consumer audio editing software? (e.g.
    Audacity, GarageBand) [Yes/No]
  \item Have you previously used any professional audio editing software? (e.g.
    ProTools, Logic, Cubase/Nuendo, SADiE, Startrack) [Yes/No]
  \item How many years (if any) have you worked with audio in a professional
    capacity? [\textit{number}]
\end{itemize}
}

\paragraph{Performance metrics}
The interface was configured to log every action the user made. This was done
by recording time-stamps, so that not only can the number of actions be counted,
but the timing and frequency of the actions can also be measured.

The actions that were recorded are listed below:

{\singlespacing
\begin{itemize}
  \item Seek (top display)
  \item Seek (bottom display)
  \item Play/pause
  \item Select (using button)
  \item Select (using slider)
  \item Zoom in
  \item Zoom out
  \item Task completion
\end{itemize}
}

\paragraph{Task load}
After completing the tasks for a given condition, participants were asked to
rate the tasks using the NASA Task Load Index (NASA-TLX) \citep{Hart1988}
metrics on a scale of $-10$ to $+10$. These are listed below:

{\singlespacing
\begin{itemize}
  \item Mental Demand -- How mentally demanding was the task? [very low/very
    high]
  \item Physical Demand -- How physically demanding was the task? [very
    low/very high]
  \item Temporal Demand -- How hurried or rushed was the pace of the task?
    [very low/very high]
  \item Performance -- How successful were you in accomplishing what you were
    asked to do? [perfect/failure]
  \item Effort -- How hard did you have to work to accomplish your level of
    performance? [very low/very high]
  \item Frustration -- How insecure, discouraged, irritated, stressed, and
    annoyed were you? [very low/very high]
\end{itemize}
}

\paragraph{Comparison}
At the end of the experiment, participants were asked to compare the conditions
directly by selecting which they thought were the easiest and most frustrating.
A thumbnail image representing each visualisation was shown to remind the
participants of how they looked.

\paragraph{Other}
In addition to the above measures, the browser and operating system each
participant used was recorded, as was the date and time at which they
submitted each data point.

\subsection{Sequence}\label{sec:studysequence}
In order to make it possible for the experiment to completed in 10-15 mins, a
sequence of nine tasks was used. The conditions were grouped rather than mixed
(e.g. AAABBBCCC instead of ACACBABCB) in order to avoid confusion when
switching between them. The TLX questions were presented after each group of
conditions (i.e. AAATBBBTCCCT) and the comparison questions were presented at
the end.

Each audio clip can only be used once per participant, otherwise they would be
able to remember where the music was. An efficient way of presenting each clip
once is to use a Latin square. A Latin square is an $n \times n$ array with $n$
different symbols, with each symbol occurring exactly once in each row and
exactly once in each column. A Williams design Latin square \citep{Williams1949}
is one which is balanced for first order carryover effects.  When $n$ is odd,
to maintain balance two Latin squares must be used, producing a $2n \times n$
array.

For the sequence of 9 audio clips, an $18\times9$ Williams design Latin square
was used (see Table~\ref{tab:clipseq}), which was generated using the
\texttt{crossdes}
package\footnote{\url{http://cran.r-project.org/web/packages/crossdes/index.html}}
in R. The visualisation sequence was generated by modifying this design. First,
the middle three columns\footnote{It was determined empirically that the
  middle 3 columns were the optimum selection when used as described} were
extracted, and the values $4-6$ and $7-9$ were mapped to $1-3$. The resulting
$18\times3$ matrix was then expanded horizontally to produce a $18\times9$
matrix (see Table~\ref{tab:visseq}). When the two sequences are combined, the
result is balanced and has minimal carryover effects.

\begin{table}
  \parbox{.45\linewidth}{
    \centering
    \begin{tabular}{rrrrrrrrr}

      1 & 2 & 9 & 3 & 8 & 4 & 7 & 5 & 6 \\ 
      2 & 3 & 1 & 4 & 9 & 5 & 8 & 6 & 7 \\ 
      3 & 4 & 2 & 5 & 1 & 6 & 9 & 7 & 8 \\ 
      4 & 5 & 3 & 6 & 2 & 7 & 1 & 8 & 9 \\ 
      5 & 6 & 4 & 7 & 3 & 8 & 2 & 9 & 1 \\ 
      6 & 7 & 5 & 8 & 4 & 9 & 3 & 1 & 2 \\ 
      7 & 8 & 6 & 9 & 5 & 1 & 4 & 2 & 3 \\ 
      8 & 9 & 7 & 1 & 6 & 2 & 5 & 3 & 4 \\ 
      9 & 1 & 8 & 2 & 7 & 3 & 6 & 4 & 5 \\ 
      6 & 5 & 7 & 4 & 8 & 3 & 9 & 2 & 1 \\ 
      7 & 6 & 8 & 5 & 9 & 4 & 1 & 3 & 2 \\ 
      8 & 7 & 9 & 6 & 1 & 5 & 2 & 4 & 3 \\ 
      9 & 8 & 1 & 7 & 2 & 6 & 3 & 5 & 4 \\ 
      1 & 9 & 2 & 8 & 3 & 7 & 4 & 6 & 5 \\ 
      2 & 1 & 3 & 9 & 4 & 8 & 5 & 7 & 6 \\ 
      3 & 2 & 4 & 1 & 5 & 9 & 6 & 8 & 7 \\ 
      4 & 3 & 5 & 2 & 6 & 1 & 7 & 9 & 8 \\ 
      5 & 4 & 6 & 3 & 7 & 2 & 8 & 1 & 9 \\ 
    \end{tabular}
    \caption{Audio clip sequence -- each row is a test sequence}
    \label{tab:clipseq}
  }
  \hfill
  \parbox{.45\linewidth}{
    \centering
    \begin{tabular}{rrrrrrrrr}

      3 & 3 & 3 & 2 & 2 & 2 & 1 & 1 & 1 \\ 
      1 & 1 & 1 & 3 & 3 & 3 & 2 & 2 & 2 \\ 
      2 & 2 & 2 & 1 & 1 & 1 & 3 & 3 & 3 \\ 
      3 & 3 & 3 & 2 & 2 & 2 & 1 & 1 & 1 \\ 
      1 & 1 & 1 & 3 & 3 & 3 & 2 & 2 & 2 \\ 
      2 & 2 & 2 & 1 & 1 & 1 & 3 & 3 & 3 \\ 
      3 & 3 & 3 & 2 & 2 & 2 & 1 & 1 & 1 \\ 
      1 & 1 & 1 & 3 & 3 & 3 & 2 & 2 & 2 \\ 
      2 & 2 & 2 & 1 & 1 & 1 & 3 & 3 & 3 \\ 
      1 & 1 & 1 & 2 & 2 & 2 & 3 & 3 & 3 \\ 
      2 & 2 & 2 & 3 & 3 & 3 & 1 & 1 & 1 \\ 
      3 & 3 & 3 & 1 & 1 & 1 & 2 & 2 & 2 \\ 
      1 & 1 & 1 & 2 & 2 & 2 & 3 & 3 & 3 \\ 
      2 & 2 & 2 & 3 & 3 & 3 & 1 & 1 & 1 \\ 
      3 & 3 & 3 & 1 & 1 & 1 & 2 & 2 & 2 \\ 
      1 & 1 & 1 & 2 & 2 & 2 & 3 & 3 & 3 \\ 
      2 & 2 & 2 & 3 & 3 & 3 & 1 & 1 & 1 \\ 
      3 & 3 & 3 & 1 & 1 & 1 & 2 & 2 & 2 \\ 
    \end{tabular}
    \caption{Visualization sequence -- each row is a test sequence}
    \label{tab:visseq}
  }
\end{table}

\section{Method}

\section{Interface design}
An online audio interface and evaluation system was developed, which is
described in detail in Section \ref{sec:iface}. It displays two audio
visualisations -- one shows the full recording, and the other shows a zoomed-in
view. Basic functionality was implemented, including seek, zoom, play/pause and
selection.

\subsection{Enhanced waveform}\label{sec:studywaveform}
The enhanced waveform was created by modifying the colour of a standard
waveform, based on a very simple speech/music discrimination (SMD) feature.

As shown in Section~\ref{sec:litreview}, low energy ratio is a popular, simple
and effective scalar metric for speech/music discrimination. It works on the
principle that speech has intermittent silences (between words) whereas music
does not. It was calculated by extracting the RMS energy (20ms frames, no
overlap) and counting the proportion of frames which fall below a threshold.
In the case of this experiment, the threshold was configured as the third
percentile of RMS energy in a one second sliding window. These parameters were
set empirically by testing them against radio programme recordings.

The low energy ratio was mapped to a colour gradient which was blue for low
values (representing speech) and to pink for high values (representing music).
The shade of blue was chosen to match that used in StarTrack and the pink is
it's inverse colour.

\subsection{Promotion}\label{sec:promo}
The experiment was promoted using two email lists which covered all staff in
BBC Research and Development (approx. 150 people) and all residents in the
Electronics and Computer Science department at Queen Mary University of London
(approx. 300 people). Although there are a large number of audio specialists
working in both groups, the recipients are mostly non-experts.

\subsection{Ethics}
Ethics approval for the experiment was gained from the QMUL Research Ethics
Committee and is logged under the reference \texttt{QMREC1348d}. Due to the
low-risk nature of the study, this was done through expedited review. The
consent form presented to participants in the study is reprinted in
Appendix~\ref{app:consent}.

\section{Results}
63 responses were completed in the three weeks the experiment ran. Emails
linking to the experiment were sent to roughly 450 people, which gives a
conversion rate of 14\%. This is much higher than was expected.  Informal
feedback suggested that many people enjoyed participating due to the listening
and task-based nature of the experiment. 

\subsection{Validation}
In order to ensure that participants were following the instructions correctly,
they were filtered using an acceptance criteria (see Figure~\ref{eq:accept}).
It stipulates that any participant who submits a response with an absolute
error of more than 5 seconds is rejected.

\begin{figure}[ht]
  \begin{center}
    $ |t_{in}-t_{inREF}| + |t_{out}-t_{outREF}| \leq 5 $
  \end{center}
  \caption{Acceptance criteria, where $t_{in}$ and $t_{out}$ are the in and out
    points of each selection, and $t_{inREF}$ and $t_{outREF}$ are the ground
    truth in and out points}
  \label{eq:accept}
\end{figure}

Of the 63 experiments completed, only 41 passed the above criteria meaning that
35\% of participants were rejected. Figure~\ref{fig:rejectdaw} shows that none
of the rejected participants had experience of using a professional audio
editor, suggesting that the mistakes may have been made due to inexperience
with audio interfaces.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.5\textwidth]{figs/reject-daw.pdf}
  \caption{Response of rejected participants when asked whether they had
    previously used consumer/professional audio editing software}
  \label{fig:rejectdaw}
\end{figure}

Figure~\ref{fig:rejectvis} plots the number of incorrect responses received for
each visualisation, which shows that no particular visualisation is responsible
for the mistakes. However, Figure~\ref{fig:rejectclip} shows that there is
variation in the mistakes made for each clip, particularly clips 4 and 5. An
analysis of the incorrect in and out points for these clips found that
the mistakes were varied and don't suggest a systematic problem. 

\begin{figure}[ht]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figs/rejects-vis.pdf}
  \caption{By visualisation}
  \label{fig:rejectvis}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figs/rejects-clip.pdf}
  \caption{By clip}
  \label{fig:rejectclip}
\end{subfigure}
\caption{Analysis of incorrect responses}
\label{fig:rejects}
\end{figure}

\subsection{Demographics}
The demographic of the participants showed a heavy bias (80\%) of male
participants, and a larger proportion in the 26-45 age range (see
Figure~\ref{fig:age}). This reflects the population to which the experiment was
promoted (see Section~\ref{sec:promo}) and is not expected to skew the results.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.5\textwidth]{figs/age.pdf}
  \caption{Age and gender of participants. Male/female ratio was 32/8
    (80\%/20\%). One participant declined to respond to the question.}
  \label{fig:age}
\end{figure}

Most participants had previous experience of using both consumer and
professional audio editing software (see Figure~\ref{fig:experiencedaw}), with
29\% of participants only having experience of consumer software or no
experience at all.

When asked about years of professional audio experience, 39\% of participants
reported having no experience, with the remainder being spread out up to a
maximum of 25 years (see Figure~\ref{fig:experienceyears}). Interestingly, the
answer people gave peaks at the 5 and 10--year marks, where people may have
given a rounded number rather than an accurate figure.

\begin{figure}[ht]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figs/daw.pdf}
  \caption{Previous use of audio editing software}
  \label{fig:experiencedaw}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figs/experience.pdf}
  \caption{Years of professional audio experience}
  \label{fig:experienceyears}
\end{subfigure}
\caption{Response of participants to questions about experience}
\label{fig:experience}
\end{figure}

\subsection{Analysis methods}
This section describes the various tools that were used to analyse the data
collected using the experiment, find differences between conditions and
determine whether they are of significance.

\paragraph{Box plot}
A box plot \citep{McGill1978} is a technique used to graph distributions by 
their quartile values (i.e. 25th, 50th and 75th percentiles). An example can be
seen in Figure~\ref{fig:seekbox}. The box extends from the first to the third
quartile, with the second quartile (median) marked as a line through the box.
Lines are drawn from the box to the minimum and maximum, known as `whiskers',
however data determined to be outliers are marked separately as crosses.
The 95\% confidence interval of the median is marked as a notch in the box.

\paragraph{ANOVA}
Analysis of variance is a method of testing whether the mean values of several
groups are equal or not. It assumes that the observations are independent, that
the data have a normal distribution, and that the variance within the groups
are similar. One-way ANOVA tests for a null hypothesis that the means values of
the factors are the same.

\paragraph{Tukey's test}
If the null hypothesis is rejected using ANOVA, we know that there is a
difference between the factors, but we don't know which ones. Tukey's test is a
post-hoc analysis for discovering the difference between individual factors.
It assumes that the observations are independent and that the variance within
the groups are similar.

When Tukey's test is graphed, the mean values are represented by a dot with a
line either side showing the confidence interval. Confidence intervals which
don't overlap can be said to be significantly different.

\paragraph{Standardisation}
Some observations can be biased through participant behaviour. For example,
person A navigates audio recordings by quickly clicking along the timeline
while person B navigates with only a few considered clicks. To block this
factor, the responses of each participant can be standardised so that they have
a mean value of 0 and a standard deviation of 1. This allows the difference
between different participants responses to be measured fairly.

Standardisation maps observations to the `\textbf{standard score}'. This is a
dimensionless unit which represents the number of standard deviations an
observation is above the mean.

\subsection{Performance metrics}\label{sec:studymetrics}
An analysis was carried out on the performance metrics that were measured while
participants used the interface (see Section~\ref{sec:metrics}).

\paragraph{Seek action}
The number of seek actions made was standardised for each participant. This was
done to reduce variation due to different search styles (i.e. frequent aimless
seeking along the timeline vs. infrequent purposeful seeking)

ANOVA found there to be a difference between the visualizations for $p < 0.01$
and Tukey's HSD test (see Figure~\ref{fig:seektukey}) showed that the number of
seek actions are different for each visualization in favour of the enhanced
version, again for $p < 0.01$. Even without standardisation, the same result
holds for $p < 0.05$.

\begin{figure}[ht]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figs/seek-std.pdf}
  \caption{Box plot}
  \label{fig:seekbox}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figs/seek-std-tukey99.pdf}
  \caption{Tukey's test (99\% confidence interval)}
  \label{fig:seektukey}
\end{subfigure}
\caption{Number of seek actions, standardised per participant}
\label{fig:seek}
\end{figure}

\paragraph{Selection time}
The time taken to make a selection was calculated as the difference between the
time the play button was first pressed and the time the final selection was
made. This reduces variation due to participants not starting the task
immediately and participants who made a selection, then checked the rest of the
recording for other pieces of music. The mean and standard deviation of the
selection time was standardised for each participant to reduce the effect of
people's natural pace.

\begin{figure}[ht]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figs/playstart-to-selectend-std.pdf}
  \caption{Box plot}
  \label{fig:selecttimebox}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figs/playstart-to-selectend-std-tukey95.pdf}
  \caption{Tukey's test (95\% confidence interval)}
  \label{fig:selecttimetukey}
\end{subfigure}
\caption{Selection time (time between start of playback and final selection),
    standardised per participant}
\label{fig:selecttime}
\end{figure}

ANOVA showed that there was a significant difference between visualizations for
$p < 0.05$. Tukey's test (see Figure~\ref{fig:selecttimetukey}) found that
there was a difference between no visualization and the enhanced version, but
not between either of them and the normal waveform.

\paragraph{Error}
The absolute error of the selections made by each participant were calculated
for the inpoint, outpoint and sum of both. ANOVA found there to be no
difference between the visualisations for the inpoint or the sum, but found a
significant difference ($p < 0.05$) for the outpoint.  Tukey's test (see
Figure~\ref{fig:outpointerrtukey}) found that the absolute error of the
outpoint for the enhanced visualization was significantly lower than the other
two visualizations, but that the other two were no different.

This is an unexpected result which is difficult to explain, as the inpoint and
sum errors were not even close to rejecting the null hypothesis ($p > 0.3$).
Although the result is significant, the improvement in accuracy is only about
150ms which is small in the context of 5-minute recordings.

\begin{figure}[ht]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figs/outpoint-abserr.pdf}
  \caption{Box plot}
  \label{fig:outpointerrbox}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figs/outpoint-abserr-tukey95.pdf}
  \caption{Tukey's test (95\% confidence interval)}
  \label{fig:outpointerrtukey}
\end{subfigure}
\caption{Absolute error for the outpoint of selections}
\label{fig:outpointerr}
\end{figure}

\paragraph{Zoom}
The experimental interface included two displays -- one overview display
covering the length of the recording, and a zoomed display which showed a
magnified part of that. The number of times the zoom in and zoom out buttons
were pressed was logged. The sum of these values for each visualization is
shown in Figure~\ref{fig:zoomtotal}. By subtracting zoom out from zoom in, we
can infer what the final zoom level was when the response was submitted. The
final zoom levels for each visualization are shown in
Figure~\ref{fig:zoomfinal}.

\begin{figure}[h!]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figs/zoomtotal.pdf}
  \caption{Total zoom actions}
  \label{fig:zoomtotal}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figs/zoomfinal.pdf}
  \caption{Final zoom level}
  \label{fig:zoomfinal}
\end{subfigure}
\caption{Analysis of zoom usage}
\label{fig:zoom}
\end{figure}

Most of the time, participants didn't touch the zoom controls and opted to leave
it on the default x5 zoom level, which displays roughly one minute of audio
across 1045 pixels ($\sim$50ms per pixel). Other than the x5 level, x15 was
more popular than x10 for all visualizations and x20 was barely used at all.

\subsection{Comparison}
Participants were asked to directly compare the three visualizations at the end
of the experiment. The results are shown in Figure~\ref{fig:compare}.

\begin{figure}[ht]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figs/easiest.pdf}
  \caption{Easiest to use}
  \label{fig:easiest}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figs/frustrating.pdf}
  \caption{Most frustrating}
  \label{fig:frustrating}
\end{subfigure}
\caption{Response of participants when asked to compare the visualisations
  using different criteria}
\label{fig:compare}
\end{figure}

A clear majority of 76\% thought that the enhanced waveform was the easiest to
use, with the normal waveform receiving 17\% of votes and 7\% for no waveform.
The strong preference for the enhanced waveform over the others shows that
participants thought the additional information added using colour made their
task easier.

Having no waveform was considered by two-thirds of participants to be the most
frustrating condition, followed by the normal waveform at 20\% and enhanced
waveform at 15\%. Although this is another strong result in favour of using
waveforms, the results are not quite as strong as the ones on ease of
use. It is possible that the false positives and negatives present in the
enhanced waveform caused some people to select it as the most frustrating.

\subsection{Task load index}
The TLX responses were standardised to account for variation in the way
different people assign scores. The distributions of the standardised scores
for each visualization are shown in Figure~\ref{fig:tlx}.

\begin{figure}[p]
  \centering
  \includegraphics[width=\textwidth]{figs/tlx-std.pdf}
  \caption{Standard score of NASA Task Load Index responses for each
    visualization, standardised per participant.}
  \label{fig:tlx}
\end{figure}

\begin{figure}[p]
  \centering
  \includegraphics[width=\textwidth]{figs/tlx-std-tukey95.pdf}
  \caption{Standard score of NASA Task Load Index responses for each
    visualization, standardised per participant.}
  \label{fig:tlxtukey}
\end{figure}

ANOVA found there to be significant differences between the visualizations in
all TLX metrics for $p < 0.01$. Tukey's test (see Figure~\ref{fig:tlxtukey})
found that for $p < 0.05$, the enhanced waveform outperformed no waveform in
all metrics except temporal demand, and outperformed the normal waveform in
all metrics except frustration. The normal waveform outperformed no waveform in
mental demand, performance, effort and frustration.

Some scepticism should be given to the physical and temporal demand metrics, as
it's not entirely clear what is meant by that terminology in the context of the
task. Participants are not working against the clock, not are they doing
anything other than moving the mouse. Although some participants may consider
fewer clicks of the mouse to represent reduced physical demand, we cannot
assume that others thought the same.

\subsection{Interaction behaviour}
This section looks at how participants used the features available in the
online audio interface, full described in Section~\ref{sec:iface}.
%Informal observation of some participants as they completed the experiment
%revealed that some people used the interface in different ways. For example,
%when one participant had completed their selection, they scanned through the
%remaining unselected content to check that there were no other pieces of music.

\begin{figure}[p]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figs/top-v-bot-pref.pdf}
  \caption{Display}
  \label{fig:displaypref}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figs/mark-v-slide-pref.pdf}
  \caption{Selection method}
  \label{fig:selectpref}
\end{subfigure}
\caption{Preference of participants}
\label{fig:pref}
\end{figure}

\paragraph{Top/bottom display}
There are two displays that make up the interface -- a zoomed display at the
top and an overview display on the bottom. Either of the displays can be used
to navigate the recording and make selections.

For each participant, the number of tasks where they used the zoom display more
than the overview (and vice-versa) were counted. Figure~\ref{fig:displaypref}
shows number of participants who, on average, used the zoomed or overview
display more.

The results show a clear preference for using the overview display more than
the zoom display. Coupled with the results of zoom usage in
Section~\ref{sec:studymetrics}, we can see that for this task most people opted
just to work in the overview display, despite only having a resolution of
roughly 300ms per pixel.

An analysis of the overview and zoom display's usage between different
visualization methods did not find any notable difference.

\paragraph{Selection method}
The design of the interface gave participants two methods of making a selection
-- buttons to mark in and out points using the cursor, and a slider where the
in and out points could be dragged around. The cursor can be moved around in
both the overview and zoomed displays, whereas the slider is only available at
the x1 zoom level. This makes the buttons more useful for fine edits, where
high precision is needed. When using the slider, the selection display updates
as you move it making it easier for people to see where their selection is.

For each response, the method with the most actions was found and these
preferences were summed for each participant to calculate their overall
preference. The results are shown in Figure~\ref{fig:selectpref}.

The buttons were the most popular method of making a selection, with only 17\%
of people using the slider more. Informal feedback found that some people has a
strong preference for using the slider but were frustrated that there wasn't a
second slider available on the zoomed display.

\section{Technology development}\label{sec:tech}

\section{Audio visualisation software}\label{sec:vis}
The direction of the research in this project centres around turning audio
data into image data. There is already some software that visualizes audio and
audio features in a number of ways, notably Sonic Visualiser \citep{Cannam2010}.
However, the visualization algorithms are always hard-coded into the program,
making prototyping of new methods prohibitively difficult.

\subsection{Design}
A system of software was developed for visualizing audio in order to allow
flexibility while maintaining consist inputs and outputs. It was
designed to expand on existing systems for audio analysis so to avoid
duplication of effort and to allow modularisation of algorithms.

The system was developed as a plugin framework with clearly defined inputs and
outputs. It is based on the existing Vamp plugin framework, developed by Queen
Mary University of London \citep{Cannam2010}, which analyses audio data and
outputs frame-- or time-based features. The visualization plugin is designed to
take these features and produce a bitmap image. An outline of the system design
is shown in Figure~\ref{fig:vampeyer}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{figs/vampeyer.png}
  \caption{High-level system diagram of the Vampeyer visualization framework}
  \label{fig:vampeyer}
\end{figure}

The visualization plugin defines which Vamp plugin outputs it requires,
including the block/step size and parameters. At least one Vamp plugin is
required, but there is no restriction of the number of different Vamp plugins
that can be used.

Both frameworks are written in C++ which allows for a fast processing time.
This is important for situations where audio has to be processed on-the-fly,
such as for exploring an archive without having to pre-process everything in
it.  The plugins are compiled into shared libraries, so they can easily be
distributed without users having to recompile locally and integrated into
third-party software.

\subsection{Implementation}
A C++ header file was created which implements the design.  Five data
structures are also defined which define how data should be sent and returned
from the functions.

{\singlespacing
\begin{itemize}
  \item \texttt{VampParameter} is a name/value pair used to store a parameter
  \item \texttt{VampParameterList} is a vector of \texttt{VampParameter}s
  \item \texttt{VampPlugin} stores the name of a Vamp plugin along with a\\
    \texttt{VampParameterList} and the preferred block and step sizes
  \item \texttt{VampOutput} stores a \texttt{VampPlugin} and output name
  \item \texttt{VampOutputList} is a vector of \texttt{VampOutput}s
\end{itemize}
}

Two primary functions are used to define the input audio features and their
conversion to a bitmap.

\begin{itemize}
  \item \texttt{getVampPlugins} returns a \texttt{VampOutputList} variable
    that contains a list of Vamp plugin outputs which must be provided as
    input
  \item \texttt{ARGB} takes the Vamp plugin output data as a
    \texttt{Vamp::FeatureSet} variable, the sample rate of the audio and the
    desired width and height. It returns a bitmap image formatted in 32-bit
    ARGB format (alpha, red, green, blue).
\end{itemize}

A number of example plugins were written to demonstrate the capabilities of
this approach, and to act as useful pieces of software in their own right.
These include a standard waveform, waveform colourised by low energy ratio (see
Section~\ref{sec:studywaveform}), waveform colourised by spectral centroid
(identical to Freesound) and an MFCC grid visualization with waveform overlay.

The plugins need to be run by a host program which reads the audio data,
processes it using the Vamp plugins, sends their output to the visualization
plugin and then writes the image data. Such a program was written as a
command-line tool that can either display the image in a window or write it to
disk as a PNG-encoded file. This makes it useful for both prototyping and for
back-end processing on a server, for example.

\section{Browser-based audio interface}\label{sec:iface}
An audio interface was developed for use in evaluating a number of different
audio visualizations (see Section~\ref{sec:study}). In order to maximise the
number of people who could participate in the experiment, it was designed to be
accessible online and run in a web browser.

The latest web standards (such as HTML5) were exploited in order to ensure a
smooth and consistent user experience. This meant that some people running
older browsers were unable to take part, but the standards are mature enough
that the vast majority of browsers are compatible.

\subsection{Design}\label{sec:interfacedesign}
The screenshot of the  final design of the interface is shown in
Figure~\ref{fig:interface}, which includes of one the training pop-ups.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{figs/interface.png}
  \caption{Experimental test interface with training pop-up}
  \label{fig:interface}
\end{figure}

\paragraph{Motivation}
The goal for the design was to include all the essential features required to
complete the task of finding and selecting a segment of audio (see
Section~\ref{sec:studytask}) in a way that can easily be picked up by novice
participants. Only the essential features were considered in order to make it
simple and to avoid distraction.

\paragraph{Displays}
Two displays were used -- one to show an overview of the entire recording, and
another showing an adjustable zoom display. It would be possible to complete
the task using only an adjustable zoom display, but it would be very difficult
to keep track of the position in the recording.

The zoom display was placed above the overview display and set to be slightly
larger than the overview in order to emphasise that the display is a
magnification. The two displays were linked using two curvy lines (splines)
which show the position of the zoom display limits in the overview display.

The current position of the audio playback was signified by adding a `cursor'
using a line on each display. The cursor can be moved by clicking or dragging
on either of the displays using the mouse.

Selected areas of audio are highlighted using a semi-transparent overlay which
appears on both displays.  A time `ruler' was also overlaid on both displays to
give users an objective scale and a sense of the relative scale of the
displays.

\paragraph{Zoom control}
The level of zoom was controlled using two buttons at the bottom -- one to zoom
in and another to zoom out. In an early prototype, users could press these
buttons as many times as they wished, but this caused confusion when maximum
zoom was reached and the button stopped doing anything. Following feedback,
this was rectified by disabling and greying out the buttons when at max/min
zoom.

\paragraph{Selection}
Two methods of selecting content were implemented -- one using buttons and
another using a slider. The buttons were used to set either the in or out point
of the selection to the current position of the cursor. The buttons could be
used while the audio is playing, allowing users to press a button as they hear
their preferred start/end point. As the cursor can be moved on the zoomable
display, the buttons can be used to set the selection precisely.

The slider acts both as a display of where the selection is and as a method of
setting or adjusting it. The slider was located just under the overview display
and has `handles' on each side which can be dragged to adjust the start and end
of the selection. After receiving feedback on a prototype, the slider was
configured to update the highlight in the displays as it is dragged, making
fine adjustments easier to see.

\paragraph{Training}
Although the interface is relatively basic as far as digital audio workstations
are concerned, a system was developed to train users in the features and
operation of the interface. This was done using a `pop-up tour', which is
a technique often found on web pages. It involves a series of dialog boxes
which point out different parts of the interface with some text explaining what
they do. You can see an example in Figure~\ref{fig:interface}.

\subsection{Implementation}

\paragraph{Front-end}
A feature-heavy web page, such one that includes an audio/visual interface, can
be slow to load and unresponsive. To avoid this, the interface was set up as a
`single-page application' which loads the content of each page dynamically
without having to reload the interface and the large Javascript libraries that
it's built on.

Angular.JS\footnote{\url{https://angularjs.org}} is a Javascript web
application framework which aids development of dynamic web pages. A
single-page application was implemented using its `route' and `view' features.
Its other features were used to send responses to the database and update the
cursor/selection on the visualization, amongst other things.

Bootstrap\footnote{\url{http://getbootstrap.com/}} was used to style the page
and the components in it. The popularity of Bootstrap means that this style is
familiar and comfortable to most web users. Bootstrap
Tour\footnote{\url{http://bootstraptour.com/}} is a Javascript library for
running `pop-up tours' on websites. This was used to implement the training
interface, as discussed in Section~\ref{sec:interfacedesign}.

\paragraph{Back-end}
A web server and database were used to serve the web pages and experimental
data, and to store the results. Node.js\footnote{\url{http://nodejs.org/}} was
used to implement and expose an API for interacting with the front-end and
CouchDB\footnote{\url{http://couchdb.apache.org/}} was used as the database.

The sequence for each experiment (described in Section~\ref{sec:studysequence})
needed to be assigned to participants evenly, so that each sequence is
completed exactly once before a second participant is assigned to it. This was
done by taking the sequences that had been completed the least number of times,
then choosing the sequence that had been assigned the longest time ago (or not
at all). This approach minimised the chance of more than one participant using
the same sequence. 

\paragraph{Audio playback}
Different browsers support a variety of different audio codecs and have
different capabilities in terms of playback methods. In order to maximise
compatibility, SoundJS\footnote{\url{http://www.createjs.com/\#!/SoundJS}} was
used to play the audio content.

The audio content was data compressed using a lossy codec to reduce the size
(and therefore loading time) of each web page. Ogg Vorbis was used as the
primary codec, with a quality setting of 2 (approx. 80kbps VBR). For browsers
that didn't support Ogg Vorbis, AAC was used with a variable bitrate of 96kbps.
The use of lossy codecs introduces some audible artefacts. However this is not
part of the experiment and it impacts every audio clip, so is not expected to
bias the results. 

\paragraph{Visualization}
The duration of each of the audio clips used in the experiment is about 5
minutes, and the interface display allows for four levels of zoom. This
produces up to $\sim$2MB of image data which, if loaded at the beginning, would
negatively impact on the loading time of the web page. To fix this, a tiling
system was used where each image was cut into tiles 240 pixels wide. The
displays then dynamically loaded the tiles as and when they were needed.

To ensure maximum browser compatibility and to simplify the implementation,
KineticJS\footnote{\url{http://kineticjs.com/}} was used to render the images,
ruler and cursor.

