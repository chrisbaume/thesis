\chapter{Conclusions and further work}\label{sec:conclusions}

The aim of this research was to ``improve radio production by developing and evaluating methods for interacting with,
and manipulating, recorded audio'' (Section~\ref{sec:aim}).  We focused our research on pre-production of speech
content by professional radio producers to make the most of the access available to us from working within the BBC.  In
fulfilment of our aim, the primary contribution of this thesis has been the development and evaluation of three methods
for editing speech recordings through audio visualization, semantic speech editing and a digital pen interface.  We
developed these methods based on genuine requirements gathered from radio producers and evaluated them in the workplace
to ensure that our methods and results were relevant to real-life application.

To conclude this thesis, we first discuss our approach, results and contributions in
Section~\ref{sec:conclusions-discussion}, where we also reflect upon some of the tensions we observed between reading
transcripts and listening, and between paper and screen interfaces.  In Section~\ref{sec:conclusions-further}, we
describe potential options for further work, including follow-up research resulting from our studies, as well as some
broader applications of semantic audio production tools.  Finally, in Section~\ref{sec:conclusions-conclusions} we
summarise the novelties and achievements of this work, and answer our research questions.

\section{Discussion}\label{sec:conclusions-discussion}

We began our research by conducting three ethnographic case studies in BBC Radio to learn more about real-life
radio production practice.  We used the results to develop theoretical models of production workflows for a news
bulletin, drama and documentary.  We developed these based on direct observation of actual practice, which gave us 
insights into the genuine processes and challenges of radio production.  In addition to the workplace studies in
Chapter~\ref{sec:ethno}, we deepened our understanding of existing production workflows through interviews with twelve
radio producers as part of the user studies in Chapters~\ref{sec:screen} and \ref{sec:paper}.  These models and
insights contribute to the academic understanding of radio production practice.  The results of this ethnographic work
highlighted three directions for research involving audio visualization, textual representation of speech, and the use
of paper.  We then investigated each of these through technical intervention.

\subsection{Semantic audio visualization}

Our initial investigation looked at using pseudocolour to visualize a semantic audio feature to support audio editing
using waveforms.  We measured the user performance for a simple editing task using our semantic audio visualization,
compared to a normal waveform.  The results showed that when using the semantic audio waveform, the participants
completed the task faster, with less effort and with greater accuracy than the normal waveform. This suggests that
there is value in the pseudocolour approach to semantic audio visualization taken by \citet{Rice2005},
\citet{Akkermans2011} and \citet{Loviscach2011a}, which had previously been untested.

Our experiment only focused on a single task of segmenting music from speech so there are many opportunities for
applications beyond the task we chose. Audio visualizations can either be designed for specific tasks, or for
general use to cover a range of tasks. Visualizations that focus on an individual task may produce better results, but
are only suitable for that task.  Designing a general audio visualization is much more challenging, but has the
potential to create a greater impact as it could be used for a variety of applications.

The semantic audio visualization we designed and tested used a rudimentary semantic audio feature rather than the
state-of-the-art.  We did not attempt to create the best possible visualization as we wanted there to be an element of
human judgement in the measured task. There is potential to make better visualizations by including more and
better semantic audio features, and using more advanced methods of visualization. For example, the false colour
approach taken by \citet{Tzanetakis2000} and \citet{Mason2007} allowed multiple features to be displayed simultaneously
in a human-readable way.

Although we found that users required less effort to complete the task with a normal waveform than without, they did
not complete the task significantly faster nor more accurately. We found this surprising as waveforms are widely used
in audio editing software, so it was expected that waveforms would improve the performance of users in completing audio
editing tasks.  This poor performance is concerning as the widespread use of audio waveforms means that this affects a
large community.  However, this finding highlights an opportunity to increase the efficiency of audio production
software by making improvements to the waveform visualization.

\subsection{Semantic speech editing}

We conducted two user studies in which semantic speech editing was successfully used by professional radio producers to
create real radio programmes that were subsequently broadcast. Our results support previous work in finding that
semantic speech interfaces help users navigate and edit speech \citep{Whittaker2002}, and that semantic editing is
faster than, and preferable to, using audio waveforms \citep{Whittaker2004,Sivaraman2016}. Our research went further by
conducting user evaluations in a natural working environment, and directly comparing semantic speech editing to the
existing editing workflow. This allowed us to gain insights into its limitations in the context of radio production.

Our semantic speech editor was similar to the system described by \citet{Rubin2013}. However, rather than using
verbatim transcripts, which are slow and expensive to produce, we used automatic speech recognition (ASR). The high
speed and low cost of ASR make it better suited for use in broadcast environments, but the erroneous transcripts it
produces affect the usability of semantic editing.  Crucially, we discovered that the quality of modern ASR systems was
sufficient to allow for semantic speech editing in radio production.  This aligns with similar findings for the
semantic editing of voice messages \citep{Whittaker2004,Sivaraman2016}.  Our studies also revealed that the accuracy of
the transcript affected the need for correction, reading speed, reliance on listening, longevity of the transcript and
the edit granularity.

The producers we tested reported that they only found semantic editing useful for creating a ``rough edit'', where
large segments of audio are selected from the original material. This contradicts findings by \citet{Sivaraman2016} for
the editing of voice messages.
We found that the main reason for this limitation was the lack of annotation and re-ordering features, which made it
difficult for producers to organise, structure and arrange their material in the later stages.  However, this
limitation was not an obstruction to the producers we tested, as our semantic speech editing tools integrated with
several digital audio workstations (DAWs), which allowed the producers to seamlessly transition to their normal tools
to complete their production.



We tested a variety of additional features to support semantic editing, including transcript correction and confidence
shading.
We found that many of the transcript errors encountered were specific to the
programme content, such as names and topic-specific words.  However, most producers were only interested in correcting
errors that were particularly distracting.  Most producers we tested found confidence shading to be useful overall,
which supports \citet{Burke2006}, but contradicts \citet{Suhm2001} and \citet{Vemuri2004}.






DAWs are powerful, feature-rich tools, but to novice users they can be intimidating to work with.
\citet{Yoon2014,Sivaraman2016} found that semantic speech editing tools are easier to use and more accessible than
waveform-based tools.  Podcasting has already democratised the distribution of audio content, and semantic speech
editing may have the potential to achieve something similar for audio production. This would allow more people to
access audio production, which could hopefully lead to the production of more and better audio content.

Alternative methods of interacting with audio content may also give rise to new creative opportunities for more
innovative programme making.  For example, the comedy duo known as ``Cassetteboy'' \citep{Perraudin2014} patiently
trawl through hours of video footage to re-order the words of famous speeches in amusing ways. This could be achieved
much more easily using a semantic speech editor.


The efficiency savings from using semantic speech editing tools and ASR could provide cost and time
savings over traditional editing and manual transcription.  This should free up time and budget for more valuable
production activities, which may lead to improvements in the quality of programmes and/or reduction in the cost of
production.


\subsection{Reading vs listening}

Transcripts display the words that were spoken in a recording, but much of the content's true meaning is hidden in the
audio. Although transcripts show \textit{what} was said, they do not reveal \textit{how} it was said, which is crucial
when producing radio programmes.  As one producer pointed out, ``radio is made with your ears''.

The only way to fully comprehend audio recordings is through listening.  We found that radio producers used listening
to process information, judge the quality of the sound and identify non-speech sounds.  Some producers reported that
reading and listening at the same time enabled them to comprehend information more easily, supporting the findings from
\citet{Vemuri2004}.  Producers listened to identify any low quality audio, such as ``umm''s and ``err''s, and long or heavy
breaths. These are visible in verbatim transcripts, as used by \citet{Berthouzoz2012} and \citet{Rubin2013}, but are
not included in ASR transcripts.  Producers also listened to identify any high quality moments that may not be
identifiable using the transcript, and non-speech sounds that they might want to include or exclude in their programme.


The existing radio production workflow involves producers ``logging'' recordings by listening to the audio and typing
rough quotes and labels.  This process is time-consuming and tedious, but it helps the producer to review and organise
their content.  ASR transcription replaces this logging process, which saves producers time and effort. However, it
also means that producers don't have to listen to the audio before editing, which may prevent some producers from
listening to the material before making editorial decisions.  There is a risk that over-reliance on transcripts may
result in missing out on good quality content or failing to avoid poor quality content, which would affect the overall
programme quality.


Providing an easy way to listen to the audio underlying the transcript is important. Our screen-based semantic speech
editor included integrated playback, which allowed users to navigate the audio using the text.  Time compression, such
as described in Section 2.6, would also allow producers to listen faster, and producers valued this feature when we
added it to our screen-based editor. Reading the transcript while listening also increases the
speed at which time-compressed audio can be comprehended \citep{Vemuri2004}.

However, we found that in some situations, transcripts are not necessary or useful. With recordings shorter than 15-25
mins, producers reported that they can remember what was said, and when it was said. Some programmes have a greater
focus on the sound design, which limits the value of a transcript.



We saw that transcripts of radio programmes are not normally published at the BBC, as they are not written during the
programme's production. ASR and transcript correction tools could make it possible for producers to create a verbatim
transcript as part of their production process.  Publishing these could allow audio content to be more easily
searchable and discoverable through Internet search engines, for example.  Word-level timings could also be used to
link directly to segments of audio. For example, NPR's online clipping tool ``Shortcut'' \citep{Friedhoff2016} allows
radio listeners to create a short clip from a radio programme and share it with their friends. 

\subsection{Paper vs screen}

Our ethnographic case studies in Chapter~\ref{sec:ethno} identified that many radio producers work on paper.
Our user study in Chapter~\ref{sec:screen} also found that many producers want to be able to work away from their
screens.  This led us to develop a novel semantic speech editing system that used digital pens to allow producers to
edit speech recordings using a paper transcript.  We conducted a qualitative user study with professional radio producers
that compared our paper interface to a screen-based semantic speech editor. The results of our study provide insights
into the relative benefits of editing speech on paper compared to screens. We found that both the paper and screen
interfaces worked well, but that each had advantages and disadvantages in different contexts.

Overall, we found that the paper interface was better suited to quick and simple edits where listening is not as
critical, such as with high accuracy transcripts, or very recent recordings.  Our screen-based semantic speech editor
included integrated playback and correction features, which made it better for more complex editing with less familiar
audio and less accurate transcripts.

Producers reported that paper transcripts were easier to read and remember, and made it easier for them to think widely
and orientate themselves.  They reported that our digital pen interface was simple, intuitive, precise and allowed edit
decisions to be made quickly and easily.  However, producers had concerns over the cost of an additional device which
would likely have to be shared amongst a team, and could easily be lost.

The physical nature of the digital pen and paper made it better suited to travel and working away from the desk. This
gives producers greater flexibility to work in more comfortable locations and while commuting.  However the digital pen
interface uses considerable amounts of paper, involves carrying the pen around, and requires access to a colour laser
printer, which would make it difficult to work ``on the road''.  The screen interface could be used on a laptop or
tablet, which have the added benefit of integrated playback. However, screens are heavier, bulkier and have a much
shorter battery life than digital pens.

Radio is usually produced by a team, so tools that facilitate collaboration are valuable in such an environment.  We
found that the physical nature and pagination of paper made it better suited to face-to-face collaboration than the
screen.  However, the screen interface is capable of remote collaboration, and could be used over the Internet for
real-time collaborative speech editing.

The limitations of the digital pen system we used prevented us from including features for integrated playback,
correction and undo. The lack of integrated playback forced producers to replay and navigate audio using a separate
device, which made it more challenging to identify errors in the transcript. This was a bigger issue with low
quality transcripts, which could also not be corrected using the digital pen.  The integrated playback of the screen
interface made it easier for the participants to find and fix mistakes in the transcript.  This also made it
easier to edit content that required more listening, such as old or unfamiliar recordings.
















































































\section{Further work}\label{sec:conclusions-further}

Further work that could follow on from the research of this thesis includes:

\paragraph{Collaborative semantic speech interaction:}

The semantic speech editing systems we developed in this thesis were designed to be operated by individuals.
However, as shown in Chapter~\ref{sec:ethno}, radio production involves teams of people.  Collaborative tools may help
these teams work together more efficiently.  Operational transformation techniques \citep{Sun2004} have enabled the
development of collaborative document editing tools, such as the popular \textit{Google Docs}, which can facilitate
concurrent team-based working.  For example, \citet{Fisher2016} describes how NPR used Google Docs to enable over a
dozen producers to collaboratively fact-check the US Presidential Election debates live, using real-time ASR
transcripts. By linking the text to audio, a similar approach could be used for collaborative semantic editing of
speech. Teams could also use annotation to make suggestions for edits that are then accepted or rejected by the
programme producer.

\paragraph{Assisted/automatic de-umming:}

In Section~\ref{sec:doc}, we saw that a large proportion of a studio manager's time was spent on cleaning-up interview
material by removing unwanted vocal noises, known as ``de-umming''. Examples of unwanted noises include ``umm''s and
``err''s, long or loud breaths, and redundant phrases, such as ``you know''.  These can be difficult to identify, as
their presence is not clearly visible using current tools, and they can be difficult to remove as they often overlap
and blend with the clean speech. \citet{Loviscach2013} demonstrated a prototype ``umm detector'' that extracted MFCC
features (see Section~\ref{sec:background-spectral}) from the audio and used template matching to visually highlight
potential ``umm''s. We could not find any other work that attempted to detect or remove unwanted speech noises.

One solution could be to train an
ASR system to ``transcribe'' unwanted sounds. These ``words'' could then be highlighted, or removed in the same way
that transcripts can be semantically edited.  However, this assumes that these noises have clearly defined boundaries,
which they often do not.  In the author's experience, de-umming involves a level of editorial and creative judgement,
so a ``human in the loop'' system may be necessary.

\paragraph{Improved automatic speech recognition:}

In this thesis, we found that the transcripts produced by current ASR systems were sufficiently accurate to perform
semantic speech editing in professional radio production. However, as discussed in
Section~\ref{sec:transcript-generation}, ASR quality affects comprehension, search, and the need for correction, so it
is desirable to improve the transcript accuracy. Prior to transcription, much is already known about the content of the
speech, and by providing this information to the ASR system in advance, it could be used to improve the quality of the
transcript.  For example, the topic of conversation could be used to weight the likelihood of words related to that
topic to appear.  Many ASR systems have a speaker diarization pre-processing stage, and the number, gender and identity
of speakers could also be used to improve the accuracy of this process.

\paragraph{Improved digital pen system:}

The design of our digital pen semantic speech editor in Chapter~\ref{sec:paper} was influenced by the technical
limitations of the technology that we used to implement it.  This prevented us from including certain features such as
integrated listening and correction, and was inflexible in interpreting the user's gestures.  These issues could be
avoided by using a different system for implementation.

The digital pen we used operated in ``batch mode'' as it connected to the
computer using a USB dock. This prevented us from integrating control over the playback of the audio. A digital pen
with a wireless link could allow a user to play, pause and navigate the audio using the paper transcript.  The design
of our paper layout used strict rectangular boundaries to capture edit commands, which created a potential source of
errors. By using a more flexible approach, the system may be able to better understand the user's intentions and avoid
inadvertent mistakes.  Additionally, the ability to distinguish between edit commands and handwriting could allow the
user to both edit and correct the transcript on paper.

\paragraph{Penless paper editor:}

In Chapter~\ref{sec:paper}, we based the design of our system on a digital pen as it combined the readability of real
paper with the digital capture of freehand annotations. However, when we evaluated the system, some producers were
concerned about losing the pen, having to share it and its cost. Several producers said they would prefer to use a
highlighter or different coloured ink, which digital pens do not support. There are also patents that cover digital
pens \citep{Fahraeus2003}, which can make development more difficult.

Scanners and cameras could be used as alternative methods of digitally capturing freehand annotations from paper.  For
example, a barcode or QR code could be used to label each page with a unique identity that links to a stored image of
the printed page. The stored image could be used to mask the picture, or scan of the paper, leaving only the user's
annotations. Image analysis could then be used to interpret the user's annotations and translate them into edits,
corrections and labels.

\paragraph{Rich audio visualization:}

In Chapter~\ref{sec:colourised}, we showed that speech/music segmentation can be performed faster, more easily and more
accurately by using pseudocolour to map a semantic audio scalar feature to a colour gradient. Previous systems from
\citet{Tzanetakis2000} and \citet{Mason2007} have also showed how false colour can be used to map multiple features to
colour space.  \textit{Onomatopoeia} is the formation of words that resemble the sound they describe.  The author
believes that there is significant untapped potential to design an onomatopoeic audio visualization that ``looks like
it sounds''.  Such a visualization could provide an efficient and accessible way of navigating all types of audio
content.

Crossmodal correspondences could be exploited when designing the mapping between semantic audio features and visual
properties.  Many of the links listed in Table~\ref{tab:crossmodal} (p.~\pageref{tab:crossmodal}) have yet to be tested
for visually navigating audio content, and previous work on audio visualization has mostly focused on colour.  Other
visual properties such as shape and texture could be used to increase the richness of the information presented in the
image.  For example, textures can be generated using procedural techniques \citep{Ebert1994}, which would allow them to
be synthesised from semantic audio features.  However, selecting the best combination of semantic audio features and
visual mappings is a huge challenge, and is likely to be different for each application.



\paragraph{Application to television}

The focus of research in this thesis has been solely on semantic audio tools for the production of radio. However,
there may be opportunities to transfer some of the tools and findings from this work to the production of television.
The weekly reach and consumption figures for television (91\%, 25 hours) are similar to that of radio (90\%, 21 hours)
\citep[pp.  82, 119]{Ofcom2017}.  However, based on the author's experience in working with BBC producers in both
television and radio, there are big differences between the two in terms of team size, budget and culture. These
differences may affect the relevance and performance of the tools.

Radio is often produced by small teams of between two and five. Television production involves dozens or sometimes
hundreds of people, as demonstrated by the list of credits at the end of each programme.  In 2016/17, the BBC spent
\textsterling2.48B on television production --- over five times the amount spent on radio production \citep[pp.  39,
111]{Ofcom2017}. There are also important cultural differences between television and radio production. For example, in
the BBC, most radio producers are employed directly by the public service arm of the BBC as full-time staff. In
television, most producers are freelancers working on short-term contracts, either for an independent company or a
commercial arm of the BBC.  It is currently unknown how these differences will affect the performance and usage of
semantic production tools, so this may be an interesting direction for the research.





\clearpage
\section{Summary}\label{sec:conclusions-conclusions}

In this thesis, we investigated how semantic audio technology could be used to improve the radio production process.
We began our research by conducting three ethnographic case studies of professional radio production.  Based on our
findings, we developed three semantic audio tools for radio production using semantic audio visualization, and semantic
speech editing on screen and paper interfaces. By evaluating our tools, we answered the following four research
questions: 
\\

\noindent
\hangindent=15pt
\hangafter=1
\textit{How can radio production be improved with new technology for interacting with and manipulating recorded audio?}
\\

  We found that radio production can be improved by using semantic audio visualization to add colour to audio
  waveforms, and using semantic speech editing to edit speech using text on both screen and paper interfaces.
  \\

\noindent
\hangindent=15pt
\hangafter=1
\textit{What is the role and efficacy of audio visualisation in radio production?}
\\

We found that radio producers regularly use audio waveforms to navigate and edit audio content.  We developed a
simple semantic audio visualization for segmenting music and speech, and conducted the first formal study on the effect
of audio waveforms and semantic audio visualization on user performance. We found that using our semantic audio
visualization was faster, more accurate and required less effort than using a waveform.  Using an audio waveform
required less effort than no visualization, but was not significantly faster, nor more accurate.
\\

\noindent
\hangindent=15pt
\hangafter=1
\textit{How can transcripts of speech be adapted and applied to radio production?}
\\

We found that some radio producers use textual representations to navigate and edit audio content.  We developed a
semantic speech editing system that allowed radio producers to edit audio using text, and evaluated this approach with
professional radio producers for the production of genuine programmes. The radio producers were successful in using
semantic speech editing with ASR transcripts as part of their workflow, and continued to use our system after the
study. Through our study, we also gained insights into the importance of annotation, collaboration, portability and
listening.
\\

\noindent
\hangindent=15pt
\hangafter=1
\textit{What is the potential role of augmented paper in radio production?}
\\

We found that some radio producers use paper to make freehand annotations and facilitate face-to-face collaboration.
We designed and developed a novel system for semantic speech editing on paper using a digital pen. We compared our
paper interface to a screen interface and normal paper through a user study of professional radio producers. We found
that the benefits of reading from paper and the simplicity of the pen interface made it better for fast, simple editing
with familiar audio and accurate transcripts.  The integrated listening and correction features of the screen interface
made it better for more complex editing with less familiar audio and less accurate transcripts.  We also gained
insights into effect of ASR accuracy, the role of listening, and the relative benefits of paper and screen interfaces
for collaboration and portability.
\\











We have shown that semantic audio production tools can benefit professional radio producers, but they could also be
adapted to make audio production more accessible to the wider public. This could empower many more people to 
use audio production as a medium for self-expression, benefiting society as a whole.

