\chapter{Measuring audio visualization performance}\label{sec:colourised}


An audio waveform is a plot of the amplitude of an audio signal over time \citep[p.~92]{Hausman2012}.  The amplitude
profile of a waveform can be used to identify different tracks, see which parts are loud or quiet, and to identify
errors such as clipping and unwanted noise.  Users can also learn to read the cadence of speech, or even to spot
certain consonants, but this requires experience and practice \citep[p.~115]{Hausman2012}.  As we saw in
Section~\ref{sec:background-daw-visual}, audio waveforms are used in many digital audio workstations as a visual guide
for navigating audio. We learned in Chapter~\ref{sec:ethno} that the radio producers we observed relied on
waveforms to navigate and edit audio content as part of the radio production process.

We saw in Section~\ref{sec:background-waveforms} that waveforms are limited in the amount of information they can
display when viewed at different levels of zoom \citep{Loviscach2011}.  The producers we observed in
Chapter~\ref{sec:ethno} used audio waveforms to navigate long audio recordings. To view a long recording on a screen,
the waveform must be zoomed-out so that it can fit on the display.  This reduces the resolution of the waveform, which
means that frequency information and fine variations in the amplitude are no longer visible.  Without this information,
users cannot see the pitch, spectrum or timbre of the audio, which may reduce their ability to efficiently navigate and
edit audio content.  Despite the widespread use of waveforms in DAWs, we could not find any formal studies that have
attempted to measure their effectiveness at navigation or editing tasks.

We saw in Section~\ref{sec:background-waveforms-related} that previous work has used semantic audio analysis to enhance
audio waveforms through the use of colour.  \citet{Rice2005,Akkermans2011,Loviscach2011a} used pseudocolour to map
scalar values to a colour gradient to display information about the timbre of the audio.
\citet{Tzanetakis2000,Mason2007} used false colour to map audio feature vectors to RGB colour space to distinguish
musical genres and navigate radio broadcasts.  These systems demonstrated the potential of enhancing audio waveforms by
mapping semantic audio features to colour. However, we could not find any formal user studies that attempted to
evaluate the effect of this approach on navigation or editing tasks.


We were interested in examining how audio waveforms affect editing tasks in radio production, and whether enhancing
audio waveforms with colour improves their performance. In Section~\ref{sec:vis-method} we describe the design of our
user study, in which we measured the performance of users in completing an editing task using different audio
visualizations. We present the results in Section~\ref{sec:vis-results}, which show that mapping semantic audio
features to colour improved user performance in our task. We discuss these results in Section~\ref{sec:vis-discuss} and
present our conclusions in Section~\ref{sec:vis-conclusions}.











\section{Methodology}\label{sec:vis-method}

In this study we aimed to discover what effect audio visualizations have on audio editing in radio production.  We
designed our study to measure user performance of a simple task, as this gave us a specific and repeatable action by
which we could assess the effect of the visualizations.  We wanted the task to be an activity that is common within
radio production, and to use audio that is representative of that used by radio producers.  However, in order to
recruit enough participants, we chose a task that did not require radio production experience.

In this section, we first explain our approach to recruitment and choice of task, as this influences the design of
our experiment. We then describe the test interface we designed, the test conditions, our selection of audio clips, the
metrics used to measure performance, and our hypotheses.  Finally, we present the protocol of our study and our
analysis methodology.

\subsection{Recruitment}
The radio production community is relatively small. Producers are very busy and not used to participating in academic
studies.  We wanted to recruit enough participants for our results to be able to show statistical significance. Based
on estimates from informal testing, we needed at least 40 participants.  To get enough respondents, we chose to recruit
technology researchers with experience in media technology and production, of which there is a much larger population.
To attract enough participants, we designed our study so that it could be completed online, and in 15 mins or less.  We
used email distribution lists to advertise our study to everyone working at BBC Research and Development, and the
Electronics and Computer Science department at Queen Mary University of London.

\subsection{Task}
In radio production, music has to be removed when turning a broadcast programme recording into a podcast. This is due
to music licensing issues, which are different for downloadable content than they are for radio broadcasts. The music
is removed by editing the audio in a DAW that uses a waveform to visualize the audio. Although the waveform can be
used to allow users to distinguish between music and speech, at typical zoom levels it is not always visible. This
means that removing the music can be a slow and tedious process. Therefore, we chose the task to be segmentation of
music and speech.

\subsection{Test interface}

To conduct the experiment, we developed a web-based test interface, shown in Figure~\ref{fig:visualization-interface}.
The interface displayed the overall visualization as well as a zoomed-in view above it. The participant could navigate
the audio by clicking on either view, which would seek to that position in the audio.  Buttons below the visualization
controlled play/pause, zoom level and setting the in and out points of the selection.  The selection was displayed by
highlighting both the visualization itself and a slider below it. The selection could be adjusted by dragging either
end of the slider.  Training was provided using a ``pop-up tour'', which guided the user through the interface's
features and operation using a series of pop-up text boxes.  The interface also captured the participant's
questionnaire responses and preferences.

\begin{figure}[p]
\centering
\includegraphics[width=\columnwidth]{figs/browser-audio-interface}
\caption{Screenshot of the user interface used to display the audio visualizations, complete the segmentation task and
measure the user's performance.}
\label{fig:visualization-interface}
\end{figure}



We generated the visualizations of the audio clips using a plugin framework we developed called ``Vampeyer'', which
mapped the results of audio analysis plugins to a bitmap image. We then integrated those images into our test interface
by developing an interactive web-based audio visualization library called ``BeatMap''.  We describe Vampeyer and
Beatmap in greater detail in Appendices \ref{sec:vampeyer} and \ref{sec:beatmap}.

\begin{figure}[p]
  \centering
  \begin{subfigure}{.3\textwidth}
    \centering
    \includegraphics[width=\columnwidth]{figs/condition1.png}
    \caption{C1: None}
  \end{subfigure}
  \begin{subfigure}{.3\textwidth}
    \centering
    \includegraphics[width=\columnwidth]{figs/condition2.png}
    \caption{C2: Waveform}
  \end{subfigure}
  \begin{subfigure}{.3\textwidth}
    \centering
    \includegraphics[width=\columnwidth]{figs/condition3.png}
    \caption{C3: Enhanced}
  \end{subfigure}
  \caption{The audio visualization conditions that were evaluated.}
  \label{fig:conditions}
\end{figure}

\subsection{Conditions}\label{sec:colourised-conditions}
As we saw in Section~\ref{sec:background-daw-visual}, radio producers edit audio with DAWs that use audio waveforms as
the medium for interaction. In Section~\ref{sec:background-waveforms-related}, we also saw that by adding colour to
audio waveforms, additional semantic information about the audio could be displayed, while retaining the familiar
audio waveform visualization.  We were interested in measuring the performance of both the normal audio waveform and a
semantically enhanced waveform, and also comparing them directly.  Therefore, we chose to use the following conditions
for the audio visualizations. An example of each condition is shown in Figure~\ref{fig:conditions}.

\begin{enumerate}[label=C\arabic*.]
  \item \textbf{None}: No visualization, audio only.
  \item \textbf{Waveform}: Audio waveform in a single colour.
  \item \textbf{Enhanced}: Audio waveform with colour mapped to low energy ratio.
\end{enumerate}

We included a condition in which there was no visualization (C1) to use it as a baseline to measure the performance of
the normal waveform. For this condition, the participant must rely purely on listening to the audio.  For the other two
conditions, they are able to both listen and use the visualization.

For the enhanced visualization, we extracted an audio feature that was relevant to the task and mapped it to the colour
of the waveform.
Speech-music discrimination (SMD) is a research topic that aims to automatically segment speech and music.  This
research often targets recordings of radio broadcasts
\citep{Goodwin2004,Wieser2014,Saunders1996,Pikrakis2008,Pikrakis2006a}.

We wanted to select an audio feature that would assist the participant in completing their task.  However, we also
wanted there to be an element of human judgement involved in the task. If we chose an audio feature that was very
accurate, the algorithm would be doing all of the work, and the human would only be processing the results.  To avoid
this situation, we restricted the performance of the audio feature by only allowing a one-dimensional (scalar) feature.

Low energy ratio (LER, also known as `silent interval frequency', `energy contour dip' and `low short-term energy
ratio') is the frequency in which the energy of a signal falls below a threshold. This is a simple but effective
feature which exploits the fact that speech has frequent silent gaps between words, whereas music does not.
\citet{Panagiotakis2005} found that on its own, LER can classify 50\% of music segments with 100\% precision.

We calculated the low energy ratio by extracting the RMS of the signal (20ms frames, no overlap) and counting the proportion
of frames which fell below a threshold (see Equation~\ref{eq:ler}).  The threshold can be set as a fixed value
\citep{Liang2005,Panagiotakis2005}, a function of the moving average \citep{Ericsson2009}, or a function of the moving
peak value \citep{Saunders1996}.  After empirically testing a variety of radio programme recordings, we chose to use a
function of the moving average, which we configured as 3\% of the mean RMS in a one second sliding window.

\begin{align}
  x_{rms}(n) &= \sqrt{\frac{1}{F} \displaystyle\sum\limits_{i=Fn}^{F(n+1)} x_i^2}\\
  X &= \{x_{rms}(n) \mid 0 \leq n < \frac{f_s}{F}\}\\
  X_{low} &= \{x\ | \ x \in X \wedge x < 0.03 \overline{X} \}\\
  \text{LER} &= 100 \times \frac{|X_{low}|}{|X|}\label{eq:ler}
\end{align}

where $x_i$ are the audio samples, $f_s$ is the audio sample rate, and $F$ is the number of samples in each frame.



We coloured the waveform by mapping the low energy ratio to a linear gradient between two easily distinguished colours.
We used blue for representing speech to match the waveform colour of the most commonly used DAW in BBC Radio. To
represent music, we inverted the RGB values of the waveform colour to produce pink.

\subsection{Audio clips}
We used radio programme recordings for the audio clips, by choosing a representative selection of programme formats,
musical genres and radio stations, shown in Table~\ref{tab:clips}.  We sourced the audio content from recordings of BBC
Radio broadcasts. We selected ten 5-minute clips that contained only one section of music, with speech before and
after, where the music had clear start and end points. We checked the performance of the LER feature to ensure it had
only modest performance and was consistent between clips. We cut the clips so that the music was in a different
position in each clip.  We chose to use nine 5-minute clips so that the segmentation tasks could be completed in around
15 mins, and one clip for training.

\begin{table}[htbp]
  \begin{center}
    {\small
    \begin{tabular}{l l l l l l}
      \hline
      \textbf{Clip} & \textbf{Network} & \textbf{Programme} & \textbf{Format} & \textbf{Music genre} \\ \hline
      Training & Radio 4 & Desert Island Discs & Interview & Ambient \\
      1 & 1 Xtra & Sian Anderson & Breakfast & Dance \\
      2 & 6 Music & Lauren Laverne & Single & Indie \\
      3 & Radio 2 & Ken Bruce & Phone quiz & Lounge \\
      4 & Radio 3 & Breakfast show & Single & Classical \\
      5 & 5 Live & Sports report & Sports & Band \\
      6 & Radio 1 & Zane Lowe & Interview & Rap \\
      7 & Radio 2 & Jo Whiley & Review & Pop \\
      8 & Radio 4 & Afternoon drama & Drama & Classical \\
      9 & Radio 4 & Front Row & Interview & Alternative \\ \hline
    \end{tabular}
  }
  \end{center}
  \caption{Descriptions of the radio programmes used as the source of the audio clips.}
  \label{tab:clips}
\end{table}

\subsection{Metrics}

We wanted to measure the user's performance in completing the speech/music segmentation task when using different audio
waveforms. We were interested not only in measuring whether there was an actual difference in the task performance, but
also measuring whether the user perceived any difference in their performance.  To do this, we used both quantitative
and qualitative metrics.

\subsubsection{Quantitative metrics}
Our audio segmentation task involves finding the target audio (in this case, music) and marking the start and end of
the desired region. The two primary activities involved in this are seeking through the audio (by clicking on the
visualization), and marking the segment (using the buttons or sliders).  We chose to use three metrics to quantify the
effort, time and accuracy of the completed task.

\begin{itemize}
  \item \textbf{Effort}: We counted the number of seek actions (i.e. clicks on the visualization) used to complete the
    task.
  \item \textbf{Time}: We calculated how long it took to complete each task. To avoid including ``downtime'' at the
    start and end of the task, we calculated the task completion time as the difference between the first user action
    (e.g.  play/seek/mark) and the last marking action.
  \item \textbf{Accuracy}: We calculated the error of the result by using ground truth data about the precise start and
    end time of the music in the audio, then finding the sum of the absolute error of the selected in-point and
    out-point.
\end{itemize}

\subsubsection{Qualitative metrics}

To gather perceptual data about the tasks, we included a questionnaire for the participants to complete after using
each visualization. To allow for greater repeatability and comparison with other studies, we chose to use a
standardised set of questions. For this, we used the NASA Task Load Index, or \textit{TLX} questionnaire
\citep{Hart1988}, listed below.  Responses are captured using on a scale between -10 and 10 with the following labels
at each end:


{\singlespacing
\begin{itemize}
  \item Mental demand --- How mentally demanding was the task? [very low/very high]
  \item Physical demand --- How physically demanding was the task? [very low/very high]
  \item Temporal demand --- How hurried or rushed was the pace of the task?  [very low/very high]
  \item Performance --- How successful were you in accomplishing what you were asked to do? [perfect/failure]
  \item Effort --- How hard did you have to work to accomplish your level of performance? [very low/very high]
  \item Frustration --- How insecure, discouraged, irritated, stressed, and annoyed were you? [very low/very high]
\end{itemize}
}

The full TLX measurement involves converting the sub-scales into an overall TLX rating by weighting them by importance
and summing the result \citep{Hart2006}. We wanted to be able to analyse each individual sub-scale, so rather than
calculating the overall TLX value, we are reporting the ``Raw TLX'' values.

Three of the six TLX scales were similar to our three quantitative metrics, as listed below. We used these to
compare the actual and perceived differences between the conditions. Additionally, we collected and analysed the
results of all TLX scales to report other perceived differences that we could not otherwise measure.


\begin{itemize}
  \item \textbf{Effort}:
    We used the TLX \textit{effort} rating to measure perceived effort.
  \item \textbf{Time}:
    We used the TLX \textit{temporal demand} rating to measure perceived task completion time.
  \item \textbf{Accuracy}:
    We used the TLX \textit{performance} rating to measure perceived accuracy.
\end{itemize}


\subsection{Hypotheses}

We expected that for all of the metrics, the enhanced waveform would perform better than the normal waveform, and
the normal waveform would perform better than no visualization. Specifically, we defined the following hypotheses:

\newcommand{\subscript}[2]{$#1 _ #2$}
\begin{enumerate}[label=H\arabic*.]
  \item \textbf{Effort}: Audio visualization affects the actual and perceived effort required to segment music from
    speech, where C1 requires more effort than C2, and C2 requires more effort than C3.
  \item \textbf{Time}: Audio visualization affects the actual and perceived time taken to segment music from speech,
    where C1 requires more time than C2, and C2 requires more time than C3.
  \item \textbf{Accuracy}: Audio visualization affects the actual and perceived accuracy of segmenting music from
    speech, where C1 is less accurate than C2, and C2 is less accurate than C3.
\end{enumerate}


\subsection{Procedure}
Before the study began, we asked the participant to read an information sheet and agree to a consent form. There were
four stages to the study:

\paragraph{Stage 1: Demographics}
We asked the participant about their gender, age and the following questions, to gauge their familiarity with DAWs and
professional experience:

{\singlespacing
\begin{itemize}
  \item Do you understand what an audio waveform is? [Yes/No]
  \item Have you previously used any consumer audio editing software? (e.g.  Audacity, GarageBand) [Yes/No]
  \item Have you previously used any professional audio editing software? (e.g.  ProTools, Logic, Cubase/Nuendo, SADiE,
    Startrack) [Yes/No]
  \item How many years (if any) have you worked with audio in a professional capacity? [\textit{number}]
\end{itemize}
}

\paragraph{Stage 2: Training}
We used a `pop-up tour' to overlay a sequence of text boxes on the interface (see
Figure~\ref{fig:visualization-interface}). These explained the features and operation of the test interface, then
prompted the user to complete and submit a training task using the enhanced waveform (C3). We measured the error of the
training task and did not allow the participant to continue until they had completed the task successfully (as defined
in Section~\ref{sec:waveform-analysis}).

\paragraph{Stage 3: Segmentation task}
The participant used the test interface to mark the position of music in a speech recording, then submit their result.
We logged and timestamped the participant's actions, including seek, play/pause, zoom and mark.  This exercise was
repeated three times for each condition, for a total of nine tasks.  We wanted to gather feedback directly after each
condition to capture the participant's reaction, and to avoid possible confusion due to switching too often.  To
achieve this, we grouped the presentation of the conditions (e.g. AAABBBCCC) rather than interleaving them (e.g.
ACACBABCB).

Each audio clip can only be used once per participant, otherwise they would be able to remember the location of the the
music. To define a balanced sequence for the audio clips, we used a Williams design Latin square \citep{Williams1949},
generated using the \texttt{crossdes} package in R \citep{Sailer2013}. We used Latin squares to block out the effect of
the order of presentation, and a Williams design, which is balanced for first-order carryover effects.  As the sequence
length is an odd number, the Williams design uses two Latin squares to produce an $18\times9$ matrix.

To generate the order of visualizations, we needed to produce a balanced $18\times3$ sequence. We did this by taking
three columns from our $18\times9$ Latin square and mapping the values 1--3, 4--6 and 7--9 to 1, 2 and 3, respectively.
By calculating the carryover effect of each column of the $18\times9$ matrix, we found that the middle three columns
had the minimum carryover effect, so we used them for our visualization sequence.


After completing the three tasks for each condition, the participant rated the workload of those tasks
using the NASA-TLX metrics. We captured the responses using sliders with the labels for each question on either end.

\paragraph{Stage 4: User preference}
After all the tasks were completed, we asked the participant to select which condition they thought was the easiest,
and which was the most frustrating. We used thumbnail images from Figure~\ref{fig:conditions} to remind them of what
each condition looked like.

\subsection{Analysis}\label{sec:waveform-analysis}
As the experiment was unsupervised, we wanted to ensure that all participants completed the tasks correctly.  We did
this by measuring the error of the task and rejecting participants that submitted a response with an error of 5 seconds
or greater. We chose this threshold after running informal tests which showed that under supervision, all responses had
an error of up to 5 seconds.  We calculated the error as the sum of the absolute error of the in-point and
out-point.


We tested for statistically significant differences in the TLX ratings by using SPSS to conduct a repeated measures
analysis of variance (rANOVA) \citep[p.~409]{Shalabh2009}.  We tested the underlying assumptions of normality and
sphericity by plotting the distribution for each condition and using Mauchly's Test of Sphericity
\citep[p.~415]{Shalabh2009}.  Mauchly's Test indicated that the assumption of sphericity had been violated for the
effort metric [$\chi^2(2) = 9.657, p=.008$] and temporal demand metric [$\chi^2(2) = 17.918, p<.001$]. Therefore, we
corrected the degrees of freedom using the conservative Greenhouse-Geisser Correction \citep[p.~416]{Shalabh2009}.  We
then used Tukey's honest significant difference (HSD) post-hoc test \citep[p.~139]{Shalabh2009} to make pairwise
comparisons between the mean values of the metrics.

Our study contained one fixed effect (visualization) and two random effects (audio clip and participant).  We could not
re-use audio clips for the tasks as participants would remember the position of the music.  Therefore, as we did not
have results from every participant for every combination of audio clip and visualization, our dataset was incomplete.
As we are using a repeated measures design, we would normally analyse the results using a repeated measures ANOVA.
However this requires a complete dataset, so we were unable to use this analysis.

We used a linear mixed model to analyse the results of the metrics because it can account for an incomplete dataset and
a repeated measures design \citep{Gueorguieva2004}.  We used SPSS to perform a linear mixed effects analysis
\citep[p.~274]{Shalabh2009} of the relationship between visualization and the three performance metrics (seek actions,
task completion time and task error).  We configured the visualizations as a fixed effect and the audio clips and
participants as random effects. We tested the underlying assumptions of homoscedasticity and normality by plotting the
residual errors and visually inspecting them, which did not reveal any obvious deviations.
We then used the Least Significant Difference (LSD) post-hoc test \citep[p.~137]{Shalabh2009} to make pairwise
comparisons between the mean values of the metrics.







\clearpage
\section{Results}\label{sec:vis-results}
Our study was completed by 63 participants, of which 41 (65\%) passed the acceptance criteria of all task errors being
under 5 seconds. The failure rate was higher than expected, so we analysed the rejected tasks and participants to look
for evidence of any systematic errors that might explain the high number of rejections.

\begin{figure}[h]
\centering
  \begin{tikzpicture}
    \begin{axis}[
      height=0.5\textwidth,
      legend pos=south east,
      legend cell align=left,
      xbar stacked,
      ytick=data,
      xmin=0,
      ylabel=Clip,
      xlabel=Rejected response count,
      symbolic y coords={9,8,7,6,5,4,3,2,1},
    ]
    \addplot[fill=black!15] coordinates
      {(4,9) (2,8) (1,7) (1,6) (6,5) (3,4) (2,3) (3,2) (3,1)};
    \addplot[fill=black!40] coordinates
      {(1,9) (5,8) (0,7) (3,6) (3,5) (3,4) (0,3) (4,2) (1,1)};
    \addplot[fill=black!65] coordinates
      {(2,9) (1,8) (0,7) (0,6) (5,5) (8,4) (2,3) (3,2) (0,1)};
    \legend{C1,C2,C3}
    \end{axis}
  \end{tikzpicture}
  \caption{Rejected responses by audio clip.}
  \label{fig:rejected-clips}
\end{figure}

Figure~\ref{fig:rejected-clips} shows the clips and conditions that made up the rejected responses.  Although clips 4
and 5 had a higher number of rejections, these incorrect responses came from all clips and all conditions. There was
also no combination of clips and conditions that caused an unusually high number of rejections.  We did not find any
notable difference in error between the in-points and out-points.  Figure~\ref{fig:demographics} shows the demographics
of the participants.  We did not find any correlation between rejected participants and DAW experience, professional
experience, age or gender.

\begin{figure}[h]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      height=0.75\textwidth,
      width=0.85\textwidth,
      enlarge y limits=0.25,
      xbar stacked,
      ytick=data,
      yticklabel style={font=\footnotesize,text width=2.2cm,align=right},
      xlabel=Participants,
      symbolic y coords={Consumer and professional,Consumer,None},
      legend style={font=\footnotesize},
    ]
    \addplot[fill=black!20] coordinates
      {(29,Consumer and professional) (6,Consumer) (6,None)};
    \addplot[fill=white] coordinates
      {(17,Consumer and professional) (5,Consumer) (0,None)};
    \legend{Accepted,Rejected}
    \end{axis}
  \end{tikzpicture}
  \caption{DAW experience}
  \label{fig:daw-experience}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      height=0.75\textwidth,
      width=0.85\textwidth,
      legend pos=south east,
      enlarge y limits=0.15,
      xbar stacked,
      ytick=data,
      yticklabel style={font=\footnotesize},
      xmin=0,
      ylabel=Years,
      xlabel=Participants,
      symbolic y coords={31--40,21--30,11--20,6--10,1--5,0},
      legend style={font=\footnotesize},
    ]
    \addplot[fill=black!20] coordinates
      {(0,31--40) (3,21--30) (3,11--20) (6,6--10) (13,1--5) (16,0)};
    \addplot[fill=white] coordinates
      {(2,31--40) (2,21--30) (5,11--20) (3,6--10) (5,1--5) (5,0)};
    \legend{Accepted,Rejected}
    \end{axis}
  \end{tikzpicture}
  \caption{Professional experience}
  \label{fig:pro-experience}
\end{subfigure}
\caption{Participant demographics.}
\label{fig:demographics}
\end{figure}

We were unable to find any systematic errors that would explain the rejected responses, so we suspect that the lack of
supervision may have led some participants to perform the task to a lower standard than was required.

Figure~\ref{fig:demographics} shows that the vast majority of participants had previous experience of using both
consumer and professional audio editing software. 61\% of participants also had professional experience of working with
audio. All participants reported that they understood what an audio waveform was.






\subsection{Quantitative metrics}

We analysed the quantitative performance metrics using a linear mixed model \citep[p.~274]{Shalabh2009}.
Figure~\ref{fig:visualization-metrics} shows
the mean values and confidence intervals of the performance metrics and Table~\ref{tab:pairwise-performance} lists the
statistical significance of the pairwise comparisons between the conditions.

\begin{figure}[p]
  \centering
  \begin{subfigure}[t]{0.5\textwidth}
    \centering
    \begin{tikzpicture} 
    \begin{axis}[
      width=\textwidth,
      ylabel=count,
      ymin=0,
      xtick={1,2,3},
      xticklabels={C1,C2,C3}]
      \addplot[black,mark=*]
        plot[error bars/.cd, y dir=both, y explicit]
        coordinates {
          (1, 29.5) +- (3.7,3.7)
          (2, 24.3) +- (3.6,3.6)
          (3, 16.8) +- (3.1,3.1)
        };
    \end{axis} 
    \end{tikzpicture}
    \caption{Seek actions}
  \end{subfigure}%
  ~
  \begin{subfigure}[t]{0.5\textwidth}
    \centering
    \begin{tikzpicture} 
    \begin{axis}[
      width=\textwidth,
      ymin=0,
      ylabel=seconds,
      xtick={1,2,3},
      xticklabels={C1,C2,C3}]
      \addplot[black,mark=*]
        plot[error bars/.cd, y dir=both, y explicit]
        coordinates {
          (1, 0.645) +- (0.121,0.121)
          (2, 0.610) +- (0.121,0.121)
          (3, 0.523) +- (0.109,0.109)
        };
    \end{axis} 
    \end{tikzpicture}
    \caption{Task error}
  \end{subfigure}

  \begin{subfigure}[t]{0.5\textwidth}
    \centering
    \begin{tikzpicture} 
    \begin{axis}[
      width=\textwidth,
      ymin=0,
      ylabel=seconds,
      xtick={1,2,3},
      xticklabels={C1,C2,C3}]
      \addplot[black,mark=*]
        plot[error bars/.cd, y dir=both, y explicit]
        coordinates {
          (1, 70.6) +- (16.76,16.76)
          (2, 68.7) +- (16.49,16.49)
          (3, 59.7) +- (16.47,16.47)
        };
    \end{axis} 
    \end{tikzpicture}
    \caption{Task completion time}
  \end{subfigure}
  \caption{Mean performance metric values with 95\% confidence intervals. Lower values represent better performance.}
  \label{fig:visualization-metrics}
\end{figure}

\definecolor{lightred}{RGB}{255,204,204}
\definecolor{lightamber}{RGB}{255,204,153}
\definecolor{lightgreen}{RGB}{204,255,204}
\newcommand{\highsig}{\cellcolor{black!20}}
\newcommand{\medsig}{\cellcolor{black!10}}
\newcommand{\nosig}{\cellcolor{white}}
\begin{table}[p]
    \begin{tabular}{L{.35\textwidth-2\tabcolsep} C{.21\textwidth-2\tabcolsep} C{.21\textwidth-2\tabcolsep} C{.23\textwidth-2\tabcolsep}}
\hline
& \textbf{C1 vs C2} & \textbf{C2 vs C3} & \textbf{C1 vs C3} \\ \hline
	Seek actions        & \highsig$<.01$ & \highsig$<.01$ & \highsig$<.01$ \\
	Task completion time& \nosig$>.05$   & \highsig$<.01$ & \highsig$<.01$ \\
	Task error          & \nosig$>.05$   & \medsig$<.05$  & \highsig$<.01$  \\
\end{tabular}
\caption[$p$-values of pairwise comparisons for the performance metrics.]{$p$-values of pairwise comparisons for the
performance metrics.  Statistically significant differences are shaded.}
\label{tab:pairwise-performance}
\end{table}


\subsubsection{Seek actions}
The audio visualization had a significant effect on the number of seek actions used to complete the task
[$F(3,366)=93.871, p<.001$]. Based on the mean averages, the enhanced waveform (C3) required 7.5 (30\%) fewer seek
actions than the normal waveform (C2), and 12.7 (43\%) fewer than having no visualization (C1). Additionally, the
normal waveform (C2) required 5.2 (17\%) fewer seek actions than having no visualization (C1).  The differences in seek
actions between all three conditions were statistically significant ($p<.01$). These results confirm hypothesis H1
(effort).

\subsubsection{Task completion time}
The audio visualization had a significant effect on the time required to complete the task [$F(3,366)=25.261,
p<.001$].  Based on the mean averages, the task was completed 9 seconds (13\%) faster using the enhanced waveform
(C3) compared to the normal waveform (C2), and 10.9 seconds (15\%) faster compared to having no visualization (C1),
both with $p<.01$.  There was no statistically significant difference in task completion time between the normal
waveform (C2) and no visualization (C1).  The mean time of the normal waveform (C2) was only 1.9 seconds (3\%) faster
than no visualization (C1).  These results confirm hypothesis H2 (time) for C2$>$C3, but not for C1$>$C2.

\subsubsection{Task error}
The audio visualization had a significant effect on the accuracy of the task result [$F(3,366)=42.462, p<.001$].
Based on the mean averages, the error when using the enhanced waveform (C3) was 87ms (14\%) lower than when using the
normal waveform (C2) with $p<.05$, and 122ms (19\%) lower than when there was no visualization (C1) with $p<.01$. There
was no statistically significant difference in task error between the normal waveform (C2) and no visualization (C1).
The error using the normal waveform was only 35ms (5\%) less than with no visualization.  These results confirm
hypothesis H3 (accuracy) for C2$<$C3, but not for C1$<$C2.







\subsection{Qualitative metrics}

\begin{figure}[p]
  \centering
  \begin{subfigure}[t]{0.5\textwidth}
    \centering
    \begin{tikzpicture} 
    \begin{axis}[
      width=\textwidth,
      height=0.8\textwidth,
      ylabel=Effort,
      ymax=2,
      ymin=-8.5,
      ytick distance=3,
      xtick={1,2,3},
      xticklabels={C1,C2,C3}]
      \addplot[black,mark=*]
        plot[error bars/.cd, y dir=both, y explicit]
        coordinates {
          (1, -0.122) +- (1.49,1.49)
          (2, -2.244) +- (1.51,1.51)
          (3, -4.220) +- (1.28,1.28)
        };
    \end{axis} 
    \end{tikzpicture}
  \end{subfigure}%
  ~
  \begin{subfigure}[t]{0.5\textwidth}
    \centering
    \begin{tikzpicture} 
    \begin{axis}[
      width=\textwidth,
      height=0.8\textwidth,
      ylabel=Frustration,
      ymax=2,
      ymin=-8.5,
      ytick distance=3,
      xtick={1,2,3},
      xticklabels={C1,C2,C3}]
      \addplot[black,mark=*]
        plot[error bars/.cd, y dir=both, y explicit]
        coordinates {
          (1, -0.829) +- (2.03,2.03)
          (2, -2.976) +- (1.67,1.67)
          (3, -5.268) +- (1.24,1.24)
        };
    \end{axis} 
    \end{tikzpicture}
  \end{subfigure}%

  \begin{subfigure}[t]{0.5\textwidth}
    \centering
    \begin{tikzpicture} 
    \begin{axis}[
      width=\textwidth,
      height=0.8\textwidth,
      ylabel=Mental demand,
      ymax=2,
      ymin=-8.5,
      ytick distance=3,
      xtick={1,2,3},
      xticklabels={C1,C2,C3}]
      \addplot[black,mark=*]
        plot[error bars/.cd, y dir=both, y explicit]
        coordinates {
          (1, -1.390) +- (1.68,1.68)
          (2, -2.585) +- (1.45,1.45)
          (3, -4.585) +- (1.23,1.23)
        };
    \end{axis} 
    \end{tikzpicture}
  \end{subfigure}%
  ~
  \begin{subfigure}[t]{0.5\textwidth}
    \centering
    \begin{tikzpicture} 
    \begin{axis}[
      width=\textwidth,
      height=0.8\textwidth,
      ylabel=Performance,
      ymax=2,
      ymin=-8.5,
      ytick distance=3,
      xtick={1,2,3},
      xticklabels={C1,C2,C3}]
      \addplot[black,mark=*]
        plot[error bars/.cd, y dir=both, y explicit]
        coordinates {
          (1, -5.415) +- (1.10,1.10)
          (2, -6.317) +- (1.11,1.11)
          (3, -7.341) +- (0.88,0.88)
        };
    \end{axis} 
    \end{tikzpicture}
  \end{subfigure}%

  \begin{subfigure}[t]{0.5\textwidth}
    \centering
    \begin{tikzpicture} 
    \begin{axis}[
      width=\textwidth,
      height=0.8\textwidth,
      ylabel=Physical demand,
      ymax=2,
      ymin=-8.5,
      ytick distance=3,
      xtick={1,2,3},
      xticklabels={C1,C2,C3}]
      \addplot[black,mark=*]
        plot[error bars/.cd, y dir=both, y explicit]
        coordinates {
          (1, -3.122) +- (1.81,1.81)
          (2, -4.098) +- (1.45,1.45)
          (3, -6.171) +- (1.08,1.08)
        };
    \end{axis} 
    \end{tikzpicture}
  \end{subfigure}%
  ~
  \begin{subfigure}[t]{0.5\textwidth}
    \centering
    \begin{tikzpicture} 
    \begin{axis}[
      width=\textwidth,
      height=0.8\textwidth,
      ylabel=Temporal demand,
      ymax=2,
      ymin=-8.5,
      ytick distance=3,
      xtick={1,2,3},
      xticklabels={C1,C2,C3}]
      \addplot[black,mark=*]
        plot[error bars/.cd, y dir=both, y explicit]
        coordinates {
          (1, -3.171) +- (1.75,1.75)
          (2, -3.268) +- (1.51,1.51)
          (3, -4.878) +- (1.39,1.39)
        };
    \end{axis} 
    \end{tikzpicture}
  \end{subfigure}%
  \caption{Mean task load index values with 95\% confidence intervals. Lower values represent better performance.}
  \label{fig:tlx-results}
\end{figure}

\begin{table}[p]
    \begin{tabular}{L{.35\textwidth-2\tabcolsep} C{.21\textwidth-2\tabcolsep} C{.21\textwidth-2\tabcolsep} C{.23\textwidth-2\tabcolsep}}
\hline
& \textbf{C1 vs C2} & \textbf{C2 vs C3} & \textbf{C1 vs C3} \\ \hline
  Effort          & \medsig$<.05$  & \highsig$<.01$ & \highsig$<.01$ \\
	Frustration     & \medsig$<.05$  & \medsig$<.05$  & \highsig$<.01$  \\
	Mental demand   & \nosig$>.05$   & \highsig$<.01$ & \highsig$<.01$ \\
	Performance     & \nosig$>.05$   & \medsig$<.05$  & \highsig$<.01$  \\
	Physical demand & \nosig$>.05$   & \highsig$<.01$ & \highsig$<.01$ \\
  Temporal demand & \nosig$>.05$ & \medsig$<.05$ & \nosig$>.05$ \\ \hline
\end{tabular}
\caption[$p$-values of pairwise comparisons for the perceptual metrics.]{$p$-values of pairwise comparisons for the
perceptual metrics.  Statistically significant differences are shaded.}
\label{tab:pairwise-perceptual}
\end{table}

We analysed the TLX ratings using repeated measures ANOVA \citep[p.~409]{Shalabh2009}, which found that the audio
visualization had a significant effect on the TLX ratings from participants [$F(12,152)=3.552, p<.001$].
Figure~\ref{fig:tlx-results} shows the mean values and confidence intervals of the TLX metrics.
Table~\ref{tab:pairwise-perceptual} lists the statistical significance of the pairwise comparisons between the
conditions.

The TLX ratings from participants show that compared to both the normal waveform (C2) and no visualization (C1),
the enhanced waveform (C3) was perceived to be less mentally and physically demanding, better performing, less
frustrating and requiring less effort (all $p<.05$).  The participants also rated the enhanced waveform as less
temporally demanding than the normal waveform ($p<.05$).  Compared to no visualization (C1), participants rated
the normal waveform (C2) as being less frustrating and requiring less effort (both $p<.05$). 

The effort ratings confirm hypothesis H1 (effort); the temporal demand ratings confirm hypothesis H2 (time) for
C2$>$C3; and the performance ratings confirm hypothesis H3 (accuracy) for C2$<$C3.



The participants rated the enhanced waveform (C3) as significantly less physically demanding than the normal waveform
(C2).  This was surprising, as all of the tasks were conducted using a screen and mouse, so did not require much
physical exertion.  We do not know how participants interpreted this metric, but it's possible that some may have
classified the movement of the mouse or number of mouse clicks as physical activity.

Participants were asked to select which audio visualization was the easiest to use, and which was the most frustrating.
The results are shown in Figure~\ref{fig:visualization-comparison}.  The enhanced waveform (C3) was rated as the
easiest to use by three quarters of the participants.  Having no visualization (C1) was rated as the most frustrating
condition by two thirds of the participants. The normal waveform (C2) was not considered by many to be the easiest, nor
the most frustrating.

\begin{figure}[h]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      height=0.5\textwidth,
      width=0.9\textwidth,
      enlarge y limits=0.25,
      xbar,
      ytick=data,
      xlabel=Participants,
      symbolic y coords={C3,C2,C1},
    ]
    \addplot[fill=black!20] coordinates
      {(31,C3) (7,C2) (3,C1)};
    \end{axis}
  \end{tikzpicture}
  \caption{Easiest}
  \label{fig:easiest}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      height=0.5\textwidth,
      width=0.9\textwidth,
      enlarge y limits=0.25,
      xbar,
      ytick=data,
      xlabel=Participants,
      symbolic y coords={C3,C2,C1},
    ]
    \addplot[fill=black!20] coordinates
      {(6,C3) (8,C2) (27,C1)};
    \end{axis}
  \end{tikzpicture}
  \caption{Most frustrating}
  \label{fig:frustrating}
\end{subfigure}
\caption{Condition preferences of participants.}
\label{fig:visualization-comparison}
\end{figure}























\section{Discussion}\label{sec:vis-discuss}

Our study found that by using an enhanced waveform visualization, participants could segment music from speech faster,
more accurately and with less effort than by using a normal waveform.  When using the normal waveform, participants could
segment music from speech with less effort than having no visualization, but we did not find any significant
differences in the time it took, nor the accuracy of the result.  Table~\ref{tab:hypotheses} summarises the findings
for the hypotheses we tested.

\begin{table}[h]
  \centering
  \begin{tabular}{l | c c | c c}
    \hline
    & \multicolumn{2}{c|}{Quantitative metrics} & \multicolumn{2}{c}{Qualitative metrics} \\
    \hline
    H1 (effort)   & C1$>$C2 & C2$>$C3 & C1$>$C2 & C2$>$C3 \\
    H2 (time)     & --      & C2$>$C3 & --      & C2$>$C3 \\
    H3 (accuracy) & --      & C2$<$C3 & --      & C2$<$C3 \\
    \hline
  \end{tabular}
  \caption{Summary of confirmed findings for each hypothesis, with $p<.05$.}
  \label{tab:hypotheses}
\end{table}

The mean number of seek actions for the enhanced waveform was 30\% less than for the normal waveform, and the mean task
completion time was 13\% faster. This shows that the participants did not have to navigate the audio as often to
complete the task. This is likely to be because the colour enhancement allowed participants to narrow their search
region, so they did not have to perform as much listening as they otherwise would. The enhanced waveform resulted in
14\% higher accuracy than the normal waveform, potentially because the colour helped to narrow the search region used
to find the precise start and end time of the music.

The increased performance of the enhanced waveform demonstrates that there is potential in the colour mapping
techniques explored by \citet{Tzanetakis2000}, \citet{Rice2005} and \citet{Mason2007}. We did not attempt to select the
best possible semantic audio feature, nor the optimum colour mapping technique, as we wanted there to be a level of
human judgement involved in the task. It is likely that optimising these would provide much better performance than the
visualization we tested.

Radio programmes must be produced in a limited time period, which means that radio producers often work to tight
deadlines. A reduction in the time and effort needed to perform simple editing tasks could give radio producers greater
freedom to focus on more creative activities. In turn, this could potentially lead to improvements in the editorial
quality of programmes.

Although normal waveforms allowed participants to complete our music segmentation task with less effort than no
visualization, there was no significant difference in the task completion time, nor the accuracy of the result.  In a
study made up of 41 participants, we would have expected to find a significant difference compared to the baseline.
This poor performance raises questions over how helpful waveforms are as a navigational aid.

The consequence of poor performance of waveforms is particularly high because waveforms are the default visualization
for most digital audio workstations. With our enhanced waveform, we have seen that it is possible to provide greater
efficiency for navigating and editing audio for at least one task.  Improving the performance of the default
visualizations in DAWs could make a marked difference to a large number of people, as audio editing software is
used around the world by various professionals, many of whom spend much of their working life interacting with audio
using these visualizations.

We selected low energy ratio (LER) as a semantic audio feature for discriminating between music and speech.  Low energy ratio
is based on detecting changes in the amplitude profile.  Although these changes can clearly be seen using an audio
waveform, they are only visible when the waveform is sufficiently zoomed-in.  It is therefore important not only to 
include the right information, but to present it in a way that humans can read.

We restricted our selection of the semantic audio feature to a one-dimensional value, and used pseudocolour to map the
value to the waveform.  Low energy ratio is just one of many features we could have used for this task.  There are
other features that are more effective and could further improve user performance. Multiple features could
also be combined by using weighting, by using logic to switch between them or by mapping them using false colour.

The ability to visually identify the location of music has applications beyond removal of unwanted music.
\citet{Mason2007} mapped three semantic audio features using false colour to display the structure of radio programmes
to help consumers navigate audio recordings.  Many daytime radio programmes alternate between speech and music, so
being able to see where the music is played would provide a visual structure of the programme. This could help
producers and the audience navigate to the next piece of speech or music, and get a sense of the programme format and
length.

\section{Conclusion}\label{sec:vis-conclusions}

We conducted an online user study in which 41 participants segmented music from speech using three within-subjects
conditions --- a normal audio waveform, a waveform enhanced by mapping semantic information to colour, and without using
any visualization.  Based on both quantitative and qualitative metrics, we found that using the enhanced waveform,
participants completed the task faster, more accurately and with less effort than the normal waveform. Using the normal
waveform required less effort than no visualization, but was not significantly faster, nor more accurate.

Our results show that mapping semantic audio features to a visual representation can improve user performance.  Given
the large-scale use of waveforms in audio production, making improvements to the audio visualization could make a
meaningful impact on a large community.  For this study, we selected a rudimentary audio feature and visual mapping.
There are opportunities to develop more efficient audio visualizations by combining multiple features and using more
advanced visualization techniques, targeting either specific applications or general use.



