
\chapter{Background}\label{sec:background}

The focus of this thesis is on the production of audio content for radio broadcast.  Radio production is both a
technical and creative endeavour that combines complex audio technology with artistic taste and judgement
\citep{Barbour2004}. The aim of radio production is to ``manipulate sound to create an effect or deliver a message'',
which is achieved by combining various sources of sound into a programme \citep[pp. 12, 20]{Hausman2012}.  In this
chapter, we review methods, systems and technologies that are related to the production of radio, and to the
development of the semantic audio production tools in this thesis.








In Section~\ref{sec:background-editing}, we start by giving a brief overview of the methods and tools of audio editing,
which is used to create radio programmes.  We show how current editing tools use visual representations to interact
with audio, and discuss the limitations of these visualizations.  In Section~\ref{sec:background-semantic}, we show how
semantic audio analysis can be used to extract information from audio content to describe the sound.  We then consider
previous research that has used this semantic data to improve the navigation and editing of audio through the use of
audio visualization (Section~\ref{sec:background-visualization}), transcripts of speech
(Section~\ref{sec:background-transcripts}), and audio playback interfaces (Section~\ref{sec:background-playback}).
Finally, in Section~\ref{sec:background-questions}, we reflect upon the literature and our research aim to formulate
the research questions that we will attempt to answer in this thesis.




\section{Audio editing}\label{sec:background-editing}


The focus of this thesis is on the production of radio programmes using recorded audio.  Recording
sound ahead of broadcast brings with it a number of benefits \citep[p. 133]{Hausman2012}.  Programmes can be much more
complex, as many more sound elements can be brought together than would be possible in a live scenario.  The producer
is able to record re-takes of the same material until they are satisfied, which allows them greater freedom to
experiment and fix any mistakes that occurred.  The ability to re-record material can lead to better quality content
and open up opportunities for a wider range of programme genres, such as drama and documentaries.  Pre-recording has a
number of practical benefits too.  The time of production is not constrained by the broadcast schedule,
and content for multiple programmes can also be recorded in one session.

Recorded audio is refined through editing. \textit{Audio editing} is the process of selecting, re-arranging,
correcting and assembling audio content into a finished product \citep[p. 112]{Hausman2012}.  According to
\citet[p.~44]{McLeish2015} and \citet[p.~116]{Hausman2012}, the three primary reasons for editing are to:

{\singlespacing
\begin{enumerate}
  \item Re-arrange recorded material into a more logical sequence.
  \item Remove uninteresting, unwanted, repetitive or technically unacceptable sound.
  \item Reduce the running time.
\end{enumerate}
}

Underlying these practical aims of audio editing is an important creative process.
\citet[p.~116]{Hausman2012} state that editing is ``somewhat like an art form'', and \citet[p.~44]{McLeish2015}
suggest that editing can be used as a ``creative effect to produce juxtapositions of speech, music, sound and silence''.

\subsection{Digital audio workstations}\label{sec:background-daw}

For more than fifty years, audio was recorded on magnetic tape. Combining sound sources required the use of a large
mixing console which was used to control the sound with faders, knobs and buttons that had to be triggered at the right
time. Editing was performed by cutting the magnetic tape with a razor blade and sticking it back together again
\citep{Barbour2004}.

The development of fast processors and high quality audio interfaces has since allowed audio to
be stored and manipulated digitally using computer software.
The primary tool for editing digital audio is the \textit{digital audio workstation}, or \textit{DAW}. A DAW is
software that provides recording, mixing and editing capabilities for digital audio.  DAWs were first introduced in the
1980s \citep{Ingebretsen1982}, and have since evolved into powerful tools that are accessible to anybody with a
computer. Examples of popular commercial DAWs include \textit{Pro Tools} by Avid, \textit{Logic Pro} by Apple and
\textit{Cubase} by Steinberg \citep{AskAudio2015,ProducerSpot2015}.

DAWs provide a feature-rich toolset for manipulating audio signals.  They can be used to navigate and edit audio with
very fine control over timing, even down to individual samples. Automation means that changes made to the audio are
remembered and repeated each time the audio is played. Automatic cross-fading between clips can be used to
create inaudible edits.

\begin{figure}[t]
\centering
  \centering
  \includegraphics[width=\textwidth]{figs/sadie-example.png}
  \caption{User interface of the \textit{SADiE} digital audio workstation from Prism Sound, which is used at the BBC
  for radio production.} 
  \label{fig:sadie-example}
\end{figure}

The introduction of DAWs has transformed radio broadcasting by allowing fast random access, high storage densities,
improved portability, and greater cost-effectiveness than analogue systems \citep{Pizzi1989}.  The powerful features of
a DAW can replace most of the activities that would traditionally have to be performed using a radio studio.  The
accessibility of digital audio production has allowed audio editing to be performed by producers without requiring
specialist knowledge of sound engineering \citep{Peus2011}.  \citet[p.~44]{McLeish2015} suggested that the improved
usability of DAWs has created a ``high level of personal job satisfaction'' \citep[p.~44]{McLeish2015}.  However, the
deskilling of audio editing has also caused a reduction in the number of people required to produce radio programmes
\citep{Dunaway2000}.


As the audio is being stored and manipulated digitally, DAWs can be used to edit audio without any loss in sound
quality. However, when the edited audio is saved, there are two approaches that can be taken --- destructive and
non-destructive \citep[p.~45]{McLeish2015}. \textit{Destructive editing} occurs when a change is made that alters the
structure of the sound file.  This prevents the edits from being easily undone. \textit{Non-destructive editing}
occurs when the original audio components are retained and can be re-used to make a change to the edit. DAWs can
perform non-destructive editing by saving an \textit{edit decision list}, or \textit{EDL}, which records the positions
of the edits, but does change any audio files. With EDLs, audio edits can be moved or undone retrospectively. Only when
the final edit is ready does the audio get destructively ``rendered'' or ``bounced'' to an audio file.

\subsection{Visual representation}\label{sec:background-daw-visual}

Digital audio editing is performed using a visual representation on a computer screen \citep{Derry2002,Hausman2012}.
\citet{Barbour2004} found through observation and interviews with radio producers that ``visualization of audio on a
screen has become the dominant focus in a radio production studio'', and that visual representations are used to
assemble, modify and balance the audio for radio programmes.

Using visual means to interact with audio has a number of benefits. It allows users to manipulate the audio using a
mouse and screen, which are commonly used in computing.  Mapping audio to an image allows the temporal information of
the sound to be displayed spatially, which means it can be searched and skimmed quickly and randomly.  However,
visualizing audio is difficult, and there are limitations to what audio visualizations can tell us about the audio.

\citet{Bouamrane2007} argue that ``visually representing audio in a meaningful manner is a particularly difficult
task as there is no obvious or intuitive way of doing so''.  Currently visual representations cannot fully represent
the sound, so producers must listen to comprehend the audio.  \citet[p. 45]{McLeish2015} argue that ``while it is
tempting to edit visually using the waveform on the screen, it is essential to listen carefully to the sound, [such as
to] distinguish between an end-of-sentence breath and a mid-sentence breath''.  Visual representations may also serve
as a distraction to the producer.  \citet{Barbour2004} found that to concentrate on listening, radio producers
disengaged their visual senses by shutting or de-focusing their eyes, or looking away.

Although we could not find any studies that surveyed the use of visualizations in DAWs, we looked at the five most
popular DAWs \citep{AskAudio2015} and found that all of them visualized the audio using a ``waveform''.








\subsubsection{Waveforms}\label{sec:background-waveforms}


An \textit{audio waveform} is a common graphical representation of an audio signal that is produced by plotting the
amplitude of an audio signal over time.
Audio signals are periodic, as sound is produced through compression and rarefaction. This can be seen from the
repeating curved lines of the waveform.  Lines that are closer together represent higher pitch sounds and lines that
are farther apart represent a lower pitch.  The height of a waveform corresponds to the amplitude, or ``volume'', of
the audio.


Waveforms have been used to visually represent audio content since the first digital audio workstations started to
appear \citep{Ingebretsen1982}.  Today, they are the default audio visualization used in the DAWs we surveyed.
The simplicity of the waveform makes it conceptually easy for users to understand and interpret the audio.  Waveforms
are relatively compact, so can be arranged vertically on top of each other to view multiple audio tracks
simultaneously. They are also computationally efficient to generate, as they are plotted in the time domain.

\begin{figure}[h]
  \centering
  \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=.9\textwidth,height=.2\textwidth]{figs/waveform-zoomin}
    \caption{Zoomed-in waveform, showing 250ms. The frequency information is visible.}
    \label{fig:waveform-zoomin}
  \end{subfigure}
  \\
  \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=.9\textwidth,height=.2\textwidth]{figs/waveform-zoomout.png}
    \caption{Zoomed-out waveform, showing 2500ms. The frequency information is not visible.}
    \label{fig:waveform-zoomout}
  \end{subfigure}
  \caption{Example audio waveforms of speech, demonstrating the effect of zoom on the visibility of frequency
  information.}
  \label{fig:waveforms}
\end{figure}

Despite their widespread use, waveforms display relatively little information about the audio.
Figure~\ref{fig:waveform-zoomin} shows a waveform that has been zoomed-in. At this scale, we can see the individual
cycles of the audio signal, and the mix of frequencies that make up the sound.  However, when we zoom out, these
curves are compressed to the point where they are no longer visible.  Figure~\ref{fig:waveform-zoomout} shows a
waveform at a zoom level typical in audio production. At this scale, it is impossible to determine which frequencies
are present.  What remains is an ``amplitude envelope'' that indicates the volume of the sound over time.

Without frequency content, there is a limit to the amount of information waveforms can convey.  The amplitude envelope 
can be used to identify silences, peaks and the relative volume of different parts of the audio.  With experience, it
is possible to use the amplitude envelope to distinguish different types of sounds. For example, the frequent short
periods of silence in Figure~\ref{fig:waveform-zoomout} indicate that this may be speech, because unlike music, speech
is broken up into words.

In order to be able to infer this information, users must learn what the amplitude envelope of different sounds look
like. This would be a problem for novice producers, but not for professionals who work with audio on a daily basis.
However, the level of information that can be inferred is limited \citep[p.~114]{Hausman2012}. For example, it is very
difficult to use a waveform to distinguish editorially relevant features, such as individual people's voices, or different styles of
music.

We are interested in learning how audio waveforms affect the performance of audio editing tasks. However, despite the
widespread use of waveforms to visualize audio, we could not find any studies that have attempted to evaluate their
performance as a method of interacting with audio.

\subsubsection{Spectrograms}\label{sec:background-spectrograms}

A \textit{spectrogram} is a plot of the intensity of the Short-Time Fourier Transform \citep{Smith2007}, which visually
represents of the spectrum of frequencies in an audio signal over time. Higher frequencies are displayed at the top of
a spectrogram, and the intensity of the signal is mapped to the brightness (or sometimes colour) of the image.
Figure~\ref{fig:spectrogram-example} shows an example spectrogram of a speech recording.


Spectrograms clearly display the frequencies that make up the sound, and in what proportions.  With spectrograms, time
and frequency can be scaled independently. Unlike waveforms, when a spectrogram is viewed at different zoom levels, the
frequency information is still visible.  Spectrograms are based on frequency analysis, so they are more computationally
expensive to generate than waveforms, but this is rarely an issue with modern processors.

Like waveforms, spectrograms are general-purpose, so can be used for a variety of tasks and applications.  Spectrograms
display a much higher density of information than waveforms, which can be used to infer more information. For
example, \citet{Zue1979,Zue1986} found that expert users were able to use spectrograms to read individual phonemes of speech,
but inexperienced users were unable to achieve this.  Although spectrograms present the data clearly, users
must still learn how to read the information.  

Reading spectrograms requires users to have a theoretical understand of audio frequencies and how they behave, such as
how a single pitch can be composed of many harmonics.  Although spectrograms display the intensity of the signal in
each frequency band, it is not apparent what the overall volume of the audio is at a given time.  Additionally,
spectrograms have a wide range of parameters that control how they are displayed, including FFT window size and shape,
linear/non-linear frequency and intensity scaling, min/max values and colour mapping.  This creates inconsistencies
between different spectrograms, which can make it difficult for users to move between software.  Waveforms don't have
as many parameters, so are much more consistent.

In Section~\ref{sec:background-visualization}, we will show how waveforms and spectrograms can be enhanced using
semantic audio features, but first we will introduce the relevant methods and applications of semantic audio analysis.

\begin{figure}[h]
\centering
  \centering
  \includegraphics[width=.9\textwidth]{figs/spectrogram-example}
  \caption{An example audio spectrogram of speech.} 
  \label{fig:spectrogram-example}
\end{figure}









\clearpage
\section{Semantic audio analysis}\label{sec:background-semantic}

Semantic audio analysis is the extraction of descriptive and perceptual attributes from an audio signal, which can be
used to describe sound in human-readable terms.  Semantic audio can make sound recordings less ``opaque'' by allowing
users to understand what is contained in the audio without having to listen to it first.  This approach can be applied
to the improvement of audio production interfaces. For example, \citet{Fazekas2007} enhanced a DAW to assist music
producers in navigating and editing their content by automatically segmenting music into verses and choruses.  We are
interested in how semantic audio analysis can be applied to user interfaces for the purpose of assisting the production
of radio.

In this section, we will provide an overview of methods and applications of semantic audio analysis.  Semantic audio
brings together a wide variety of disciplines, including speech recognition, information retrieval, audio analysis,
signal processing, psychoacoustics, and machine learning \citep{Foote1999}. As such, we will only aim to provide a
brief overview of selected methods and applications that are relevant to the technology used and the systems developed
in this thesis.  As the focus of our research is on the pre-production of speech programmes, we will only cover methods
and applications related to speech content, which notably excludes the active field of music information retrieval
\citep{Downie2008}.

\subsection{Semantic audio features}\label{sec:features}

Semantic audio analysis is conducted by processing the audio using an algorithm to extract one or more semantically
relevant ``features''. This process known as \textit{feature extraction}.  Audio features are numerical representations
of certain properties of the audio, which are often categorised into low-level and high-level features
\citep[p.~31]{Fazekas2012}.  Low-level features include physical and perceptual properties, such as the energy and
spectral content of the sound.  High-level audio features correspond to more meaningful concepts, such as words and
people, or structural segments, such as programmes or topics.  Many semantic audio algorithms use classification or
machine learning to map low-level features into high-level features. For example, in speech recognition, a
language model is used to map individual phonemes of speech into words and sentences \citep{Junqua1995}.









There are many different types of audio features that can be extracted. With music, rhythmic features are used to
extract the beats and tempo, and harmonic features are used to determine the notes and chords.  Speech is in some ways
a more complex signal to analyse, so more generic features are often used.  In this section, we will describe selected
audio features that are touched upon later in this thesis, to help illuminate the reader's understanding of their
origin.  Below we have outlined three types: energy, temporal and spectral features.










\subsubsection{Energy features}

Energy features are based on the energy of the audio signal, and how it changes over time.  Similarly to audio
waveforms, energy features can be used to infer certain properties of the sound, such as whether it is likely to be
music or speech.  Calculating energy features is often computationally efficient, which makes them attractive for use
in real-time applications, or on large data sets.

A simple and popular low-level energy feature is \textit{root mean square} (RMS), which is calculated as the square
root of the mean square of the audio signal (see Equation~\ref{eq:background-rms}). RMS is commonly used in scientific
work as a measurement of a signalâ€™s power.  The statistics of an audio signal's RMS value can be used as an effective
classifier of music and speech, as demonstrated by \citet{Ericsson2009} and \citet{Panagiotakis2005}.

\begin{align}
  x_{rms} &= \sqrt{\frac{1}{N} \displaystyle\sum\limits_{i=0}^{N} x_i^2}\label{eq:background-rms}
\end{align}

where $x_i$ are the audio samples and $N$ is the frame size.

RMS is also used as the basis for other features.  \textit{Low energy ratio} (also known as ``silent interval
frequency'', ``silence ratio'' or ``energy contour dip'') is a measure of the number of RMS values in a moving window
that fall below a threshold \citep{Liang2005}. It is used for speech/music discrimination (SMD), and works by
exploiting the fact that speech has frequent silent gaps between words, whereas music does not. The threshold can be
set as a fixed value \citep{Liang2005}, a function of a moving average \citep{Ericsson2009} or moving peak value
\citep{Saunders1996}.

\subsubsection{Temporal features}\label{sec:background-temporal}

Temporal features are based on statistics of the audio samples. These statistics are calculated in the time domain, so
like energy features, temporal features are computationally efficient.  A popular temporal feature is
\textit{zero-crossing rate} (ZCR), which is the rate at which a signal crosses the time axis \citep[p. 37]{Zhang2001}.
ZCR can be used as a crude measure of pitch, or distribution of spectral energy.

Early work in SMD \citep{Saunders1996} identified that ``speech signals produce a marked rise in the ZCR during periods
of fricativity occurring at the beginning and end of words'', whereas music does not. This causes a bimodality in the
distribution of the ZCR, which can be detected by measuring its ``skewness''.
\citet{Panagiotakis2005} also found that ``RMS and ZCR are somewhat correlated for speech signals, while essentially
independent for music'', and so the product of RMS and ZCR can also be used as a SMD classifier.



\subsubsection{Spectral features}\label{sec:background-spectral}

Spectral features decompose the audio signal into individual frequency bands to analyse the frequencies that are present in the
signal, and in what proportion.  This is commonly performed using a fast Fourier transform \citep{Smith2007}.

\textit{Spectral centroid} \citep{Smaragdis2009} is a measure of the ``centre of mass'' of the spectrum, calculated as the mean of the audio
frequencies, weighted by the magnitude of each frequency bin (see Equation~\ref{eq:background-centroid}). Audio that
has more higher
frequencies than lower frequencies has a higher spectral centroid value, and vice-versa.  Spectral centroid is a good
predictor of the perceived ``brightness'' of the audio, which can be used to distinguish sounds of different timbre
\citep{Schubert2004}.

\begin{align}
  s_{centroid} &= \frac{
  \sum_{n=0}^{N-1}
    f(n)
    x(n)
} {
  \sum_{n=0}^{N-1}
    x(n)
  }\label{eq:background-centroid}
\end{align}

where $x(n)$ is the magnitude and $f(n)$ is the centre frequency of bin $n$.

The \textit{cepstrum} of a signal is the power spectrum of the log of its power spectrum \citep{Noll1967}. The cepstrum
is a compact representation of how the frequencies in a signal change over time.  The Mel-frequency cepstrum is
calculated by spacing the frequency bands using the Mel scale \citep{Stevens1937}, which gives a better approximation
to the human hearing system. The audio features produced through this process are called \textit{Mel-frequency Cepstral
Coefficients}, or \textit{MFCCs} \citep{Imai1983}.
MFCCs are commonly used as a speech analysis tool, and have been successfully applied to SMD
\citep{Liang2005,Pikrakis2008,Pikrakis2006a,Sell2014,Wieser2014} and speaker segmentation
\citep{AngueraMiro2012,Friedland2009}, as well as many other problems.

Now that we have a general understanding of some common semantic audio features, we will see how they have been used for
applications related to radio production.







\subsection{Applications}

Semantic audio analysis allows us to gain insights into the content of audio recordings without having to listen to
them. The semantic audio features we described have already been used to tackle a variety of problems
\citep{Foote1999}.  In this section, we outline the aim, methods and performance of three applications of semantic
audio analysis that are used later in this thesis: speech/music discrimination, speaker diarization and automatic
speech recognition.



\subsubsection{Speech/music discrimination}

\textit{Speech/music discrimination} (SMD) is the task of segmenting and labelling audio content into sections of
either music or speech.  Many SMD systems have been specifically developed for use with radio broadcasts
\citep{Saunders1996,Pikrakis2006,Pikrakis2008,Ericsson2009,Wieser2014} and television broadcasts
\citep{Seyerlehner2007,Sell2014}.  SMD systems have been successfully implemented using a variety of different
features, including low energy ratio \citep{Ericsson2009}, ZCR skewness \citep{Saunders1996}, spectral entropy
\citep{Pikrakis2006}, continuous frequency activation \citep{Seyerlehner2007,Wieser2014}, chromagrams \citep{Sell2014}
and MFCCs \citep{Pikrakis2008}.  \citet{Carey1999} compares the performance of some common SMD audio features.

Most SMD systems report high accuracy figures of 96\% and above, which shows that automatic SMD is likely to be useful
in real-life applications.  However, as \citet{Pikrakis2008} argues, each system is evaluated using different data sets
that are inconsistent in content and length, which makes it difficult to compare them.

\citet{Wieser2014} showed that by including a ``human in the loop'', the accuracy of their SMD increased from 96.6\% to
100\%. They achieved this by adding a user-adjustable slider to their interface that controlled the detection
threshold. When the user adjusted the slider, they could see the effect on the segmentation directly to help them find
the correct setting.

\subsubsection{Speaker diarization}\label{sec:background-diarization}
\textit{Speaker diarization} is the task of segmenting an audio recording into labelled segments that identify ``who
spoke when'' \citep{AngueraMiro2012}. With this task, the location of any speech content and number of speakers is
usually unknown. Speaker diarization has clear applications to the production of radio, where there are often multiple
people speaking in a single recording, and it is desirable to know where they are speaking without having to listen.

Review papers from \citet{Tranter2006} and \citet{AngueraMiro2012} show that the vast majority of speaker diarization
systems are based on clustering of MFCCs, and that current research is focused on the improvement of clustering
algorithms and pre-processing stages, rather than audio features. They also show that most of the recent research has
focused on recordings of meetings, rather than broadcast content.



\citet{AngueraMiro2012} found that the average error rate for speaker diarization systems was 11.6\% and 17.7\% for two
standard data sets \citep{NIST2016}. However, these data sets are based on microphone recordings of meetings, rather
than broadcast content.  \citet{Bell2015} conducted an evaluation of speaker diarization systems on television
recordings of multiple genres.  These results showed that the error rate was 47.5\%, which is considerably higher.
However, rather than just trying to match speakers within individual recordings, their evaluation was conducted across
multiple recordings, which made matching speakers between them all more difficult.  A breakdown of the results
showed that most of the errors were misidentification of speakers, and that misidentification of speech accounted for
less than 8\% of the error rate.

Speaker diarization systems assign a unique identity to each speaker, but they do not attempt to identify who the
speaker is. \textit{Speaker recognition} is the task of identifying a person based on the sound of their voice
\citep{Doddington1985,Lee1999a}. Extracting metadata such as participant names and genders from radio content could be
used to enable automated information searching and indexing \citep{Kinnunen2010}. Speaker recognition relies on access
to a database of trained speaker models, which represent people's voices. In radio, many of the contributors are from a
small pool of presenters, so it may be feasible to use speaker recognition techniques to detect their voices with
sufficient accuracy.

\citet{Raimond2014} introduced the \textit{BBC World Service Archive prototype}, which was an interface that used
automatic keyword tagging and crowd-sourcing to support the search and discovery of a large radio archive.
The interface used speaker diarization and speaker recognition to help users navigate within individual radio
programmes. Figure~\ref{fig:bg-world-service-archive} shows an example of a radio programme that has been segmented
into five named speakers.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{figs/world-service-archive.jpg}
  \caption{Speaker diarization and recognition interface in the BBC World Service Archive prototype, from
  \citet{Raimond2014}}
  \label{fig:bg-world-service-archive}
\end{figure}



\subsubsection{Automatic speech recognition}\label{sec:asr}

\textit{Automatic speech recognition} (ASR) can be used to automatically convert speech to text.  The ability to
convert audio signals to text opens up many possibilities in radio production, such as being able to navigate audio
recordings through searching and skimming.  These opportunities are discussed in greater detail in
Section~\ref{sec:background-transcripts}.

Modern ASR systems can be broken down into two main stages \citep{Junqua1995}. The first stage uses an acoustic model
to map the audio to a set of \textit{phonemes}, which are the individual noises that make up the speech. In the second
stage, a language model converts the sequence of phonemes into words and sentences. Both the acoustic and language
models are developed using machine learning techniques to train the system based on recordings and transcripts of
speech.  As such, the success of an ASR system depends on the quality and fitness of the data that it is trained on.

Despite advances in the field \citep{Lee1999a}, ASR produces erroneous transcripts.  \citet{Bell2015} conducted an
evaluation of ASR systems on television programmes of various genres. Each system was judged by the proportion of
incorrect words, known as the ``word error rate'' (WER).  The mean average WER of the systems tested was 23.7\%,
however the variance across programme genres was high, with the WER varying from 10 -- 41\% across the 16 genres tested.

Figure~\ref{fig:asr-example} shows an example of a transcript generated by an ASR system with a WER of approximately
16\%.  ASR transcripts don't include letter capitalisation or punctuation, but this can be estimated and added using
post-processing \citep{Gravano2009}.

\newcommand{\low}[1]{\textcolor{gray}{#1}}
\begin{figure}[h]
  \centering
  \setlength\fboxsep{5pt}
  \fbox{\parbox[t][][c]{.9\textwidth}{\small\sffamily\setlength{\parskip}{1em}
    [Speaker 1] the manchurian candidate both \low{seems} to play up these fears and to be in a way \low{in} she comes
    to \low{sudden he can have} a critique of the idea of the moral panic around brainwashing i wondered where pavlov
    fits into that story and how seriously \low{are} his \low{ideas} taken in the literature of the nineteen fifties
    around brainwashing
    
    [Speaker 2] \low{we'll} have \low{a viz} is everywhere in in the discussion of the american p.o.w.s \low{they're}
    sometimes referred to \low{in} magazine articles and in popular commentary at the \low{time as} as prisoners of
    pavlov so there was a larger of \low{of of our} popular discussion about pavlov often not very well informed
    \low{but only rouge to} his experiments with the conditioned reflex \low{and} his famous salivating dogs and
    \low{ringing bow and} so on \low{that} was was everywhere so certainly many americans would have at associated some
    kind of pavlovian conditioning with what had been done to the p.o.w.s but but it wasn't generally carried very far
    into in terms of actually trying to \low{him} better understand how pavlovian principles or psychology might might
    actually have been at work in the p.o.w. camps
  }}
  \caption[Example automatic speech recognition transcript of a radio interview clip, with an approximate 16\% word
  error rate.]{Example automatic speech recognition transcript of a radio interview clip, with an approximate 16\% word
    error rate. Speaker diarization is used to segment the transcript (see Section~\ref{sec:background-diarization}),
    and confidence shading is used to shade words with a low ASR confidence rating (see
  Section~\ref{sec:background-confidence}).}
  \label{fig:asr-example}
\end{figure}



\clearpage
\section{Audio visualization}\label{sec:background-visualization}
In the last section, we explored how semantic audio analysis can be used to extract information from audio, but did not
discuss how such information is presented to the user.  As we shall see in this section, semantic information can be
used to support interaction with audio recordings by using it to enhance audio visualizations.

Audio visualization is the task of mapping an audio signal to an image. The human visual system is capable of viewing
an entire image at once, and is adept at searching and skimming images \citep{Wolfe2004}. On the other hand, sound must
be consumed serially and over a period of time. Mapping sound to vision allows temporal information to be displayed
spatially, which can overcome some of the limitations of a time-based medium like sound.

We saw in Section~\ref{sec:background-daw} that audio visualization is already used by DAWs to help users navigate and
edit audio content.  However, we also saw that current audio visualizations are limited in what they can display.  For
example, waveforms only display amplitude information, much of which cannot be seen at typical zoom levels.  To
effectively navigate audio waveforms, users must read the shape of the visualization.



In this section, we will see how previous research has proposed a number of enhancements to current audio
visualizations that aim to improve their performance.  We start by looking at the relationship between sound and
vision, and considering the perceptual mappings between the two that already exist. We then review techniques that have
previously been used to process or enhance waveforms and spectrograms to make it easier for users to navigate and edit
audio recordings.













\subsection{Crossmodality}\label{sec:crossmodality}
To be able to represent audio visually, we must map auditory properties to visual properties.  When attempting to link
sound and vision, it is desirable to create a mapping that is coherent and makes sense to the user.  By creating an
audio visualisation that ``looks likes it sounds'', it might be possible for users to comprehend the sound without
having to listen to it.

\textit{Crossmodal perception} is a term used to describe interaction between the different senses \citep{Spence2011}.
Previous work has shown that there are perceptual mappings between auditory and visual stimuli that are experienced by
most of the population. These could be exploited to aid the navigation and editing of audio recordings.

The ``bouba/kiki effect'' is a demonstration of crossmodal mapping between speech sounds and the visual shape of
objects, originally discovered in an experiment by \citet{Koehler1929}.  Participants were shown two abstract shapes,
shown in Figure~\ref{fig:boubakiki}, and asked which shape was called ``bouba'' and which was called
``kiki''\footnote{K\"ohler used the words ``baluma'' and ``takete'' in the original experiment, but the result was the
same.}.  \citet{Ramachandran2001} found that 95--98\% of the population gave the same
answer\footnote{\label{bouba-kiki-answer}The vast majority of participants chose to name the curvy, rounded shape on
the left ``bouba'', and the sharp, pointy shape on the right ``kiki''.}. This is an example of just one audio-visual
mapping that is common amongst the population.

\begin{figure}[h]
\centering
  \centering
  \includegraphics[width=0.8\textwidth]{figs/bouba-kiki}
  \caption[Demonstration of the ``bouba/kiki effect'' --- an example of crossmodal perception.] {Demonstration of the
  ``bouba/kiki effect'' --- an example of crossmodal perception.  \citet{Ramachandran2001} found that 95--98\% of the
population assigned the names ``bouba'' and ``kiki'' to these shapes in same order. See footnote
\ref{bouba-kiki-answer} on page \pageref{bouba-kiki-answer} for answer.}
  \label{fig:boubakiki}
\end{figure}




\citet{Spence2011} presented a review of psychology experiments that attempted to find crossmodal links in the human
brain, including audio-visual mapping.  He found that there was strong evidence for five audio-visual mappings, shown in
Table~\ref{tab:crossmodal}.
These findings were supported by \citet{Tsiros2014}, who attempted to generate images to match different sounds, and
measured their success through a user study. In addition to confirming the strong links between loudness/size and
pitch/elevation, he found weaker links for pitch/colour, dissonance/granularity, and dissonance/colour complexity.

\begin{table}[h]
\centering
\begin{tabular}{l l}
\hline
\textbf{Link} & \textbf{Direction} \\
\hline
Loudness/brightness  & louder=brighter \\
Pitch/elevation & higher=higher \\
Pitch/size & higher=smaller \\
Loudness/size & louder=bigger \\
Pitch/spatial frequency & higher=higher \\
\hline
\end{tabular}
\caption{Audio-visual mappings supported by strong evidence, from \citet{Spence2011}.}
\label{tab:crossmodal}
\end{table}









Current audio visualizations exploit some of these crossmodal mappings. For example, waveforms map loudness to size,
and spectrograms map loudness to brightness, and pitch to elevation. However, this previous work shows that there are
many more links between sound and vision that could be further exploited by audio visualizations.

\subsection{Waveforms}\label{sec:background-waveforms-related}

As we discussed in Section~\ref{sec:background-waveforms}, the audio waveform is commonly used by DAWs as a
visualization of an audio signal \citep{Derry2002}.  As such, many users are familiar with navigating audio content
using waveforms, and have learned how to read the shapes of the waveform.
Enhancing a waveform, either by processing it or adding additional information to it, could allow users to
navigate and edit audio content more efficiently whilst retaining this familiarity, and using the skills they have
developed. Our survey of the literature found that two main approaches have been used to enhance waveforms --- scaling
and colour.

\subsubsection{Scaling}
When an audio waveform is zoomed out, the curves of the waveform are compressed which can make it difficult to read.
This affects both horizontal zoom (on the time axis) and vertical zoom.
One very simple technique for improving waveform readability is to automatically scale the vertical zoom to match what
is visible on the horizontal timeline.  However, if the scale of the waveform constantly shifts, there is no reference
level by which to compare the amplitude of the audio.  The solution proposed by \citet[p. 39]{Goudeseune2012} was to
overlaying a dimmed version of the scaled waveform on top of the normal waveform. This allowed users to simultaneously
judge the overall amplitude whilst being able to see the detail of the amplitude envelope.

Frequency information is useful for understanding the timbre of an audio signal. When viewed at the right scale, this
information is visible in a waveform, but at typical zoom levels, this information is lost. \citet{Loviscach2011}
proposed a novel solution to this problem called the \textit{quintessence waveform}. This approach used extreme pitch
shifting so that the individual cycles of the audio waveform are visible, even at different scales.  This works well
for repeating monoaural sounds --- for example, a sine wave would be identifiable as a sine wave at every zoom level.
However, typical real-life applications use complex polyphonic audio, which would not benefit from quintessence
waveforms as there is no repeating signal to display.


\begin{figure}[t]
  \centering
  \includegraphics[width=.8\textwidth]{figs/Gohlke2010.png}
  \caption{Lens view for magnifying an audio waveform at the current playhead position, from \citet{Gohlke2010}.
  Republished with permission.}
  \label{fig:Gohlke2010}
\end{figure}


\citet{Gohlke2010} proposed five novel ideas on how to improve multi-track DAW displays, including techniques for
saving screen space by overlaying and stacking waveforms. One of these proposals was for a lens-like view, shown in
Figure~\ref{fig:Gohlke2010}, which magnified the area of the waveform around the current playhead position. This
allowed users to simultaneously view the waveform at two different scales --- an overview of the audio waveform and a
detailed local view. This technique has the potential to display frequency information in regions of interest, and help
make more precise audio edits without having to adjust the overall zoom level.


\subsubsection{Colour}
The use of colour is a simple and effective way of adding additional information to a waveform.  However, many DAWs
only use waveform colour to allow users to label audio clips, and most others have monochromatic waveforms.  Previous
research has experimented with mapping semantic audio features to colour, using either pseudocolour or false colour.

\textit{Pseudocolour} is a method of mapping a scalar value to a colour gradient \citep{Moreland2009}, an example of
which can be seen on thermal imaging cameras.  Colour gradients are composed of at least two colours (e.g. blue to red)
or a spectrum of colours (e.g. a rainbow).  Pseudocolour allows values to be mapped to colours that might be
perceptually relevant (e.g. green/red for good/bad).  It can emphasise small variations between values by using a full
spectrum, pick out high/low values using non-linear gradients, or categorise values using stepped gradients.  However,
as pseudocolour can only represent one dimension, it does not make full use of the available colour space.

\textit{False colour} exploits the tristimulus theory of vision to map three values to the dimensions of a colour space
\citep{Moreland2009}. Commonly, values are mapped to red/green/blue (RGB) colour space. Other colour spaces can be
used, such as hue, saturation, value (HSV), which better matches human perception of colour \citep{Smith1978}.
\textit{Hue} can be described as ``the colour on a rainbow'', \textit{saturation} represents lack of greyness, and
\textit{value} means brightness. The advantage of false colour is that it can make full use of the available colours.
On the other hand, it can be challenging to select three values and map them to colour in a way that is perceptually
relevant and understandable.

\citet{Rice2005} presented \textit{Comparisonics} --- a patented \citep{Rice2001a} method of using pseudocolour to map
the frequency content of an audio signal to a colour spectrum.  Comparisonics was designed for identifying timbrally
distinct sounds and he claims that, with training, it can be used to identify certain sound effects.  His technique
maps frequency to colour using an unpublished algorithm, where low frequencies are blue and high frequencies are red.
Comparisonics has since been integrated into the \textit{Scratch LIVE} DJ software from Serato Audio Research, where it
is used to distinguish between different drum noises, such as bass kicks, snares and high-hats. However, the author
could not find any formal evaluation of Comparisonics.

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{figs/freesound2.png}
  \caption{An audio waveform colourised by using pseudocolour to map the spectral centroid of the audio
  to a rainbow colour gradient.}
  \label{fig:pseudocolour-example}
\end{figure}

\citet{Akkermans2011} implemented a similar system in the audio clip sharing website \textit{Freesound} to help users
quickly find and compare sound effects and music clips.  They used pseudocolour to map the spectral centroid of the
audio (see Section~\ref{sec:background-spectral}) to a rainbow colour gradient. This colours lower frequency sounds
blue and higher frequency sounds red, matching the effect seen in \citet{Rice2005}.
An example of this approach is shown in Figure~\ref{fig:pseudocolour-example}.
\citet{Loviscach2011a} used pseudocolour to enhance the navigation of speech in a video editor by distinguishing
different phonemes of speech.  This was achieved by mapping the zero-crossing rate of the audio
(see Section~\ref{sec:background-temporal}) to a rainbow colour spectrum.
The author could not find any studies that attempted to evaluate these approaches.

\citet{Tzanetakis2000} used false colour to design a visualisation technique known as \textit{Timbregrams}. Their aim
was to ``use colour perception and the pattern recognition capabilities of the human visual system to depict timbral
and temporal information''.  Their implementation extracted a large vector of common audio features, then used
principal component analysis to reduce the size of the vector.  They mapped the first three principal components, which
contained 80\% of the variance in their data, to RGB or HSV colour space.  They found that the RGB colour space was
more uniform and aesthetically pleasing, but that the HSV colour space had better contrast at segmentation boundaries.
When using RGB, speech, classical music and rock could easily be distinguished as they appeared as light green, dark
blue and dark green, respectively. Tibregrams were later used to colour a waveform in a basic audio editor \citep[p.
253]{Tzanetakis2001}, but the author could not find any formal evaluation of Timbregrams.


\citet{Mason2007} used false colour to assist radio listeners in navigating recently-broadcast material. They mapped
three empirically-chosen audio features to RGB colour space. The authors reported that the system was successful at
indicating the location of music within speech content, and highlighting low-bandwidth material such as phone calls.
However, this was not formally evaluated.  The authors proposed that the system could be also be applied to other
applications such as segmentation of radio programmes for re-editing into podcasts.  Figure~\ref{fig:Mason2007} shows
an example of this approach for a BBC radio programme that contains five segments.  Although the segments are not
visible in the waveform, the false colour visualization displays the voice of the female presenter in a lighter colour,
which makes the segments visible.




\begin{figure}[p]
  \centering
  \begin{subfigure}[b]{0.8\textwidth}
      \includegraphics[width=\textwidth]{figs/fooc-waveform.png}
      \caption{Waveform}
      \label{fig:Mason2007-waveform}
  \end{subfigure}

  \begin{subfigure}[b]{0.8\textwidth}
      \includegraphics[width=\textwidth]{figs/fooc-false.png}
      \caption{False colour}
      \label{fig:Mason2007-false}
  \end{subfigure}
  \caption[False colour audio visualization of an episode of the BBC radio programme ``From Our Own Correspondent'',
  from \citet{Mason2007}.]{False colour audio visualization of an episode of the BBC radio programme ``From Our Own
    Correspondent'', from \citet{Mason2007}.  The location of the five segments of the programme can be seen in the
    false colour visualization, but not the waveform. Republished with permission.}
  \label{fig:Mason2007}
\end{figure}

\begin{figure}[p]
  \centering
  \includegraphics[width=.8\textwidth]{figs/Lin2013.png}
  \caption{Comparison of a normal spectrogram (top) and a saliency-maximised spectrogram (bottom), from
  \citet{Lin2013}. Republished with permission.}
  \label{fig:Lin2013}
\end{figure}


\subsection{Spectrograms}




As we discussed in Section~\ref{sec:background-spectrograms}, spectrograms are an information-rich representation of
the spectrum of frequencies in an audio signal over time, but they can be difficult for novice users to read.
\citet{Lin2012} introduced a method of filtering spectrograms to visually emphasise non-speech events in long audio
recordings. The filtering was done using an ``image saliency algorithm'' that detected differences in the intensity and
orientation of the spectrogram. This \textit{saliency-maximised spectrogram} was integrated into an audio
navigation interface called \textit{Timeliner} \citep{Goudeseune2012}, which displayed the spectrogram alongside a
waveform. \citet{Lin2013} describes an evaluation in which 12 novice participants used Timeliner to find sound effects
hidden in meeting room recordings using both saliency-maximised and normal spectrograms. The results show that
saliency-maximised spectrograms significantly outperformed normal spectrograms.  Filtering spectrograms shows promise as
a way of detecting unusual events, however it is unclear how useful this sort of application would be in the context of
radio production.























\clearpage
\section{Semantic speech interfaces}\label{sec:background-transcripts}

Speech recordings can be converted to text in a process known as ``transcription''. Transcripts can be used to record
exactly what somebody said, and the transcript text can be read, copied, shared, skimmed and searched using a variety
of tools, such as word processors, or on paper.  \citet[p.~133]{Hausman2012} notes that radio producers currently
``cut, paste and copy sound files much the same way we use a word processor to manipulate words, sentences and
paragraphs''.  In this section, we will see how transcripts can be used as an interface to aid the navigation and
editing of speech recordings.

\subsection{Transcript generation}\label{sec:transcript-generation}
Transcripts can be written manually, either using pen and paper or a word processor,
but this is a slow and tedious process. Transcription can be completed faster by only transcribing the most salient
words, but this makes the transcript much less readable, particularly to others who haven't heard the original
recording.  Alternatively, a third-party can be used to transcribe the speech, but this slow and expensive.  For
example, transcribing speech using \texttt{rev.com} currently costs US\$1 per minute and takes 12
hours\footnote{\url{https://www.rev.com/transcription}, accessed 11/12/2017}.

As we saw in Section~\ref{sec:asr}, ASR can be used to convert speech to text automatically.  ASR is quicker and
cheaper than manual transcription.  ASR also produces accurate timestamps for each word, which can be used to precisely
navigate and edit the audio, but word-level timestamps can also be added to manually-written transcripts using speech
alignment \citep{Griggs2007,Bohac2013}.

Erroneous transcripts reduce listener comprehension \citep{Stark2000,Vemuri2004} and increase the time it takes to
search audio content \citep{Ranjan2006} and correct errors \citep{Burke2006}.  However, despite the errors in ASR
transcripts, they provide a highly effective tool for browsing audio content as users can visually scan the text to
focus on regions of interest, known as ``strategic fixation'' \citep{Whittaker2007}.






\subsection{Transcript navigation}
Transcripts have previously been used by several systems as an interface for improving the navigation of speech-based
content, such as news reports and voicemail messages. 
One of the first such systems was \textit{NewsTime} from \citet{Horner1993}, which used transcripts to aid the
navigation of audio news stories.  For television news, subtitles were aligned the audio to provide an accurate 
transcript with word timings.  NewsTime included several additional features including searching by keyword, segmenting
the transcript by story and speaker, jumping to the next or previous speaker/story, and categorising stories into one
of seven common topics. There were no reported user studies of NewsTime.

\textit{SCAN} \citep{Whittaker1999} was an interface designed to support retrieval from speech archives. It used ASR
transcripts to allow users to search for keywords and visually search the recording by reading the transcript.  In a
user study of 12 participants, the transcript was found to support navigation by reducing the listening time needed to
complete information retrieval tasks. Participants rated the tasks as being easier, and the browser as being more
useful, with the transcript than without.  SCAN was further developed into \textit{SCANMail} \citep{Whittaker2002}, an
interface designed for interacting with voicemail messages.  It added a number of features including paragraph
segmentation, and the ability to seek to a point in the audio recording by clicking on a word in the transcript.
\citet{Whittaker2002} evaluated SCANMail through a study of eight experienced users, which found that the transcript
display enabled them to visually scan the content of recordings to quickly extract information, and to judge which
parts were relevant, without having to play the audio.

\subsection{Semantic speech editing}\label{sec:background-semantic-editing}
In addition to supporting the navigation of speech recordings, transcripts have also been used as a method of editing
speech content, known as \textit{semantic speech editing}. The first of these was the ``Large Interactive Display
System Wave Speech Editor'', catchily shortened as \textit{LIDSWSEdit}, from \citet{Apperley2002}, which used ASR
transcripts to allow users to navigate and edit lecture recordings.  Any edits made to the transcript were
correspondingly applied to the underlying audio recording. Users could re-arrange sentences and words by selecting the
text, and using a drag-and-drop action.  Alternatively, speech could be removed by selecting text then clicking a
button to either delete the selected text, or everything except the selected text.  LIDSWSEdit was further developed
into the ``TRanscription-based Audio EDitor'', or \textit{TRAED} \citep{Masoodian2006}.  TRAED used the same editing
actions as LIDSWSEdit, but rather than displaying the text and audio waveform separately, it displayed the waveform
in-line with the text. Individual words were delineated by drawing boxes around the waveform/word pair. The boundary
between each pair could be adjusted by dragging the boundary edge.  The author could not find any user studies of
LIDSWSEdit or TRAED.

\citet{Whittaker2004} created an interface for editing voicemail messages using ASR transcripts.  Users could
cut-and-paste parts that they wanted, or delete parts they didn't.  They evaluated their system in a formal study of 16
voicemail users, which found that semantic editing was faster and as accurate as editing with a waveform.
Crucially, they found that this was true even though the transcripts had an average word
error rate of 28\%. This suggests that semantic editing is beneficial even when using erroneous transcripts.

\citet{Rubin2013,Rubin2015} presented a novel interface for creating ``audio stories'' that combine speech and music,
which is similar to radio production.  The interface, shown in Figure~\ref{fig:Rubin2013}, used an editable transcript
with two columns, one for each of a pair of speakers.  It allowed the user to cut, copy, paste and delete the audio
using the text, and highlighted repeated words and similar sentences.  The transcripts were generated using an online
service that produced 100\% accurate, or ``verbatim'', transcripts.  As they were manually-generated, the transcripts
also included ``umm''s, breaths and pauses, which were displayed and labelled in the interface. However, the manual
transcripts did not include timestamps, so speech alignment software was used to recover the timestamps for each word.
The system also included additional functionality for finding and adding music tracks, and for varying the length of
music using automatic looping. The system was evaluated through a short informal study of four participants where the
editing capabilities received positive feedback. The author could not find any follow-up studies.

\begin{figure}[h]
\centering
  \centering
  \includegraphics[width=\textwidth]{figs/Rubin2013.png}
  \caption{User interface of a semantic speech editor for creating ``audio stories'', from \citet{Rubin2013}.
  Republished with permission.}
  \label{fig:Rubin2013}
\end{figure}


\citet{Sivaraman2016} created a semantic editing system for asynchronous voice-based discussions, where users could
quickly edit their speech recording before sending it to the recipient.  Their system used near-live ASR
and detected pauses in the speech. Their interface allowed users to delete selected words/pauses, insert
additional pauses and fix incorrect words.  In a formal qualitative study of their system with nine users, they found
that text-based editing was considered good enough to replace waveform editing, and to be more accessible. They
observed that most users only used the system to make fine-grained edits, instead of editing large chunks.  Users said
that the transcript also allowed them to quickly review all the points that were made, and that the errors in the
transcript weren't a heavy distraction.

\citet{Yoon2014} created a collaborative tablet-based document annotation system called \textit{RichReview}, which
offered users three modalities in which to annotate documents --- free-form inking, voice recording and deictic gestures
(i.e. pointing to areas of interest). The voice recordings were displayed using a waveform, overlaid with an ASR
transcript of the speech.  Users could trim or tidy the voice recordings by drawing a line through words or pauses to
remove them.  The system was evaluated using a qualitative study of 12 students which found that the editing features
were considered easy to use and efficient for removing ``umm''s and long pauses.  However many participants reported
that the transcripts were not accurate enough to use without having to listen to the audio.
\citet{Yoon2016} describes two deployment studies that used a similar system called RichReview\textsuperscript{++}, but
they did not report there being any semantic editing functionality.

\subsection{Video editing}
Semantic speech editing has also been used to support video editing.  \textit{SILVER} \citep{Casares2002, Long2003} was
a video editor that aligned words from subtitles to the video, and displayed them in a transcript window.  Gaps, errors
and edits were displayed in the transcript using special characters, such as ``||â€™â€™ for clip boundaries, ``â€”â€˜â€™ for
gaps, and ``*â€™â€™ for noise or recognition errors.  The video could be edited by deleting text in the transcript.  SILVER
was evaluated in an informal study with seven students, but the study did not report any results about the
transcript-based editing feature.

Hyperaudio Pad is an open-source audio and video editor, first proposed by \citet{Boas2011}, and now available online
as a free service \citep{Hyperaudio2016}. This web-based interface, shown in Figure~\ref{fig:Boas2011}, allows users to
navigate and edit online media using transcripts, which are generated from subtitles.
Editing is performed by selecting a part of the transcript and dragging it into a window on the right to create a
``clip''. Clips can be re-ordered, split using a ``trim'' tool, and fade effects can be added between clips. Clips from
different recordings can be mixed together, and the final edited version can be played and shared with others. No user
studies of this system could be found.

\begin{figure}[h]
\centering
  \centering
  \includegraphics[width=.8\textwidth]{figs/hyperaudio-pad-example2.png}
  \caption[User interface of \textit{Hyperaudio Pad} --- a semantic speech editor for video, from
  \citet{Boas2011}.]{User interface of \textit{Hyperaudio Pad} --- a semantic speech editor for video, from
  \citet{Boas2011}.  Drag-and-drop is used to select clips from the left transcript and arrange them on the right
transcript.}
  \label{fig:Boas2011}
\end{figure}


When editing a video interview, it is desirable to avoid making a cut while the person speaking is in shot, because it
causes the image to jump unnaturally.  \citet{Berthouzoz2012} used image processing algorithms to create a video editor
that can help the user hide these edit points. The system had an editable transcript window that displayed suitable
edit points and allowed the user to edit the video by selecting and deleting text. The transcripts were generated
manually using an online crowd-sourcing service, and word timings were added using speech alignment software. The
system also allowed users to easily remove ``umm''s or repeated words as they were explicitly marked in the manual
transcript. No user study was reported, however the system received positive feedback from nine professionals who were
given a demonstration.

\clearpage
\subsection{Pre-written scripts}
The systems so far have only considered transcripts that have been generated from the speech itself. Sometimes
speech is recorded based on a pre-written script, or from notes.  Avid Technology released a feature for their Media
Composer video editing software in 2007 called \textit{ScriptSync} \citep{Avid2011}.  This feature aligns a
user-supplied transcript to a video recording by placing a marker in the video for each line of the transcript
\citep{Griggs2007}. This allows users to jump to a particular line, or see which line in the transcript corresponds to the
current point in the video.  A second version of ScriptSync was launched in February 2017 \citep{Avid2017} which added
script correction and collaborative note-taking.

\citet{Shin2016} created a system called \textit{Voice Script} that supports an integrated workflow for writing
scripts, and recording/editing audio. An informal study with four amateur participants found that it could support
various workflows including multiple iterations. It included a ``master script'' layout to bring together
different recordings,  which was found to work well.  A second study of four amateur participants directly compared
the system to that of \citet{Rubin2013}, which found that participants were able to complete an audio production task
25\% faster using the Voice Script system.  This study demonstrates that for workflows that involve pre-written
scripts, there is potential to improve the audio editing by using an integrated writing and editing system.

\textit{QuickCut} from \citet{Truong2016} was an interface designed to help producers edit a narrated video from a
pre-written script, voiceover audio and raw video footage.  Producers could label their video footage using their
voice, which was manually transcribed using a crowd-sourced online service in combination with speech alignment.
Selecting text in the script also selected the corresponding segment in the voiceover audio, and displayed video clips
labelled with similar words. After selecting an appropriate clip, it could be associated with a position in the script
by using drag-and-drop to add it to the timeline.  The completed timeline could then be exported as an EDL for use in
professional video editing software.  QuickCut was evaluated by the researchers themselves and one professional
filmmaker, who were able to use the system to produce a minute of video in 8--31 minutes, rather than the 2--5 hours
professional filmmakers suggest they require.  Voice-based logging makes sense for logging video footage as it is easy
to watch and talk at the same time. However, for speech content it would be difficult to talk and listen
simultaneously.  The ability to export edits to professional software allows for a smooth continuation of the
production workflow.



\subsection{Transcript correction}

\citet{Whittaker2004} found that users of their semantic speech editing system ``wanted to be able to correct errors
they found in the transcript''. ASR errors reduce listener comprehension \citep{Stark2000,Vemuri2004} and increase the
time it takes to search audio content \citep{Ranjan2006} and correct errors \citep{Burke2006}.


Four of the ASR transcript interfaces mentioned above included correction functionality, with each using a different
method to edit the text.  SILVER \citep{Casares2002} required the user to both type the replacement word and select the
start and end time of the word in the video timeline.  TRAED \citep{Masoodian2006} allowed users to correct a word by
selecting it and typing the replacement. Typing a space created a new word by dividing the time of the original word in
half.  \citet{Sivaraman2016} initially planned to have two editing modes --- one for audio editing and the other for
text editing.  However, in pilot testing they found that having two modes confused users, so they developed a pop-up
box that indicated to the user when they are editing the text, rather than the audio.  SCANMail did not initially
include transcript correction, but this was later added and evaluated by \citet{Burke2006}.  These changes allowed
users to either replace an individual word by selecting a replacement from a drop-down menu, or replace multiple words
by selecting them and typing the replacement.  A user study of 16 participants who corrected voicemail messages found that
compared to typing, selecting the replacement word required the user to listen to less of the audio.

The correction process can be made more efficient by correlating the user's input with contextual information
\citep{Suhm2001}.  In particular, the words immediately before and after the incorrect word can be used to reduce the
number of candidate words, or even estimate the replacement.  \citet{Liang2014} showed that once an incorrect word is
identified, in 30\% of cases the correct word can automatically be inferred using $n$-grams and acoustic features.

Correction is normally a process that happens after ASR transcription, but as \citet{Wald2007} demonstrated, it is
possible to correct ASR transcripts in real-time as the audio is captured. However, during recordings, radio producers
are normally pre-occupied with operating the equipment, asking questions or listening to the answers. As this does not
leave enough space for performing real-time correction, an extra producer would be required, which is costly.

The above systems used a keyboard and mouse interface to correct transcripts. \citet{Suhm2001} tested alternatives
methods that used vocal correction and a pen interface.  They found that for skilled typists, keyboard and mouse input
was faster than the alternatives, but that voice and pen input would be attractive for use by poor typists, or for 
devices that don't allow fast keyboard input.


ASR transcripts contain errors in the text, but sometimes there are errors in the speech itself that producers may want
to correct. \textit{TypeTalker} from \citet{Arawjo2017} was an interface for editing synthesised speech using ASR
transcripts. The speech was not synthesised from text, but from a recording of the user speaking. This was done to
reduce the self-consciousness that results from hearing one's own voice. As well as being able to remove unwanted
speech, the use of speech synthesis meant that new speech could be synthesised and words could be changed.
\citet{AdobeSystems2016} demonstrated an unreleased prototype system called \textit{VoCo} that enabled users to change
a word in a speech recording, whilst retaining the natural characteristics of the original speaker's voice. Such
technology could be used to create seamless repairs to errors in speech recordings. However, the use of such technology
has ethical and legal implications, particularly in a broadcasting context \citep{Bendel2017}\footnote{There's an
interesting episode of Radiolab that discusses this: \url{http://www.radiolab.org/story/breaking-news/}}.

\subsection{Confidence shading}\label{sec:background-confidence}
In addition to producing a transcript, many ASR systems return a confidence score for each
transcribed word, indicating how sure the system is that the word is correct. \textit{Confidence shading} is a technique for
displaying this score by colouring words with a low confidence score in a lighter shade \citep{Suhm2001}.
Confidence shading has been used to try to make mistakes easier to locate, and transcripts easier to read.
However, confidence scores may themselves be incorrect by indicating that a correct word is probably incorrect, or that
an incorrect word is probably correct. The trade-off between these two types of errors is controlled using the threshold
value \citep{Feng2004}.

\citet{Suhm2001} conducted a user study of 15 participants who corrected an ASR transcript with and without confidence
shading (in this case, highlighting). The results showed that correction with confidence shading took slightly longer
than without, although this was not statistically significant.  Conversely, \citet{Burke2006} reported that in their
user study of 16 participants, most agreed that confidence shading was helpful for identifying mistakes in the
transcripts. One notable difference between these studies is that \citet{Suhm2001} optimised their confidence threshold
to minimise the overall accuracy of the confidence shading, whilst \citet{Burke2006} increased the threshold to treat
false negatives more seriously.

\citet{Vemuri2004} studied whether confidence shading improved comprehension of the transcript. They conducted a user
study of 34 participants and measured the comprehension of short audio clips when using ASR
transcripts with and without confidence shading. Although the results indicated better comprehension with confidence
shading, there was no statistically significant difference.





















\section{Audio playback interfaces}\label{sec:background-playback}

The previous work we have considered so far has used audio visualization and transcripts to represent audio content.
Visual presentation of audio content makes it easier for users to search and skim the information, but it is difficult,
if not impossible, for humans to fully comprehend sound using visual means. Listening is the natural way for humans to
consume audio content, but the time required to listen can make it a lengthy and inefficient process.

As we shall see in this section, audio processing can be used to increase the speed at which users can listen to audio
recordings. Through our literature review, we found that previous research has used two main techniques to achieve this. The first
uses processing to improve the comprehension of speech at higher playback rates.  The second exploits the ``cocktail
party effect'' by playing multiple audio streams simultaneously and using audio processing to help the listener
separate the sounds.  We discuss each of these techniques below.

\subsection{Time compression}\label{sec:background-time-compression}

Listening to long audio recordings of speech can be time-consuming. A simple way to reduce the listening time is to
increase the rate of playback.  However, this increase in speed causes an upward shift in the pitch of the sound, which
is sometimes described as sounding ``like chipmunks'' \citep{Vemuri2004,Ranjan2006}.  The increased speed with which
the content is presented also makes it difficult for listeners to process the information fast enough.

In this field, \textit{intelligibility} is defined as the ability to identify words, and can be measured by the
accuracy with which a specific word is recalled. \textit{Comprehension} is defined as the ability to understand the
content of the material, measured by the number of correctly answered questions about the subject matter
\citep{Foulke1969}.  The change in pitch caused by speeding-up audio negatively affects both the intelligibility and
comprehension of speech \citep{Arons1997}.


There are several approaches for reducing the time required to listen to a recording while being able to extract
critical information, which can be divided into two categories. \textit{Speed-up} techniques aim to increase the speed
of playback without affecting the pitch of the speech, and \textit{excision} techniques aim to remove parts of the
speech in a way that minimises the reduction in comprehension \citep{Arons1997}.

\citet{Tucker2006} performed a user study that compared two different excision techniques and a speed-up technique,
using both 5-min and 30-min audio recordings.  Participants ranked a list of utterances to match what they heard, which
was compared to a reference response to produce a score for comprehension. This score was normalised by the listening
time to measure ``comprehension efficiency''.  The results showed that for short recordings, excision outperformed
speed-up, but that they performed similarly for long recordings. However, when using excision, participants were less
likely to switch to normal-speed playback, and they reported that they preferred excision to speed-up.



The simplest excision technique is to remove frames of audio at regular intervals, known as ``isochronous sampling''
\citep{Arons1997}.  However, this approach does not discriminate between valuable and redundant information. It also
fails to take into account speech boundaries, so may cut the audio mid-way through a word. Shortening or removing
pauses between words is a simple and effective approach that reduces the length of the audio whilst retaining all of
the information and respecting speech boundaries. However, once all of the pauses have been removed, other techniques
must be used to further compress the speech.


Many excision algorithms operate by segmenting the audio at points of increased saliency, then playing only the
beginning of each segment before moving onto the next. The saliency can be determined by measuring pause length, pitch,
speaker turns and using transcripts.  Long pauses in speech often signal a new sentence, thought or topic, which can be
an indication of importance. The pitch of the voice tends to increase in range when introducing a new topic
\citep{Hirschberg1992}, which can be used as a measure of emphasis.  Speaker diarization techniques
\citep{AngueraMiro2012} can be used to detect changes in speaker, which can be a cue for changes in topic. Transcripts
of the speech have also been used with summarisation techniques to determine the most salient parts of the speech,
using both ASR transcripts \citep{Hori2003} or manually-written transcripts \citep{Tucker2006}.

\textit{SpeechSkimmer} by \citet{Arons1997} combined three excision techniques into a single time compression interface
by switching between them for different rates of playback. He used pause shortening and removal for modest speed
increases, followed by pause-based segmentation for faster playback. For the fastest playback rate, he used segmentation
resulting from a pitch-based emphasis detection algorithm. He evaluated the system through a qualitative study of 12
participants, which compared two systems that used different algorithms for the fastest playback rate --- one using
pitch-based emphasis segmentation and the other using isochronous sampling. The participants reported that pitch-based
emphasis was effective at extracting interesting points, and performed better than excision using isochronous sampling.

There are limits to how far time compression can be used to increase playback speed.  For example, speed-up techniques
are only intelligible up to a maximum of around $2\times$ to $2.6\times$ real-time
\citep{Vemuri2004,Tucker2006,Ranjan2006,Arons1997}.  However, transcripts can be used in combination with time
compression to increased this maximum rate.  \citet{Vemuri2004} conducted a user study of 34 participants and measured
their comprehension of short audio clips at different rates of playback using speed-up. The mean self-reported maximum
playback rate was $2.6\times$ real-time for listening only. The addition of an ASR transcript increased this to
$2.8\times$, and a verbatim transcript increased this further to $3.0\times$. \citet{Whittaker2002} exploited this by
including time-compressed playback in the SCANMail semantic speech interface.



\subsection{Simultaneous playback}

The \textit{cocktail party effect} is ``the ability to focus one's listening attention on a single talker among a
cacophony of conversations and background noise'' \citep{Arons1992}.  This effect can be exploited to help listeners
find a particular piece of audio in a recording by playing different parts of that recording simultaneously. To help
listeners separate the sounds, previous work has experimented with using headphones to play different sounds in each
ear, or using binaural audio to spatially separate the sounds. 

\textit{AudioStreamer} from \citet{Schmandt1995} used binaural spatialization techniques to play three simultaneous
audio streams of broadcast news around a listener's head. The system tracked the movement of the listener's head to
boost the level of the stream they were facing as they turned.  In addition, they used pause-based segmentation and
speaker diarization to alert the listener to new stories using a short bleep sound. No user studies of AudioStreamer
were conducted.

\textit{Dynamic Soundscape} from \citet{Kobayashi1997} also used spatialization to help users navigate audio files by
mapping the sound to fixed positions a virtual soundscape. The system was designed to take advantage of human abilities
for simultaneous listening and memorising location. Users would start by listening to a virtual ``speaker'' that played
the audio while slowly orbiting their head in a clockwise direction. Audio could be replayed by pointing their hand 
at the location where it was originally heard, which would create a second speaker that played from that position.
Similarly, users could skip ahead by pointing to a position ahead of the original source.  Speakers could be grabbed
and moved, and an audible ``cursor'' allowed users to hear where they were pointing.  Through informal feedback, users
suggested that they could use their spatial memory to navigate the audio. Based on their observations, the authors
suggested that the system could also help with transfer to long-term memory.

\citet{Ranjan2006} attempted to reduce the time needed to search an audio recording by using \textit{dichotic
presentation}, where different sounds are played into each ear.  In their system, the left ear played from the
beginning of the recording while the right ear played from the half-way point. Through a user study of 13 participants,
they tested the effectiveness of this approach for a search task. The results showed that dichotic presentation reduces
the overall search time compared to normal audio playback, particularly when the answer is in the second half of the
recording. The overall time reduction was around 20\%.  Dichotic presentation can be combined with time
compression, but this creates high cognitive load and 8 of the 13 participants reported it to be ``very demanding''.







\clearpage
\section{Research questions}\label{sec:background-questions}

In this chapter, we described the context of our research topic by introducing audio editing, semantic audio analysis,
audio visualization, semantic speech interfaces and audio playback interfaces.  We are now in a position to reflect
upon our aim (Section~\ref{sec:aim}) and the literature to formulate the research questions we want to address in this
thesis.

In Sections~\ref{sec:background-semantic}, \ref{sec:background-visualization}, \ref{sec:background-transcripts} and
\ref{sec:background-playback}, we introduced a variety of methods and technologies that could potentially improve
interaction with, and manipulation of, recorded audio. However, it is unclear which of these are most appropriate or
most effective for radio production.
\\

\noindent
\hangindent=15pt
\hangafter=1
\textbf{Question 1:} How can radio production be improved with new technology for interacting with and manipulating
recorded audio?
\\

In Section~\ref{sec:background-daw-visual}, we saw how DAWs use audio waveforms for the navigation and manipulation of
audio content, but that there are limitations to this approach.  Despite their widespread use, the author
could not find any studies that attempted to measure the performance of audio waveforms.
Section~\ref{sec:background-waveforms-related} described several promising methods for enhancing audio waveforms by
using colour to add semantic information. However, the author could also not find any formal evaluations of these
methods.
\\

\noindent
\hangindent=15pt
\hangafter=1
\textbf{Question 2:} What is the role and efficacy of audio visualisation in radio production?
\\

In Section~\ref{sec:background-transcripts}, we saw how user studies from \citet{Whittaker2004,Yoon2014,Sivaraman2016}
found that semantic speech editing is faster and more accessible than waveform editing, and easy to use.  However,
these systems were designed for navigating and editing voice messages and spoken comments, which use a different style
of audio content and have different requirements than radio production.  \citet{Rubin2013} demonstrated a system for
the production of ``audio stories'', which has many similarities to radio production, but this system was not formally
evaluated, so it is unclear what effect semantic editing has on the radio production process.
\\

\noindent
\hangindent=15pt
\hangafter=1
\textbf{Question 3:} How can transcripts of speech be adapted and applied to radio production?
\\






To be able to answer these questions, we first need to have a solid understanding of the radio production process.
Despite the large scale of radio production activity around the world, the author could only find two studies that
involved radio producers \citep{Dunaway2000,Barbour2004}, both of which were written by radio producers working in
academia.  This shortage of studies may be a result of the limited number of radio producers, and their demanding
workload, which can make it challenging to recruit them for academic research. For example, \citet{Kim2003} worked with
National Public Radio (NPR) to develop a speech archive interface, but reported that they were unable to recruit any
radio producers to evaluate their system due to the small population and their limited availability.


The author of this thesis is an employee of BBC R\&D, which gives us access to the resources of BBC Radio. This is
unusual in academic research, where studies are often conducted with student participants and under laboratory
conditions. We want to exploit our position within the BBC to be able to capture and share information about how radio
programmes are produced.

In Chapter~\ref{sec:ethno}, we begin our research by conducting three ethnographic case studies of production practice
within BBC Radio.  The results of this study will allow us to be better informed about the tasks and challenges
involved in production, which will guide our research direction and design choices.  This study will also allow us to
take advantage of access available to us that other researchers would not have. Once we have gained a better
understanding of the processes and challenges of radio production, in Section~\ref{sec:ethno-strategy} we will reflect
upon our findings and our research aim to determine a research strategy for achieving our goal.



















